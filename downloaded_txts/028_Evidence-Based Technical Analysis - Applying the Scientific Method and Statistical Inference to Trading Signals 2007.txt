==================================================
                      PAGE 1                      
==================================================

6!:')
WileyTrading



==================================================
                      PAGE 2                      
==================================================

EVIDENCE-BASED TECHNICAL ANALYSIS
Praise for
"In clear language, Aronson demonstrates the theoretical flaws in interpretative techni
calanalysis methodologies, theflawed premisesandconclusionsofthe EfficientMarket
Hypothesis, andtheappropriatetechniques fordevelopingandtestingtechnical analysis
methods that do have validity. Readers will learn alot from this book."
-JackSchwager,authorofMarlcetWizardsandtheSchwageron Futuresbookseries
"Aronson's explanation of data mining is a must-read for every analyst, and his overall
discussion of statistical inference is critical to success.The book is filled with common
sense examples and provides atesting and validation process that saves time, frustra
tion, and money."
- Perry Kaufman,authorof NewTradingSystemsandMethods, Fourth Edition
"This bookdebunks manyofthe mythsoftechnical analysis. One should read this book
before buying atechnical system.The book is agood reference to the literature on the
subjectwith extensive footnotes and bibliography."
-SandorStraus,Managing Member,Merfin,LLC
"You may not agree with everything David Aronson says in this controversial, but com
pelling new study. Still, every trader who wants to invest technical analysis with the
dignityofagreat science should read this discerning account."
- Nelson Freeburg, Editor, FonnulaResearch
"There is an emerging trend in the technical analysis community toward a new, more
rigorous type of analysis that utilizes the scientific process and depends on statistical
testing to gauge the quality of one's results. Just as quantitative analysts have long
used the tools of technical analysis, market technicians are taking up the quant's tools.
Evidence-BasedTechnicalAnalysis is awelcome milestone on aroad that leads toward
abetter, more dependable type of analysis."
-John Bollinger,CFA,CMT,BollingerBands.com
"There are illusions of the mind that are every bit as real as optical illusions. Aronson's
criticisms of popularforms oftechnical analysis are right on target."
- Fred Gehm,authorof QuantitativeTradingandMoneyManagement
ISBN 0-470-00874-1
59500
.'C."'T."""AL
~1807~
=6!JWILEY:
~.'~'~T~.~.J
9 780470 008744



==================================================
                      PAGE 3                      
==================================================

$95.00 USA/$113.99 CAN/£65.00 UK
san approach to research, technical analy
sis has suffered because it is a"discipline"
practiced without discipline. In order for
technical analysis to deliver useful knowledge
that can be applied to trading, it must evolve
into arigorous observational science.
Over the past two decades, numerous articles
in respected academic journals have approached
technical analysis in a scientifically rigorous
and intellectually honest manner, and now,
Evidence-Based Technical Analysis looks to
continue down this path. Organized into two
parts, this valuable resource first establishes the
methodological, philosophical, and statistical
foundations of eVidenced-based technical
analysis (EBTAl, and then demonstrates this
approach-by using twenty-five years of historical
data to test 6,400 binary buy/sell rules on the
S&P 500.
Evidence-Based Technical Analysis examines
how you can apply the scientific method, and
recently developed statistical tests, to deter
mine the true effectiveness of technical trading
signals. Throughout these pages, expert David
Aronson details this newtype oftechnical anal
ysis that-unlike traditional technical analysis
is restricted to objective rules, whose historical
profitability can be quantified and scrutinized
Filledwithin-depthinsightsandpracticaladvice,
Evidence-Based Technical Analysis provides
you with comprehensive coverage of this new
methodology, which is specifically designed for
evaluating the performance of rules/signals that
are discovered by data mining. Experimental
results presented in the book will show
you that data mining-a process in which
many rules are back-tested and the best
performing rules are selected-is an effective
( continued on back flap)



==================================================
                      PAGE 4                      
==================================================

( continued from front flap)
procedure for discovering useful rules/signals.
However, since the historical performance
of the rules/signals discovered by data min
ing are upwardly biased, new statistical tests
are required to make reasonable inferences
about future profitability. Two such tests, one
of which has never been discussed anywhere
heretofore, are described and illustrated.
Ifyou wanttousetechnical analysisto navigate
today's markets, you must first abandon the
subjective, interpretive methods traditionally
associated with this discipline, and embrace
an approach that is scientifically and statisti
cally valid. Grounded in objective observation
and statistical inference, EBTA is the approach
to technical analysis you need to succeed in
your trading endeavors.
DAVID ARONSON is an adjunct professor at
Baruch College, where he teaches agraduate
level course in technical analysis He is also
a Chartered Market Technician and has pub
lished articles on technical analysis. Previously,
Aronson was aproprietarytraderand technical
analystfor Spear Leeds & Kellogg. He founded
Raden Research Group, afirmthatwas an early
adopter of data mining within financial markets.
Priorto that, Aronson founded AdvoCom, afirm
that specialized in the evaluation of commodity
money managers and hedge funds, their
performance, and trading methods. For free
access to the algorithm for testing data mined
rules, go to www.evidencebasedta.com.
JacketDesign: Pro-ArtGraphicDesign
SubscribetoourfreeFinanceandInvestingeNewsletterat
www.wiley.comJenewsletters
I I
Visitwww.wileyfinance.com
G)WILEY
wiley.com



==================================================
                      PAGE 5                      
==================================================

Evidence-Based
Technical
Analysis



==================================================
                      PAGE 6                      
==================================================

Founded in 1807,John Wiley & Sons is the oldestindependent publishing
companyinthe UnitedStates. WithofficesinNorthAmerica, Europe,Aus
tralia and Asia, Wiley is globally committed to developing and marketing
printand electronicproductsandservicesfor ourcustomers'professional
and personalknowledge andunderstanding.
TheWileyTradingseriesfeatures booksbytraderswho havesurvived
the market's ever changing temperament and have prospered-some by
reinventing systems, others by getting back to basics. Whether a novice
trader, professional or somewhere in-between, these books will provide
the adviceandstrategiesneeded toprospertoday and well into thefuture.
For a list of available titles, please visit our Web site at www.Wiley
Finance.com.



==================================================
                      PAGE 7                      
==================================================

Evidence-Based
Technical
Analysis
Applying the Scientific
Method and
Statistical Inference
to Trading Signals
DAVID R. ARONSON
.ICItNTCHNIAL
J •
;1807:
~~WILEY:
.z z
~2007;
,
John Wiley& Sons, Inc.



==================================================
                      PAGE 8                      
==================================================

Copyright©2007byDavidR.Aronson. Allrightsreserved.
PublishedbyJohnWiley& Sons,Inc.,Hoboken, NewJersey.
PublishedsimultaneouslyinCanada.
Nopartofthispublicationmaybereproduced,storedinaretrievalsystem,or
transmittedinanyform orbyanymeans,electronic,mechanical,photocopying,
recording,scanning,orotherwise,exceptaspermittedunderSection107or108
ofthe 1976UnitedStatesCopyrightAct, withouteitherthepriorwritten
permissionofthePublisher,orauthorizationthroughpaymentoftheappropriate
per-copyfeetotheCopyrightClearanceCenter,Inc.,222RosewoodDrive,
Danvers,MA01923, (978) 750-8400,fax (978)646-8600,oronthewebat
www.copyright.com. RequeststothePublisherforpermissionshouldbe
addressedtothePermissionsDepartment,JohnWiley& Sons,Inc., III River
Street,Hoboken, NJ07030, (201) 748-6011,fax (201)748-6008,oronlineat
http://www.wiley.com/go/permissions.
LimitofLiabilitylDisclaimerofWarranty: Whilethepublisherandauthorhave
usedtheirbesteffortsinpreparingthisbook,theymakenorepresentationsor
warrantieswithrespecttotheaccuracyorcompletenessofthecontentsofthis
bookandspecificallydisclaimanyimpliedwarrantiesofmerchantabilityorfitness
foraparticularpurpose. Nowarrantymaybecreatedorextendedbysales
representativesorwrittensalesmaterials.Theadviceandstrategiescontained
hereinmaynotbesuitableforyoursituation.Youshouldconsultwitha
professionalwhereappropriate. Neitherthepublishernorauthorshallbeliable
foranylossofprofitoranyothercommercialdamages,includingbutnotlimited
tospecial,incidental, consequential,orotherdamages.
Forgeneralinformationonourotherproductsandservicesorfortechnical
support,pleasecontactourCustomerCareDepartmentwithintheUnitedStates
at(800)762-2974, outsidetheUnitedStatesat(317)572-3993orfax
(317)572-4002.
Wileyalsopublishesitsbooksinavarietyofelectronicformats. Somecontent
thatappearsinprintmaynotbeavailableinelectronicbooks. Formore
informationaboutWileyproducts,visitourwebsiteatwww.wiley.com.
LibraryofCongress Cataloging-in-PublicationData:
Aronson, DavidR., 1945-
Evidence-basedtechnicalanalysis: applyingthescientificmethodand
statisticalinferencetotradingsignals/DavidR. Aronson.
p. cm.-(Wileytradingseries)
Includesbibliographicalreferencesandindex.
ISBN-13: 978-0-470-00874-4(cloth)
ISBN-I0: 0-470-00874-1 (cloth)
1. Investmentanalysis. 1. Title. II. Series.
HG4529.A77 2007
332.63'2042-dc22
2006014664
PrintedintheUnitedStatesofAmerica.
10 9 8 7 6 5 4 3 2



==================================================
                      PAGE 9                      
==================================================

To Jack andBelma



==================================================
                     PAGE 10                      
==================================================





==================================================
                     PAGE 11                      
==================================================

Contents
Acknowledgments Ix
About theAuthor xi
Introduction 1
PART I Methodological, Psychological,
Philosophical, and Statistical
Foundations
CHAPTER' Oblective Rules and Their Evaluation 15
CHAPTER 2 The IllusoryValidity ofSublective
Technical Analysis 33
CHAPTER 3 The ScientiOc Method and Technical
Analysis 103
CllAP'rER 4 Statistical Analysis 165
CHAPTER 5 Hypothesis Tests and Confidence Intervals 217
CHAP1'ER 6 Data-Minin8 Bias: The Fool's Gold
ofOblective 11\ 255
CHAPTER 7 Theories ofNonrandom Price Motion 331
PART II Case Study: Signal Rules
fol' the S&P 500 Index
CIIAPTER 8 Case Study ofRule Data Minin8
for the S&P 500 389
CIIJ\P1'ER 9 Case Study Results and the Future of11\ 441
vii



==================================================
                     PAGE 12                      
==================================================

viii CONTENTS
APPENDIX ProofThat Detrending Is Equivalent to
Benchmarking Based on Position Bias 475
Notes 477
Index 517



==================================================
                     PAGE 13                      
==================================================

Acknowledgments
T
houghabookisattributedto itsauthor(s), ittrulyreflectstheefforts
of many more people. I wish to acknowledge those individuals
without whom this book would have been impossible or a much
lesserwork.
Iam mostindebtedto Dr. TimothyMasters, whom Ihave had theplea
sure of knowing for over 10 years. His patient and intelligent guidance
keptmeonasolidstatisticalfooting. Timnotonlygavemeimportantfeed
back on technical issues but was responsible for coding and running the
ATR rule experiments and the statistical routines used to test the over
6,400 rules examined. Tim also innovated the Monte Carlo permutation
method as an alterative to the patented method of White, called Reality
Check, for testing the statistical significance of rules discovered by data
mining. Tim hasgraciouslydecidedtoputthemethodinthepublicdomain
andhasallowedittobe publishedfor thefirsttime here.
Also crucial were the programming talents of Stuart Okorofsky and
the database creation by Dr. John Wolberg. I am indebted Dr. Halbert
White, inventor of Reality-Check and for the help of Professor David
Jensen, director of the Knowledge Discovery Lab at the University of
Massachusetts-Amherst.
Ialso wish to express my appreciation to the following people for re
viewing and commenting on various chapters. Theirfeedback was essen
tial: Charles Neumann, Lance Rembar, Dr. Samuel Aronson, Dennis Katz,
Hayes Martin, George Butler, Dr. John Wolberg, Jay Bono, Dr. Andre Shle
fier, Dr. John Nofsinger, Doyle Delaney, Ken Byerly, James Kunstler, and
KennyRome.
Special thanks to the helpful folks atJohn Wiley &Sons: Kevin Com
mins, for seeing the value ofa critical appraisal oftechnical analysis, and
Emilie Herman, for hersteadyhandin editingthe book. Thanks as well to
MichaelLiskandLauraWalsh.
ix



==================================================
                     PAGE 14                      
==================================================





==================================================
                     PAGE 15                      
==================================================

About the Author
DavidAronson isan adjunctprofessoroffinance atBaruch College'sZick
lin School of Business in New York, where he teaches a graduate-level
course in technical analysis to MBA and financial-engineering students,
andvice-presidentofHoodRiverResearchInc., afirm thatdevelopssignal
filters and predictive models. He was formerly a proprietary trader and
technical analyst at Spear, Leeds and Kellogg and president ofRaden Re
search Group Inc., a consulting firm that developed the data-mining soft
ware PRISMandfilters andsystemsforvarioustradingfirms. Priortothat,
he founded AdvoCom Corporation, which managed clientfunds inportfo
lios offutures trading advisors usingportfolio optimization. He received a
BA in philosophy from Lafayette College in 1967 and served in the Peace
Corpsin ElSalvador.
xi



==================================================
                     PAGE 16                      
==================================================





==================================================
                     PAGE 17                      
==================================================

Introduction
T
echnical analysis (TA) is the study ofrecurring patterns in financial
market data with the intent offorecasting future price movements.
I
Itiscomprisedofnumerousanalysismethods, patterns, signals, indi
cators, and trading strategies, each with its own cheerleaders claiming
thattheirapproachworks.
MuchofpopularortraditionalTAstandswheremedicinestoodbefore
it evolved from a faith-based folk art into a practice based on science. Its
claims are supported by colorful narratives and carefully chosen (cherry
picked) anecdotes ratherthan objectivestatistical evidence.
This book's central contention is that TA must evolve into a rigorous
observational science ifit is to deliver on its claims and remain relevant.
Thescientificmethodisthe onlyrationalwayto extractuseful knowledge
from market data and the only rational approach for determining which
TA methods have predictive power. I call this evidence-based technical
analysis (EBTA). Grounded in objective observation and statistical infer
ence (Le., thescientificmethod), EBTAchartsa course betweenthe magi
cal thinking and gullibility ofa true believer and the relentless doubt ofa
randomwalker.
ApproachingTA, oranydisciplinefor thatmatter, in a scientificman
nerisnoteasy. Scientific conclusionsfrequently conflictwith whatseems
intuitively obvious. To early humans it seemed obvious that the sun cir
cled the earth. It took science to demonstrate that this intuition was
wrong. An informal, intuitive approach to knowledge acquisition is espe
cially likely to result in erroneous beliefs when phenomena are complex
or highly random, two prominent features of financial market behavior.
Although the scientific methodis notguaranteed to extractgold from the
mountains of market data, an unscientific approach is almost certain to
producefool's gold.
This book'ssecondcontentionisthatmuchofthe wisdomcomprising
thepopularversionofTAdoesnotqualifyas legitimateknowledge.
1



==================================================
                     PAGE 18                      
==================================================

2 INTRODUCTION
KEV DEFIMTIONS: PROPOSITIONS AND ClAIMS,
BELIEF AND KNOWLEDGE
I have already used the terms knowledge and beliefbut have not rigor
ously defined them. These and several other key terms will be used re
peatedlyinthisbook, sosomeformal definitionsare needed.
The fundamental building block ofknowledge is a declarative state
ment, also known as a claim oraproposition. Adeclarative statementis
one offour types ofutterances thatalso include exclamations, questions,
and commands. Declarative statements are distinguished from the others
in that they have truth value. Thatis to say, they can be characterized as
eithertrueorfalse orprobablytrue orprobablyfalse.
Thestatement"Orangesareonsaleatthesupermarketforfive centsa
dozen" is declarative. Itmakes a claim about a state ofaffairs existing at
the local market. It may be true or false. In contrast, the exclamatory
statement"Holycow, whata deal," the command"Gobuymea dozen," or
the question "Whatisan orange?" cannotbecalled true orfalse.
Our inquiry into TA will be concerned with declarative statements,
suchas, "RuleXhaspredictivepower." Ourgoalis to determine which of
thesedeclarativestatementswarrantourbelief.
What does itmean to say, "I believe X."? "With regard to states ofaf
fairs in general (i.e., 'matters of fact' or 'what will happen') believing X
amounts to expectingto experience Xifand when we are in aposition to
do SO."2 Therefore, ifI believe the claim that oranges are on sale for five
cents a dozen, it means that I expect to be able to buy oranges for five
cents a dozen ifIgo to the store. However, the command to buysome or
anges ortheexclamationthatIamhappyaboutthe opportunity, setup no
suchexpectation.
Whatdoes allthisall meansfor us? Foranystatementtoevenbecon
sidered as a candidate for belief, it must"assertsome state ofaffairs that
can be expected.3 Such statements are said to have cognitive content
they convey something that can be known. "If the statement contains
nothingto knowthenthereisnothingthere tobe believe."4
Although all declarative statements presumably have cognitive con
tent, not all actually do. This is not a problem if the lack of cognitive
content is obvious, for example, the declaration "The square root of
Tuesday is a prime number."5 This utterance is, on its face, nonsense.
There are other declarative statements, however, whose lack of cogni
tive content is not so obvious. This can be a problem, because such
statements can fool us into thinking that a claim has been made that
sets up an expectation, when, in fact, no claim has really been put for
ward. These pseudo-declarative-statements are essentially meaningless
claims oremptypropositions.



==================================================
                     PAGE 19                      
==================================================

Introduction 3
Although meaningless claims are not valid candidates for belief, this
does notstop manypeoplefrom believinginthem. Thevaguepredictions
made in the daily astrology column or the nebulous promises made by
promoters of bogus health cures are examples of meaningless claims.
Those who believe these empty propositions simply do not realize that
whattheyhavebeentoldhas no cognitivecontent.
Awayto tell ifa statementhas cognitive contentand is, thus, a valid
candidate for beliefis the discernible-difference test6 described by Hall.
"Utterances with cognitive content make claims that are either true or
false; and whether they are true or false makes a difference that can be
discerned. That is why these utterances offer something to believe and
whythere isno pointintryingto believe an utterance thatmakesnosuch
offer"7Inotherwords,apropositionthatpassesthediscernible-difference
test sets up an expectation such that the state ofaffairs, ifthe statement
were true, is recognizably different from the state ofaffairs, if the state
mentwerefalse.
The discernible-difference criterioncanbe applied tostatementspur
portingtobepredictions.Apredictionisaclaimtoknowsomethingabout
the future. If a prediction has cognitive content, it will be clearly dis
cernibleinthe outcomeifthe predictionwas accurate ornot. Many, ifnot
most, ofthe forecasts issued bypractitioners ofpopularTAare devoid of
cognitive content on these grounds. In other words, the predictions are
typicallytoovague toeverdetermineiftheywere wrong.
The truth or falsity ofthe claim oranges are on saleforfive cents a
dozen will make a discernible difference when I get to the market. It is
thisdiscernibledifferencethatallowstheclaimtobetested. Aswillbede
scribed in Chapter 3, testing a claim on the basis ofa discernible differ
enceiscentralto thescientificmethod.
Hall, in his book Practically Profound, explains why he finds
Freudianpsychoanalysisto bemeaningless whenexaminedinlightofthe
discernible-differencetest.
"Certain Freudian claims about human sexual development are com
patiblewithallpossiblestatesofaffairs.Thereisnowaytoconfirmordis
confirm either 'penis envy' or 'castration complex' because there is no
distinguishabledifferencebetweenevidenceaffirmingandevidencedeny
ing these interpretations of behavior. Exactly opposite behaviors are
equally predictable, depending on whether the alleged psychosexual
stress is overt orrepressed." The requirement of"cognitive contentrules
out all utterances that are so loose, poorly formed or obsessively held
(e.g., conspiracy theories) that there is no recognizable difference be
tweenwhatwouldbethecaseiftheywereso, andwhatwouldbethecase
ifthey were not."8 In a like vein, the Intelligent Design Theory carries no
cognitivefreightinthesensethatnomatterwhatlifeform isobserveditis



==================================================
                     PAGE 20                      
==================================================

INTRODUCTION
consistent with the notion that it manifests an underlying form specified
bysomeintelligentdesigner.9
Whatthen isknowledge? Knowledge can be defined asjustijied true
belief Hence, in order for a declarative statement to qualify as knowl
edge, not onlymust it be a candidate for belief, because it has cognitive
content, but it must meet two other conditions as well. First, it must be
true (orprobablytrue). Second, thestatementmustbe believedwithjus
tification. Abeliefisjustified when it is based on sound inferences from
solidevidence.
Prehistoric humans held the false beliefthat the sun moved across
the sky because the sun orbitedthe earth. Clearlytheywere not in pos
session of knowledge, but suppose that there was a prehistoric person
who believed correctly that the sun moved across the sky because of
the earth's rotation. Although this beliefwas true, this individual could
not be described as possessing knowledge. Even though they believed
what astronomers ultimately proved to be true, there was no evidence
yet to justify that belief. Without justification, a true belief does
not attain the status of knowledge. These concepts are illustrated in
Figure 1.1.
~\\ Statements
EmptyClaims
FIGURE I.t Knowledge:justified true belief.



==================================================
                     PAGE 21                      
==================================================

Introduction 5
From this it follows that erroneous beliefs orfalse knowledge fail to
meetone ormore ofthe necessary conditions ofknowledge. Thus, an er
roneousbeliefcanariseeitherbecauseitconcernsameaninglessclaimor
because it concerns a claim that, though meaningful, is not justified by
validinferencesfrom solidevidence.
Still, evenwhen we have done everythingright, by drawingthe best
possible inference from sound evidence, we can still wind up adopting
erroneous beliefs. In other words, we can be justified in believing a
falsehood, and honestly claim to know something, if it appears to be
true according to logicallysound inferencesfrom the preponderance of
available evidence. "We are entitled to say 'I know' when the target
of that claim is supported beyond reasonable doubt in the network
of well-tested evidence. But that is not enough to guarantee that we
do know."lo
Falsehoods are an unavoidable fact oflife when we attempt to know
things about the world based on observed evidence. Thus, knowledge
based on the scientific method is inherently uncertain, and provisional,
though less uncertain than knowledge acquired by less formal methods.
However, over time, scientific knowledge improves, as it comes to de
scribe reality in a progressively more accurate manner. It is a continual
work in progress. The goal ofEBTAisa bodyofknowledge aboutmarket
behaviorthatisasgoodascanbehad,giventhelimitsofevidencegather
ingand thepowers ofinference.
ERRO EOUS 'IJ\ KNOWLEDGE: THE COST
OF UNDISCIPLINED ANALYSIS
To understandwhythe knowledge produced bythepopularversion ofTA
is untrustworthy, we must consider two distinct forms ofTA: subjective
and objective. Bothapproachescanleadto erroneous beliefs, butthey do
soindistinctways.
Objective TAmethodsare well defined repeatable procedures thatis
sueunambiguoussignals. Thisallowsthemto beimplementedascomput
erizedalgorithmsandback-tested onhistoricaldata. Results producedby
abacktestcanbeevaluatedinarigorousquantitative manner.
Subjective TA methods are not well-defined analysis procedures. Be
cause oftheirvagueness, an analyst'sprivate interpretationsare required.
This thwarts computerization, back testing, and objective performance
evaluation. In other words, it is impossible to either confirm or deny a
subjective method's efficacy. For this reason they are insulated from evi
dentiarychallenge.



==================================================
                     PAGE 22                      
==================================================

6 INTRODUCTION
Fromthe standpointofEBTA, subjective methods are the mostprob
lematic. They are essentially meaningless claims that give the illusion of
conveying cognitive content. Because the methods do not specify how
they are to be applied, different analysts applying it to the same set of
market datacan reach different conclusions. This makes itimpossible to
determine if the method provides useful predictions. Classical chartpat
ternanalysis, hand-drawntrend lines, ElliottWave Principle,12 Gannpat
II
terns, Magic T's and numerous other subjective methods fall into this
category.13 Subjective TA is religion-it is based on faith. No amount of
cherry-picked examples showing where the method succeeded can cure
this deficiency.
Despite their lack of cognitive content and the impossibility of ever
beingsupportedbysoundevidence, there isnoshortageofferventbeliev
ersinvarioussubjectivemethods. Chapter2explainshowflaws inhuman
thinking can produce strong beliefs in the absence ofevidence oreven in
theface ofcontradictoryevidence.
Objective TA can also spawn erroneous beliefs butthey come about
differently. They are traceable to faulty inferences from objective evi
dence. The mere fact that an objective method has been profitable in a
back test is not sufficient grounds for concluding that it has merit. Past
performancecanfool us. Historicalsuccess isa necessarybutnotasuffi
cient condition for concluding that a method has predictive power and,
therefore, is likely to be profitable in the future. Favorable past perfor
mance can occurby luck or because ofan upward bias produced by one
form of back testing called data mining. Determining when back-test
profits are attributable to a good method rather than good luck is aques
tion that can only be answered by rigorous statistical inference. This is
discussed in Chapters 4 and 5. Chapter 6 considers the problem ofdata
miningbias. AlthoughIwillassertthatdatamining, when done correctly,
is the modemtechnician'sbestmethodfor knowledge discovery, special
ized statistical tests must be applied to the results obtained with data
mining.
HOW EBTA IS DIFFERENT
What sets EBTA apartfrom the popularform ofTA? First, itis restricted
to meaningful claims-objective methods thatcan be tested on historical
data. Second, it utilizes advanced forms ofstatistical inference to deter
mine ifa profitable back test is indicative of an effective method. Thus,



==================================================
                     PAGE 23                      
==================================================

Introduction 7
theprimefocus ofEBTAisdeterminingwhichobjectivemethodsarewor
thy ofactual use.
EBTA rejects all forms of subjective TA. Subjective TA is not even
wrong. Itis worse than wrong. Statementsthat can be qualified as wrong
(untrue) atleastconveycognitivecontentthatcanbetested. Theproposi
tions of subjective TA offer no such thing. Though, at first blush, they
seemto conveyknowledge, whentheyareexaminedcritically, itbecomes
cleartheyareemptyclaims.
Promoters of New Age health cures excel at empty claims. They tell
you that wearing their magic copper braceletwill make you will feel bet
ter and put more bounce in your step. They suggest your golf game will
improveandmaybeevenyourlovelife. However, the claim'slackofspeci
ficity makes itimpossible to nail down exactlywhat is beingpromised or
how itcan betested. Suchclaimscanneverbe confirmed orcontradicted
with objective evidence. On these same grounds, it can be said that the
propositionsofsubjectiveTAareemptyandthusinsulatedfrom empirical
challenge. Theymustbetakenonfaith.
In contrast, a meaningful claim is testable because it makes measur
able promises. It states specifically how much your golf game will im
prove or how bouncy yoursteps will be. This specificity opens the claim
to beingcontradictedwithempiricalevidence.
Fromtheperspective ofEBTA, proponentsofsubjectivemethodsare
faced with a choice: They canreformulate the method to be objective, as
one practitionerofthe Elliott Wave Principle has done,14 thus exposingit
to empirical refutation, orthey mustadmit the method must be accepted
on faith. Perhaps Gann lines actually provide useful information. In their
presentform, wearedeniedthis knowledge.
With respect to objective TA, EBTA does not take profitable back
tests at face value. Instead, they are subjected to rigorous statistical
evaluation to determine if profits were due to luck or biased research.
As will be pointed out in Chapter 6, in many instances, profitable back
tests maybe a dataminerfool's gold. This mayexplainwhy many objec
tive TAmethodsthatperformwellina backtestingperformworse when
applied to new data. Evidence-based technical analysis uses computer
intensive statisticalmethodsthatminimize problemsstemmingfrom the
data-miningbias.
The evolution ofTA to EBTA also has ethical implications. It is the
ethical and legalresponsibilityofall analysts, whateverform ofanalysis
they practice, to make recommendations that have a reasonable basis
and notto make unwarranted claims.15The onlyreasonable basis for as
serting an analysis method has value is objective evidence. Subjective



==================================================
                     PAGE 24                      
==================================================

8 INTRODUCTION
TAmethodscannotmeetthisstandard. ObjectiveTA, conductedinaccor
dancewiththestandardsofEBTAcan.
EBTA RESULTS FROM ACADEMIA
Evidence-based technical analysis is not a new idea. Over the past two
16
decades, numerous articles in respected academic journals have ap
proached TA in the rigorous manner advocated by this book.17 The evi
dence is not uniform. Some studies show TA does not work, but some
showthatitdoes. Becauseeachstudyisconfinedto aparticularaspectof
TAand a specificbody ofdata, itispossible for studiesto reach different
conclusions. Thisis oftenthe caseinscience.
The following are a few of the findings from academic TA. It shows
that, whenapproachedina rigorous and intellectuallyhonestmanner, TA
isaworthwhileareaofstudy.
• Expertchartistsareunableto distinguishactualpricechartsofstocks
from chartsproducedbyarandomprocess.18
• Thereisempiricalevidenceoftrendsincommoditiesl9andforeign ex
change markets thatcanbe exploitedwiththe simple objectivetrend
indicators. In addition, the profits earned by trend-following specula
tors may be justified by economic theory20 because their activities
provide commercial hedgers with a valuable economic service, the
transference ofpriceriskfrom hedgertospeculator.
• Simple technical rules used individually and in combinations can
yieldstatisticallyandeconomicallysignificantprofitswhenapplied to
stockmarketaveragescomposedofrelativelyyoungcompanies(Rus
sell2000and NASDAQComposite).21
• Neural networkshavebeenableto combinebuy/sellsignalsofsimple
moving-average rules into nonlinear models that displayed good pre
dictive performance on the DowJones Average over the period 1897
22
to 1988.
• Trendsinindustrygroupsandsectorspersistlongenoughafterdetec
23
tionbysimplemomentumindicatorstoearnexcessreturns.
• Stocks that have displayed prior relative strength and relative weak
ness continue to display above-average and below-average perfor
24
mance overhorizonsof3to 12months.
• United States stocks, selling near their 52-week highs, outperform
other stocks. An indicator defined as the differential between a
stock's currentprice and its 52-week high is a useful predictor offu-



==================================================
                     PAGE 25                      
==================================================

Introduction
ture relative performance.25The indicatorisan evenmore potentpre
dictorforAustralianstocks.26
• The head-and-shoulders chart pattern has limited forecasting power
when tested in an objective fashion in currencies. Better results can
behad withsimplefilter rules. The head-and-shoulders pattern, when
tested objectively on stocks, does not provide useful information.27
Traderswhoactonsuchsignalswouldbe equallyservedbyfollowing
a randomsignal.
• Trading volume statistics for stocks contain useful predictive infor
mation28 and improve the profitability ofsignals based on large price
changesfollowing apublic announcement.29
• Computer-intensive data-modeling neural networks, genetic algo
rithms, and other statistical learning and artificial-intelligence meth
odshavefound profitablepatternsintechnicalindicators.3o
WHO AM I TO CRITICIZE 'fA?
My interest in TA began in 1960 at the age of 15. During my high-school
and collegeyears Ifollowed a large stable ofstocks using the Chartcraft
point and figure method. I have used TA professionally since 1973, first
as a stock broker, then as managing partner of a small software com
pany, Raden Research Group Inc.-anearlyadopterofmachine learning
anddatamininginfinancial marketapplications-andfinally asapropri
etary equities trader for Spear, Leeds & Kellogg.31 In 1988, I earned the
Chartered Market Technician designation from the Market Technicians
Association. My personal TA library has over 300 books. I have pub
lished approximately a dozen articles and have spoken numerous times
on the subject. Currently I teach a graduate-level course in TA at the
ZicklinSchool ofBusiness, Baruch College, CityUniversityofNew York.
Ifreely admitmypreviouswritingsand researchdonotmeetEBTAstan
dards, in particular with regard to statistical significance and the data
miningbias.
My long-standing faith in TA began to erode in response to a very
mediocre performance over a five-year period trading capital for Spear,
Leeds and Kellogg. How could what I believed in so fervently not work?
Wasitmeorsomethingto do withTAin general?Myacademic trainingin
philosophyprovidedfertile grounds for my growing doubts. My concerns
crystallized into full fledged skepticism as a result ofreading two books:
How WeKnow What Isn't So byThomasGilovichand Why PeopleBelieve
Weird Things, by Michael Shermer. My conclusion: Technical analysts,



==================================================
                     PAGE 26                      
==================================================

10 INTRODUCTION
including myself, know a lot of stuff that isn't so, and believe a lot of
weird things.
TECHNICAL ANALYSIS: ART, SCIENCE,
OR SUPERSTITION?
There is a debate inthe TAcommunity: Is itanartora science?The ques
tionhasbeenframedincorrectly.Itismoreproperlystatedas: ShouldTAbe
basedonsuperstitionorscience?Framedthiswaythedebateevaporates.
SomewillsayTAinvolvestoomuchnuanceandinterpretationtoren
derits knowledge in the form ofscientificallytestable claims. To this Ire
tort: TA that is not testable may sound like knowledge, but it is not. It is
superstitionthat belongs in the realmofastrology, numerology, and other
nonscientificpractices.
Creativity and inspiration playa crucial role in science. They will be
importantinEBTAaswell. Allscientificinquiriesstartwitha hypothesis,
a new idea or a new insight inspired by a mysterious mixture of prior
knowledge, experienceandaleapofintuition. Yet, goodsciencebalances
creativity with analytical rigor. The freedom to propose new ideas must
be married to an unyielding discipline that eliminates ideas that prove
worthlessinthe crucibleofobjectivetesting. Withoutthisanchortoreal
ity, peoplefall inlove withtheirideas, andmagicalthinkingreplacescrit
icalthought.
Itis unlikelythatTAwilleverdiscoverrules thatpredictwiththepre
cisionofthe lawsofphysics. Theinherentcomplexityandrandomness of
financial markets and the impossibility of controlled experimentation
preclude such findings. However, predictive accuracy is not the defining
requirement ofscience. Rather, itis defined byan uncompromising open
nesstorecognizingandeliminatingwrongideas.
Ihave four hopesfor this book: First, that it will stimulate a dialogue
amongsttechnicalanalyststhatwill ultimatelyputourfield onafirmerin
tellectualfoundation; second,thatitwillencouragefurtherresearchalong
the lines advocated herein; third, that it will encourage consumers ofTA
to demand more "beef' from those who sellproducts and services based
upon TA; and fourth, that it will encourage TApractitioners, professional
and otherwise, to understand their crucial role in a human-machine part
nership that has the potential to accelerate the growth of legitimate TA
knowledge.
No doubt some fellow practitioners ofTA will be irritated by these
ideas. This can be a good thing. An oyster irritated by a grain of sand
sometimes yields a pearl. I invite my colleagues to expend their



==================================================
                     PAGE 27                      
==================================================

Introduction
energies adding to legitimate knowledge rather than defending the
indefensible.
This book is organized in two sections. Part One establishes the
methodological, philosophical, psychological, and statistical foundations
ofEBTA. PartTwo demonstrates one approach to EBTA: testing of6,402
binary buy/sell rules on the S&P 500 on 25 years ofhistorical data. The
rulesareevaluatedforstatisticalsignificanceusingtestsdesignedto cope
withtheproblemofdata-miningbias.



==================================================
                     PAGE 28                      
==================================================





==================================================
                     PAGE 29                      
==================================================

Methodological,
Psychological,
Philosophical,
and Statistical
Foundations



==================================================
                     PAGE 30                      
==================================================





==================================================
                     PAGE 31                      
==================================================

Objective Rules
and Their
Evaluation
T
his chapter introduces the notion ofobjective binary signaling rules
and a methodologyfortheirrigorous evaluation. Itdefinesanevalua
tion benchmarkbased onthe profitabilityofa noninformativesignal.
It also establishes the need to detrend market data so that the perfor
mancesofruleswithdifferentlong/shortpositionbiasescanbecompared.
'fIlE GREAT DMDE: OBJECTIVE VERSUS SUBJECTIVE
TECUNICAL ANALYSIS
Technical analysis eTA) divides into two broad categories: objective and
subjective. Subjective TA is comprised ofanalysis methods and patterns
that are not precisely defined. As a consequence, a conclusion derived
from asubjectivemethod reflectstheprivateinterpretationsoftheanalyst
applying the method. This creates the possibility that two analysts apply
ingthesamemethod to thesamesetofmarketdatamayarrive atentirely
different conclusions. Therefore, subjective methods are untestable, and
claimsthattheyareeffectiveareexemptfrom empirical challenge. Thisis
fertile ground for mythstoflourish.
In contrast, objective methods are clearly defined. When an objective
analysis method is applied to market data, its signals or predictions are
unambiguous. This makesitpossible tosimulate the methodonhistorical
data and determine its precise level ofperformance. This is called back
testing. Thebacktestingofan objectivemethodis, therefore, a repeatable
15



==================================================
                     PAGE 32                      
==================================================

16 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
experiment which allows claims ofprofitabilityto be tested and possibly
refuted with statistical evidence. This makes itpossible to find outwhich
objectivemethodsareeffective and whicharenot.
Theacidtestfordistinguishingan objectivefrom a subjectivemethod
is theprogrammability criterion:A method is objectiveifand only ifit
can be implemented as a computerprogram that produces unambigu
ous marketpositions (long,! short,2orneutral3 ). All methods thatcannot
bereducedtosuchaprogramare, bydefault, subjective.
1J\ RULES
Objective TA methods are also referred to as mechanical trading rules or
trading systems. In this book, all objective TA methods are referred to
simplyasrules.
Arule isa function that transforms one ormore itemsofinformation,
referred to as the rule's input, into the rule's output, which is a recom
mended market position (e.g., long, short, neutral). Input(s) consists of
one or more financial market time series. The rule is defined by one or
more mathematical and logical operators that convert the input time se
ries into a new time seriesthatconsists ofthe sequence ofrecommended
market position (long, short, out-of-the-market). The output is typically
representedbya signednumber(e.g., +1 or-1). Thisbookadoptsthe con
vention of assigning positive values to indicate long positions and nega
tive values to indicate shorts position. The process by which a rule
transforms one ormore input series into an outputseries is illustrated in
Figure 1.1.
Arule is saidto generate a signalwhenthevalue ofthe outputseries
changes. Asignal callsfor a change in a previously recommended market
position. For example a change in output from +1to -1 would call for
closing a previously held long position and the initiation of a new short
position. Outputvalues need not be confined to {+I, -II. Acomplex rule,
whose outputspans the range {+IO, -WI, is able to recommend positions
thatvaryinsize. Forexample, an outputof+10mightindicatethat 10long
positions are warranted, such as long 10contracts ofcopper. Achange in
the output from +10 to +5 would call for a reduction in the long position
from 10contractsto 5(i.e., sell5).
Binary Rules and Thresholds
The simplestrule is onethat has a binary output. In otherwords, its out
put can assume only two values, for example +1 and -1. A binary rule



==================================================
                     PAGE 33                      
==================================================

Objective Rules and Their Evaluation 17
INPUT OUTPUT
Market Time Series
Time of
Series Market Position
~
Time Rule
~
Mathematical Long
c:=>
&
c:=>
,, ,.-- - -1 ''
'
Logical , ,
,
,
, : Time
Time Operators Short 1- •
•
•
•
Etc.
FIGURE... TA rule transforms input time series into a time series of mar
ket position.
couldalso be designed to recommendlong/neutralpositionsorshort/neu
tral positions. All the rules considered in this book are binary long/short
(+1,-I).
An investmentstrategy based on a binarylong/short rule is always in
either a long or short position in the market being traded. Rules of this
type are referred to as reversal rules because signals call for a reversal
from long to short or short to long. Over time a reversal rule produces a
time series of+1's and-1's that representanalternating sequence oflong
andshortpositions.
The specific mathematical and logical operators that are used to de
fine rules can vary considerably. However, there are some common
themes. Onetheme is the notionofa threshold, a criticallevelthatdistin
guishestheinformativechangesinthe inputtimeseriesfrom itsirrelevant
fluctuations. Thepremiseisthattheinputtimeseriesisamixtureofinfor
mationand noise. Thusthe thresholdactsasafilter.
Rules that employ thresholds generate signals when the time series
crosses the threshold, either by the rising above it or falling beneath it.
These critical events can be detected with logical operators called in
equalities such asgreater-than (>) and less-than (<). Forexample, ifthe



==================================================
                     PAGE 34                      
==================================================

18 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
time series is greaterthan the threshold, then rule output = +1, otherwise
rule output=-1.
Athresholdmaybesetatafixedvalue oritsvaluemayvaryovertime
as a result of changes in the time series that is being analyzed. Variable
thresholds are appropriate for time series that display trends, which are
large long-lasting changes in the level of the series. Trends, which make
fixed threshold rules impractical, are commonlyseenin assetprices (e.g.,
S&P 500 Index) and asset yields (AAA bond yield). The moving average
andtheAlexanderreversalfilter, also known asthe zigzagfilter, areexam
ples of time series operators that are commonly used to define variable
thresholds. The operatorsused in the rules discussed in this book are de
tailedinChapter8.
The moving-average-cross rule is an example of how a variable
threshold is usedto generate signalson a timeseries thatdisplays trends.
Thistype ofruleproducesa signalwhenthe timeseries crossesfrom one
sideofitsmovingaverage to the other. Forexample;
Ifthe time series is above its moving average, then the rule output
value =+1, otherwise therule outputvalue =-1.
This isillustratedinFigure 1.2.
Because it employs a single threshold, the signals generated by the
moving-average-cross rule are, by definition, mutually exclusive. Given a
single threshold, there are onlytwo possible conditions-the timesseries
is either above or beloW! the threshold. The conditions are also exhaus
tive (no otherpossibilities).5Thus, itisimpossible for the rule'ssignals to
beinconflict.
Price
Level Time Series
Rule Time
Output
r···················)
1·········:
+1
-1 : .
FIGURE 1.2 Moving-average-cross rule.



==================================================
                     PAGE 35                      
==================================================

Objective Rules and Their Evaluation 19
Rules with fixed value thresholds are appropriate for markettime se
ries thatdo notdisplaytrends. Suchtimeseriesaresaidto bestationary.
There is a strict mathematical definition of a stationary time series, but
here Iam using the term inaloosersenseto meanthataserieshasa rela
tively stable average value over time and has fluctuations that are con
fined to a roughly horizontal range. Technical analysis practitioners often
referto theseseriesasoscillators.
Timeseriesthatdisplaytrends can bedetrended. Inotherwords, they
can be transformed into a stationary series. Detrending, which is de
scribed in greater detail in Chapter 8, frequently involves taking differ
ences or ratios. For example the ratio of a time series to its moving
averagewillproduceastationaryversionoftheoriginal time series. Once
detrended, the series will be seen to fluctuate within a relatively well
defined horizontal range around a relatively stable mean value. Once the
time series has been made stationary, fixed threshold rules can be em
ployed. An example ofa fixed threshold rule using a threshold ofvalue of
75 is illustrated in Figure 1.3. The rule has an output a value of +1when
theseriesisgreaterthan the thresholdandavalueof-1 atothertimes.
Binary Rules from Multiple Thresholds
Aspointedoutearlier, binaryrulesarederived, quite naturally, from a sin
gle threshold because the threshold defines two mutually exclusive and
exhaustive conditions: the time series is eitherabove or below threshold.
However, binary rules can also be derived using multiple thresholds, but
employing more than one threshold creates the possibility that the input
lOa
75
50
25
a
+i .
Rule : :
L !
Output_I :
FIGURE 1.3 Rule with asingle fixed threshold.



==================================================
                     PAGE 36                      
==================================================

20 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
timeseries canassume more than two conditions. Consequently, multiple
thresholdrulesrequire amoresophisticatedlogicaloperatorthanthesim
ple inequality operator (greater-than or less-than), which suffices for sin
gle thresholdrules.
Whenthere are two ormore thresholds, there are more thantwo pos
sible conditions. For example, with two thresholds, an upper and lower,
there are three possible conditions for the input time series. It can be
above the upper, belowthe lower, or betweenthe two thresholds. To cre
atea binaryrule inthissituation, therule isdefined intermsoftwomutu
ally exclusive events. An event is defined by the time series crossing a
particularthreshold inaparticulardirection. Thus, oneeventtriggers one
of the rule's output values, which is maintained until a second event,
which is mutually exclusive ofthe first, triggers the other output value.
Forexample, anupwardcrossingofthe upperthresholdtriggersa +1,and
adownward crossingofthe lowerthresholdtriggersa-I.
Alogical operatorthatimplements this type ofrule is referred to asa
flip-flop. The name stems from the fact that the rule's output valueflips
oneway, uponthe occurrence ofone event, and thenflops the otherway,
upon the occurrence ofthesecondevent. Flip-flop logic can be used with
eithervariableorfixed thresholdrules. Anexampleofarulebasedontwo
variable thresholds is the movingaverage bandrule. See Figure 1.4. Here,
the movingaverageissurrounded byanupperand lowerband. Thebands
maybeafixedpercentageaboveandbelowthemovingaverage, orthe de
viation of the bands may vary based on of the recent volatility of the
times, asisthecasewiththeBollingerBand.6Anoutputvalueof+1istrig
gered byanupwardpiercingofthe upperthreshold. Thisvalueisretained
Price
Rule
Output
I·.....·······]
+1
t....·......····..·....··
. ...................-. ..
L .:
-1
FIGURE 1.4 Moving average bands rule.



==================================================
                     PAGE 37                      
==================================================

Objective Rules and Their Evaluation 21
untilthelowerthreshold ispenetratedinthe downward direction, causing
the outputvalue to changeto-l.
Obviously, there are many other possibilities. The intent here has
been to illustrate some of the ways that input time series can be trans
formed into atinleseries ofrecommended marketpositions.
Hayes7 adds another dimension to threshold rules with di'rectional
modes. Heappliesmultiple thresholdstoastationarytimeseriessuchasa
diffusions indicator. At a given point in time, the indicator's mode is de
fined bythe zoneitoccupiesand its recentdirection ofchange (e.g., up or
down over the most recentfive weeks). Each zone is defined byan upper
and lower threshold (e.g., 40 and 60). Hayes applies this to a proprietary
diffusion indicator called Big Mo. With two thresholds and two possible
directional modes (up/down), six mutually exclusive conditions are de
fined. Abinary rule could be derived from such an analysis by assigning
oneoutputvalue (e.g., +1) to one ofthesixconditions, and thenassigning
the other output value (i.e., -1) to the other five possibilities. Hayes as
serts thatone ofthe modes, when the diffusion indicatoris above 60 and
its direction is upward, is associated with stock market returns (Value
Line Composite Index) of 50 percent per armum. This condition has oc
curred about 20 percent of the time between 1966 and 2000. However,
when the diffusion indicatoris > 60, and its recentchangeis negative, the
market's annualized return is zero. This condition has occurred about 16
percentofthe time.D
TRt\DITIONAL RULES AND INVERSE RULES
PartTwo ofthis bookis a casestudythatevaluatesthe profitabilityofap
proximately 6,400 binary long/short rules applied to the S&P 500 Index.
Manyofthe rulesgeneratemarketpositionsthatareconsistentwithtradi
tional principles oftechnical analysis. For example, under traditional TA
principles, a moving-average-crossrule isinterpretedto be bullish (output
value +1) when the analyzed time series is above its moving average, and
bearish (output value of-1) when it is below the moving average. I refer
to theseas traditionalTArules.
Giventhatthe veracityoftraditionalTAmaybequestionable, itisde
sirable to test rules thatare contrary to the traditional interpretation. In
otherwords, itis entirelypossible thatpatterns thatare traditionallyas
sumed to predict rising prices may actually be predictive of falling
prices. Alternatively, itis possiblethatneitherconfigurationhas anypre
dictivevalue.
Thiscanbeaccomplished bycreatinganadditionalsetofruleswhose



==================================================
                     PAGE 38                      
==================================================

22 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Price
Moving
Average
Traditional Rule
~11
l r---ou--ouLou_ouou__jouououou--ou. •
Time
~t--------
+11f-- .•.
-1 Inverse Rule
FIGURE •.5 Traditional rules and inverse rules.
output is simply the opposite of a traditional TA rule. I refer to these as
inverse rules. This is illustrated in Figure 1.5. The inverse ofthe moving
average-cross rule would output a value of-1 when the inputtime series
is above its moving average, and +1when the series is below its moving
average.
There is yet another reason to consider inverse rules. Many of the
rulestestedinPartTwo utilizeinputseriesotherthan theS&P500, forex
ample the yield differential between BAA and AAA corporate bonds. Itis
not obvious how this series should be interpreted to generate signals.
Therefore, both up trends and down trends in the yield differential were
consideredaspossiblebuysignals. The detailsofthese rules are taken up
inChapter8.
THE USE OF BENCHMARKS IN RULE EVALUATION
In manyfields, performance isa relative matter. Thatis tosay, itisperfor
mance relative to a benchmarkthatis informativeratherthan an absolute
level of performance. In track and field, competitors in the shot-put are
compared to a benchmarkdefined as bestdistance ofthatday orthe best
ever recorded in the state orworld. To say that someone put the shot43
feet does not reveal the quality ofperformance, however ifthe bestprior
efforthad been23feet, 43feetisasignificantaccomplishment!
This pertains to rule evaluation. Performance figures are only infor
mative when they are compared to a relevant benchmark. The isolated



==================================================
                     PAGE 39                      
==================================================

Objective Rules and Their Evaluation 23
fact that a rule earned a 10 percent rate ofreturn in a back test is mean
ingless. Ifmany other rules earned over 30 percent on the same data, 10
percent would indicate inferiority, whereas ifall other rules were barely
profitable, 10percentmightindicatesuperiority.
What then is an appropriate benchmark for TA rule performance?
Whatstandard musta rule beatto be considered good?There are a num
ber of reasonable standards. This book defines that standard as the per
formance of a rule with no predictive power (i.e., a randomly generated
signal). This is consistentwith scientific practice in otherfields. In medi
cine, a new drug must convincingly outperform a placebo (sugar pill) to
be considered useful. Of course, rational investors might reasonably
chooseahigherstandardofperformancebutnotalesserone. Someother
benchmarks that could make sense would be the riskless rate of return,
the return ofa buy-and-hold strategy, orthe rate ofreturn ofthe rule cur
rentlybeingused.
In fact, to be considered good, it is not sufficientfor a rule to simply
beat the benchmark. It must beat it by a wide enough margin to exclude
the possibility thatitsvictory was merely due to chance (good luck). Itis
entirelypossibleforarulewith nopredictivepowertobeatitsbenchmark
inagivensample ofdatabysheerluck. Themarginofvictorythatissuffi
cienttoexcludeluckasalikelyexplanationrelatestothematterofstatis
ticalsignificance. Thisistaken up in Chapters4, 6, and6.
Havingnowestablishedthatthe benchmarkthatwe will use isthe re
turnthatcouldbeearnedbyarule withnopredictivepower, we nowface
anotherquestion: How much mighta rule with no predictivepowerearn?
Atfirst blush, itmightseem thata return ofzero is a reasonable expecta
tion. However, this is only true under a specific and rather limited set of
conditions.
Infact, the expectedreturn ofa rule with no predictivepowercan be
dramatically differentthan zero. This is so because the performance ofa
rule can beprofoundlyaffected byfactors thathave nothingto dowithits
predictivepower.
The Conjoint Effect ofPosition Bias and
Market Trend on Back-Test Performance
In reality, a rule's back-tested performance is comprised oftwo indepen
dent components. One component is attributable to the rule's predictive
power, ifithasany. Thisisthe componentofinterest. Thesecond, andun
wanted, component ofperformance is the result oftwo factors that have
nothingto dowiththe rule'spredictivepower: (1) therule's long/shortpo
sitionbias, and (2) the market'snettrend duringthe back-testperiod.
Thisundesirable componentofperformancecandramaticallyinfluence



==================================================
                     PAGE 40                      
==================================================

24 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
back-test results and make rule evaluation difficult. It can cause a rule
with no predictive power to generate a positive average return or it can
cause arule withgenuinepredictivepowertoproduce anegative average
return. Unless this component ofperformance is removed, accurate rule
evaluation is impossible. Let's consider the two factors that drive this
component.
The first factor is a rule's long/short position bias. This refers to the
amount oftime the rule spent in a +1output state relative to the amount
of time spent in a -1 output state during the back test. If either output
state dominated during the back test, the rule is said to have a position
bias. Forexample, ifmore time wasspentinlongpositions, the rule hasa
longpositionbias.
The secondfactor is the market'snet trend orthe average dailyprice
changeofthemarketduringtheperiodofthebacktest. Ifthemarket'snet
trend isotherthan zero, and the rule has along orshortpositionbias, the
rule'sperformancewillbeimpacted. Inotherwords, theundesirablecom
ponentofperformancewill distortback-testresultseitherbyaddingto or
subtracting from the component ofperformance that is due to the rule's
actual predictive power. If, however, the market's net trend is zero or if
the rule has no position bias, then the rule's past profitability will be
strictly due to the rule's predictive power (plus or minus random varia
tion). Thisisdemonstratedmathematicallylater.
To clarify, imagine a TA rule that has a long position bias but that we
know has no predictive power. The signals ofsuch a rule could be simu
lated by a roulette wheel. To create the long position bias, a majority of
the wheel's slots would be allocated to long positions (+1). Suppose that
one hundred slots are allocated as follows; 75 are +1and 25 are -1. Each
day, over a period of historical data, the wheel is spun to determine ifa
long or short position is to be held for that day. If the market's average
daily change during this period were greaterthan zero (Le., net trend up
ward), the rule would haveapositive expectedrate ofreturneventhough
the signals contain no predictive information. The rule's expected rate of
return can be computed using the formulaused to calculatethe expected
value ofarandomvariable (discussedlater).
Just as itis possible for a rule with no predictivepowerto produce a
positive rate of return, it is just as possible for a rule with predictive
powerto produce a negative rate ofreturn. This can occurifa rule has a
positionbiasthatiscontrarytothemarket'strend. Thecombinedeffectof
the market's trend and the rule's position bias may be sufficientto offset
any positive return attributable to the rule's predictive power. From the
preceding discussion it should be clear that the component of perfor
mance due to the interaction ofposition bias with market trend must be
eliminatedifoneisto develop avalidperformancebenchmark.



==================================================
                     PAGE 41                      
==================================================

Objective Rules and Their Evaluation 25
At first blush, it might seem as ifa rule that has a long position bias
during a rising market trend is evidence of the rule's predictive power.
However, thisisnotnecessarilyso. The rule'sbullishbiascouldsimplybe
due to the way its long and shortconditions are defined. Ifthe rule's long
condition is more easilysatisfied than its short condition, all otherthings
being equal, the rule will tend to hold long positions a greaterproportion
ofthe time thanshortpositions. Sucharule would receive aperformance
boost when back tested over historical data with a rising market trend.
Conversely, a rule whose short condition is more easily satisfied than its
longconditionwould be biasedtowardshortpositions and itwould geta
performanceboostifsimulatedduringa downwardtrendingmarket.
The readermaybewondering how the definition ofa rule can induce
a biastoward eitherlong orshortpositions. This warrants some explana
tion. Recall that binaryreversal rules, the type tested in this book, are al
ways in either a long or short position. Given this, if a rule's long (+1)
condition is relatively easy to satisfy, then itfollows that its short condi
tion (-1) must be relatively difficult to satisfy. In other words, the condi
tion required for the -1 output state is more restrictive, making it likely
that, overtime, the rule will spend more time long thanshort. Itisjustas
possible to formulate rules where the long condition is more restrictive
than the short condition. All other things being equal, such a rule would
recommend short positions more frequently than long. It would be con
trary to our purpose to allow the assessment ofa rule's predictive power
to be impacted by the relative strictness or laxity ofthe way in which its
longandshortconditionsare defined.
To illustrate, consider the following rule, which has a highly restric
tive short condition and, therefore, a relatively lax long condition. The
rule, whichgeneratespositionsintheS&P500index, isbasedonthe Dow
Jones Transportation Average.1O Assume that a moving average with
bandssetat +3 percentand-3percentisapplied to the DJTA. The rule is
to beshortthe S&P500while the DJTAisbelowthe lowerband, bydefin
itionarelativelyrare condition, and longatallothertimes. SeeFigure 1.6.
Clearly, such a rule would benefitifthe S&P were in an uptrend overthe
back-testperiod.
Now let's consider the back test of two binary reversal rules which
are referred to as rule 1and rule 2. Theyare tested on S&P 500 dataover
the periodJanuary 1, 1976through December2004. During this period of
approximately 7,000 days, the S&P 500 had an average daily return of
+0.035 percentperday compounded, or +9.21897 percentannualized. As
sume thatrule 1was ina longstate 90 percentofthe time and rule 2was
in a long state 60 percent ofthe time. Also, suppose that neither rule has
predictivepower-asiftheiroutputvalues were determined bya roulette
wheel with 100 slots. The output for rule 1is based on a roulette wheel



==================================================
                     PAGE 42                      
==================================================

26 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Price
DJTA
~
Rule Time
Output
~
+_11 1----------------i-+-;------------.- -------'--+--------.-.---------------..- -
:.;~....: L..~..l
FIGURE 1.6 Rule with restrictive short condition and long position bias.
with 90slotsassigneda value of+1 and the remaining 10assigned avalue
of-1. Theoutputforrule 2isbasedonawheelwith60slotsassigneda +1
valueand40avalueof-1. BytheLawofLargeNumbers,llitisreasonable
to expectthatoverthe 7,000 days, rule 1will be longveryclose to 90 per
cent ofthe time and rule 2 will be long approximately 60 percent ofthe
time. Although the rules have different long/short biases, they have equal
predictive power-none. However, their expected rates ofreturn will be
quitedifferentoverthissegmentofmarkethistory.
The expected return ofa rule depends upon three quantities; (1) the
proportion oftime the rule spent in long positions, (2) the proportion of
timespentinshortpositions (1 minustheproportionoftimelong) and (3)
the market's average daily price change during the historical test period.
Theexpectedreturn (ER) isgiven bythefollowing equation.
Expected Return
ER [p(L) ADC] - [p(S) ADC]
= x x
Where
p(L)- probability oflong position (proportion long)
p(S)- probabilityofshort position (proportion short)
ADC: average daily change in markettraded
Based on this calculation, the expected return for rule 1is .028 per
l2
centperdayor7.31percentannualized. Theexpectedreturnforrule2is



==================================================
                     PAGE 43                      
==================================================

Objective Rules and Their Evaluation 27
0.007percentperday or1.78percentannualized. Thisdemonstratesthat
13
the rules' historical performance misleads us in two ways. First, both
rules generate positive returns, yet we know that neither has any predic
tivepower. Second, rule 1appearsto besuperiortorule 2eventhoughwe
knowtheyhaveequalpredictivepower-none.
Whentestingactualtradingrules, onewaytoremovethedeceptiveef
fect due to the interaction ofposition bias and market trend would be to
do the following: Subtract the expected return of a nonpredictive rule
withthesamepositionbiasas the rule testedfrom the observedreturn of
the tested rule. For example, assume that we did not know rules 1and 2
hadnopredictivepower. Simplybyknowingtheirhistoricalpositionbias,
90 percentlongfor rule 1and 60percentfor rule 2, and knowingthe mar
ket's average daily return over the back-testperiod, we would be able to
compute the expected returns for rules with no predictive power having
these position biases using the equation for the expected return already
shown. The expected returns for each rule and would then be subtracted
from each rule's observed performance. Therefore, from rule 1's back
tested return, which was 7.31 percent, we would subtract 7.31 percent,
giving a resultofzero. The resultproperlyreflects rule 1's lack ofpredic
tivepower. Fromrule2'sreturnof1.78percent,wewouldsubtractavalue
of 1.78 percent, also giving a value ofzero, also revealing its lack ofpre
dictivepower.
The bottom line is this: by adjusting the back-tested (observed) per
formance by the expected return ofa rule with no predictive powerhav
ing an equivalentposition bias, the deceptive component ofperformance
can be removed. In other words, one can define the benchmark for any
rule as the expected return ofa nonpredictive rule with an equivalentpo
sitionbias.
ASimpler Solution to Benchmarking:
Detrending the Market Data
The procedurejustdescribed can be quite burdensome when many rules
are beingtested. Itwould requirethataseparatebenchmarkbecomputed
for each rule based onits particularpositionbias. Fortunately there is an
easierway.
The easier method merely requires that the historical data for the
marketbeingtraded (e.g., S&P500 Index) be detrended priorto rule test
ing.Itisimportanttopointoutthatthe detrendeddataisusedonlyforthe
purpose ofcalculating daily rule returns. It is not used for signal genera
tion ifthe time series ofthe market being traded is also being used as a
rule inputseries. Signalswouldbegeneratedfrom actualmarketdata(not
detrended).



==================================================
                     PAGE 44                      
==================================================

28 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Detrendingis a simple transformation, which results in a new market
dataseries whose average daily price change is equal to zero. As pointed
outearlier, ifthemarketbeingtradedhasanetzerotrendduringtheback
test period, a rule's position bias will have no distorting effect on perfor
mance. Thus, the expected return ofa rule with no predictive power, the
benchmark, will be zero ifits returns are computed from detrended mar
ket data. Consequently, the expected return ofa rule that does have pre
dictive power will be greater than zero when its returns are computed
from detrendeddata.
To perform the detrending transformation, one first determines the
average daily price change ofthe market being traded over the historical
test period. This average value is then subtracted from each day's price
change.
The mathematical equivalence between the two methods discussed,
(1) detrending the market data and (2) subtracting a benchmark with a
equivalent position bias, may not be immediately obvious. A detailed
mathematicalproofisgivenintheAppendix, butifyouthinkaboutit, you
will see that ifthe market's average daily price change during the histori
cal testing period is equal to zero, then rules devoid ofpredictive power
must have an expected return ofzero, regardless oftheir long/shortposi
tion bias.
Toillustratethispoint, let'sreturntotheformulaforcomputingtheex
pectedvalue ofa randomvariable. You will notice that ifthe average daily
price change ofthe market being traded is zero, it does not matter what
p(long) orp(short)are. Theexpectedreturn(ER) willalwaysbezero.
ER = [p (long) x avg. daily return] - [p (short) x avg. daily return]
For example, ifthe position biases were 60 percent long and 40 per
centshort, the expected returniszero.
0= [0.60) x OJ - [0.40 x 0] Position Bias: 60 percent long, 40 percent short
If, on the otherhand, a rule does have predictive power, its expected
return ondetrendeddatawill begreaterthanzero. Thispositivereturnre
flects the fact thatthe rule's longandshortpositions are intelligentrather
than random.
Using Logs ofDaily Price Ratio Instead
ofPercentages
Thusfar, the returns for rules and the market beingtraded have been dis
cussed in percentage terms. This was done for ease ofexplanation. How-



==================================================
                     PAGE 45                      
==================================================

Objective Rules and Their Evaluation 29
ever, there are problems with computing returns as percentages. These
problemscanbeeliminatedbycomputingdaily returns asthelogsofdaily
priceratioswhichisdefinedas:
La (current day's price)
9 prior day'5 price
The log-based market returns are detrended in exactly the same way
as the percentage changes. The log ofthe daily price ratio for the market
beingtradediscomputedforeachdayoverthe back-testperiod. Theaver
age is found, and then this average is deducted from each day. This elimi
natesanytrend in the marketdata.
OTitER DElf\1LS: THE LOOK-AHEAD BIAS AND
TRADING COS1'S
Itis said the devil lives in the details. When it comes to testing rules, this
truthapplies.Therearetwomoreitemsthatmustbeconsideredto ensure
accurate historical testing. They are (1) the look-al1ead bias and the re
lated issue, assumed executionprices, and (2) tradingcosts.
Look-Ahead Bias and Assumed Execution Prices
Look-ahead bias,14 also known as "leakage offuture information," occurs
in the context of historical testing when information that was not truly
available at a given point in time was assumed to be known. In other
words, the information that would be required to generate a signal was
nottrulyavailableatthe time thesignalwasassumedto occur.
In many instances, this problem can be subtle. If unrecognized, it
canseriouslyoverstate the performance ofrule tests. Forexample, sup
pose a rule uses the market's closingprice orany input series thatonly
becomes known atthe time ofthe close. When this is the case, it would
notbelegitimateto assume thatone could enterorexitapositionatthe
market's closing price. Assuming this would infect the results with
look-ahead bias. In fact, the earliest opportunity to enter or exit would
bethefollowing day's openingprice (assumingdailyfrequency informa
tion). All ofthe rules tested in PartTwo ofthis book are based on mar
ket data that is known as of the close of each trading day. Therefore,
the rule tests assume execution at the opening price on the following
day. This means that a rule's daily return for the current day (dayo) is
equal to the rule's output value (+1or-1) as ofthe close ofdayo multi-



==================================================
                     PAGE 46                      
==================================================

30 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
plied by the market's change from the opening price of the next day
(open day+1) price to the opening price on day after that (open day+2)'
That price change is given as the log of the ratio defined as opening
price ofdaY+2 dividedbythe openingprice on day+l' asshowninthefol
lowing equation:
Poso Log
x
Where:
POSo= Rule's market position as ofthe close ofdayo
0+1= Open S&P 500 on daY+1
0+2= Open S&P 500 on daY+2
This equation doesnotshowthe detrendedversion ofrule returns, as
shownhere:
Log [O+J -
ALR
[O+IJ
Where:
POSo= Rule's market position as ofthe close ofdayo
0+ 1= Open S&P 500 on daY+l
0+2= Open S&P 500 on daY+2
ALR= Average Log Return over BackTest
Look-ahead bias can also infectback-testresults when a rule uses an
input dataseries that is reported with a lag orthat is subject to revision.
Forexample, theback-testofarulethatusesmutualfund cashstatistics,15
which is released to the public with a two-week delay, must take this lag
into account by lagging signals to reflect the true availability ofthe data.
None ofthe rules tested in this book use information reported with a lag
orthatissubjectto revision.



==================================================
                     PAGE 47                      
==================================================

Objective Rules and Their Evaluation 31
Trading Costs
Shouldtradingcostsbetakenintoaccountinrule back-tests?Ifthe intent
is to use the rule on a stand-alone basis for trading, the answer is clearly
yes. For example, rules that signal reversals frequently will incur higher
trading costs than rules thatsignal less frequently and this must be taken
into account when comparing their performances. Trading costs include
brokercommissions andslippage. Slippageis due to the bid-askedspread
and the amount that the investor's order pushes the market's price-up
when buyingordownwhenselling.
If, however, the purposeofrule testingis to discoversignalsthatcon
tain predictive information, then trading costs can obscure the value ofa
rule that reverses frequently. Since the intent of the rule studies con
ducted in this book are aimed atfinding rules that have predictive power
ratherthanfinding rulesthatcan beusedasstand-alonetradingstrategies
itwasdecided notto imposetradingcosts.



==================================================
                     PAGE 48                      
==================================================





==================================================
                     PAGE 49                      
==================================================

The Illusory
Validity of
Subjective
Technical
Analysis
The difference between a crank and a charlatan is
the charlatan knows he is dealing in snake oil, the
crankdoes not.
-MartinGardner
T
hechapterhas twopurposes. First, itisintendedto encourageanat
titude of skepticism toward subjective TA, a body of propositions
that are untestable because they lack cognitive content. Second, it
underscorestheneedfor arigorousandobjectiveapproachto knowledge
acquisition, to combat the human tendency to form and maintain strong
beliefs in the absence ofsolid evidence or even in the face ofcontradic
toryevidence.
Besides what we take on faith, most of us are under the impression
that our beliefs are justified by sound reasoning from good evidence. It
can be said that we know something when we have a belief that is true
and we hold it because we have drawn a correct inference from the right
evidence.l We know that ice cream is cold, gravityis real, and some dogs
bite, on the basis offirst-hand experience, butwithout the time or exper
tise to acquire all requisite knowledge directly, we willingly accept wis
domfrom secondhandsourceswe deemreliable. Howeverwecomebyit,
wedonotadopt knowledgewilly-nilly, orsowebelieve.
Unfortunately, thisbeliefand manyothersthatweholdareerroneous.
Without realizing it, by a process that is as automatic as breathing, we
adoptallsortsofbeliefswithoutrationalthoughtorreliableevidence. Ac
cordingto agrowing bodyofresearch, thisisdue to avarietyofcognitive
errors, biases, and illusions. This is a serious liability, because once a
33



==================================================
                     PAGE 50                      
==================================================

34 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
( )
B
)>-------«
A
FIGURE 2.1 Senses can deceive us.
falsehood isadopted, ittendstopersistevenwhen newevidenceshowsit
to bewrong. This longevityis also attributable to variouscognitive errors
thatlead usto defendexistingbeliefs.
A visual illusion is an example of an erroneous belief that persists
even after it has been pointed out. In Figure 2.1, line segmentAappears
longer than segment B, but ifyou apply a ruler, you will see they are of
equal length. However, this knowledge does not undo the illusion. Seeing
canbedeceiving, and the deceptionlasts.
Figure 2.22 depicts another visual illusion. The table top on the right
appears more elongated. Ifyou compare them you will see they have the
same dimensions.
Under normal circumstances, sensory impressions as interpreted by
the brain produce accurate beliefs about the world. The selective pres
sures of evolution assured the development of such a vital capability.
However, as well adapted as the eye/brainsystem is, itis notperfect. Un
der adverse conditions, conditions outside ofthose thatshaped its evolu
tion, thesystemcanbefooled.
Just as there are illusory perceptions, there is illusory knowledge. It
seemsvalid, but it is not, and similarto perceptual illusions, false knowl
edge tends to occurin situations beyond those that shaped the evolution
R
FIGURE 2.2 Both table tops have same size and shape.
''Turning the Tables" from MIND SIGHTS by Roger N. Shepard. Copyright © 1990
by Roger N. Shepard. Reprinted by permission ofHenry Holt and Company, LLC.



==================================================
                     PAGE 51                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 35
of our cognitive abilities. Under such adverse conditions, our generally
successfullearningstrategiesfail, and we cometo "know"whatisnotSO.3
Over the last 30 years, cognitive psychologists have discovered that
erroneous knowledge is often the result ofsystematic errors (biases) in
the way we process information about complex and uncertain situa
tions. Financial market behavior is complex and uncertain, so it should
be no surprise that informal (subjective) analysis would produce illu
soryknowledge.
Asystematicerror, unlikea random error, occurs overand overagain
in similarsituations. This is good news because itmeans the erroris pre
dictable and steps can be taken to avoid it. The first step is realizing that
sucherrorsare common.
SUBJECTIVE 'fA IS NOT LEGITIMATE KNOWLEDGE
As defined in Chapter 1, subjective TA is any method ofanalysis that, be
cause ofvagueness, cannot be defined as an algorithm that can be back
tested by computer. Though the subjective domain ofTA is comprised of
many different methods, they share a faith thatvalid knowledge aboutfi
nancial markets can be discovered in an informal and nonscientific man
ner. Icontendthisfaith ismisplaced.
As discussed in the Introduction, subjective TA cannot be called
wrong, becausetocalla methodwrongimpliesithasbeentestedandcon
tradicted by objective evidence. Subjective TA is immune to empirical
challengebecauseitisuntestable.Thus, itisworsethanwrong;itismean
ingless.
Beliefs thatcannotbetested mustbe accepted onfaith, anecdotalev
idence, orauthoritarianpronouncement. Inthisway, subjectiveTAissim
ilarto practices such as New-Age medicine, astrology, numerology, and a
hostofotheruntestable claims, none ofwhich can legitimately be classi
fied asknowledge.
However, many TA practitioners and consumers believe strongly in
the validity ofsubjective methods, and ifasked, no doubt eachwould as
sertthat theirbeliefisjustifiedandsupportedby evidence. I contend this
is a consequence ofthe same cognitive flaws that underlie erroneous be
liefs in general and the unscientific manner in which traditional TA is
practiced.
TA is certainly not the only field to suffer the ill effects ofnonscien
tific methods. Medicine and psychology, where the penalties for faulty
wisdom are much greater, are also burdened with erroneous knowledge.
Thishasspawnedanemergingmovementcalledevidence-basedmedicine



==================================================
                     PAGE 52                      
==================================================

36 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
thatappeals to doctors to restrictthemselves to practiceswithproven ef
ficacy. There isasimilarmovementinpsychotherapy. Ajournalcalledthe
Scientific Review ofMental Health Practicrt! describes itsmission as rid
ding the field ofunsubstantiated and untested treatments. Unfortunately,
it is unlikely that traditional doctors and psychotherapists will abandon
theiringrainedways. Oldbeliefsare hard to change. However, newpracti
tioners may be persuaded, and it is their patients who will benefit from
methods withprovenefficacy.
Although this chapter is a critique of subjective TA, objective TA is
also vulnerable to erroneous knowledge. However, it comes about in an
entirely different manner. Whereas subjective TA suffers from a lack of
quantitative evidence, objective TA suffers from faulty inferences drawn
from quantitative evidence. This problem and possible solutions are ex
ploredin Chapter6.
A PERSONAL ANECD01'E: FIRS1' A TRUE 11\ BEUEVER,
THEN A SKEPTIC
A book on the necessity of quantitative objective evidence may be a
strange place for personal anecdotes. Nevertheless, Ioffera firsthand ac
count ofmy initial conversion to a true believerin TA, myfall from faith,
and my rebirth as a scientific skeptic who believes TA can offer value
when researchedina disciplined manner.
Ifirst learned ofTA as a teenagerand wanted to believe its claims
were true. Making money from chart patterns seemed magical yet
plausible. Until that time, my earnings were derived from summers
spenttending lawns, digging clams on Long Island's North Shore, and
working on a garbage truck at a local resort. Though I've never shied
away from physical labor, the idea of making money without sweat
had great appeal.
Like mostTAtrue believers, my initial convictions camesecondhand,
reading the revealed wisdom of the field's self-proclaimed authorities. It
neveroccurredtome thattheirclain1smightnotbegroundedinsound re
search. Theveryterm technical analysis hadascientificringtoit. Laterit
became clearthat in manyinstances the authorities were not basingtheir
claims onfirsthand research butmerely regurgitating what they had read
from even earlier self-proclaimed experts. Humorist and philosopher
Artemus Ward has said, "It's not so much the things we don't know that
getus into troubleas the things we knowthatjustain'tso."
Myfirst read wasHow IMade Two Million Dollars in theStock Mar
ketbyNicholasDarvas,aprofessionaldancer. Heattributed hissuccessto



==================================================
                     PAGE 53                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 37
a chartingmethod called the BoxTheory. As myfirst exposure to TA, the
idea that buy-and-sell signals could be derived from a stock's behavior
was entirelynovel and exciting. Darvas based his analysis on a staircase
like arrangementofprice ranges thathe calledboxes. Soon Ilearned this
was oldwine in a new bottle-the good old-fashioned support and resis
tance zones ofTA. My enthusiasm was undeterred. Next I studied Chart
craft's Point and Figure Method and began keeping charts on a large
stable ofstocks. Formysixteenthbirthdaymyparentsgave me the bible,
TechnicalAnalysis ofStock Trends, byEdwardsand Magee, and mycon
versionwascomplete.
Beforelong, Iwasgivingstocktipstomychemistryteacher, Mr. Com.
My initial success with a stock called Cubic made several other teachers
fans ofmymysterious charts. Ipicked several more winners. No one, not
even I, considered the possibility that these early successes might have
beennothing more thanarun ofgoodluck. Therewas no reasonto. None
ofthe TAbooks Istudied everdiscussed the roles ofrandomness in mar
ket behavior or luck in the performance of an investment method. My
early scores were plausible because they were consistent with the anec
dotes that filled the books, but it did not last. My track record began to
blemish and myfans faded. However, myenthusiasm for TAcontinued to
grow because it was always possible to explain away my failures. After
the fact, Icould always see where Ihad misread the chart. I'd get it right
nexttime.
In high school Iwas a good science student, although I now realize I
wasscientificallyilliterate. Later, college courses in the philosophyofsci
encetaughtmethatscienceismorethanasetoffacts. Firstandforemost,
it is a method for distinguishing real from illusory knowledge. It would
take me the betterpartof40years toseethe connectionbetweenthephi
losophyofscienceand TA.
Myskepticismgrew outofmy experience as a proprietary or"prop"
trader for the firm of Spear, Leeds & Kellogg from the fall of 1996
through the spring of2002. Prop traders have the ultimate funjob. They
are giventhefreedom tospeculatewithafirm's capital. Mytradingstrat
egy was based on whatIhad learned aboutTAoverthe prior35 years. I
made profits during the first three and one-half years, October 1996
through February 2000. Because my trading methods were subjective, it
is not clear ifthese gains were the result ofmy abilities with TA or my
ever-bullishbias coincidingwithan upward-trending market. Isuspectit
was the latter because an analysis of my monthly returns relative to a
market benchmark indicated I was not beating the market but merely
matching it. In other words, I had not generated a significantly positive
alpha.5Also suggestive was the fact that I gave back all my gains in the
two years after the market trend turned down in March 2000. Over the



==================================================
                     PAGE 54                      
==================================================

38 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
full five-and-one-half-year period, my results, to put it generously, were
lackluster.
Prior to joining Spear, Leeds & Kellogg, I had been a proponent of
objective trading methods, so while at Spear, I made efforts to develop
a systematic trading program in hopes that itwould improve my perfor
mance. However, with limited time and development capital, these
plans never came to fruition. Thus, I continued to rely on classical bar
chart analysis, supplemented with several indicators that I interpreted
subjectively.
However, I was objective in several ways. Early in my trading career
withSpear, Ibegankeepinga detailedjournal. Priorto eachtrade Iwould
note its TArationale. In addition, each trade was based on an objectively
falsifiable prediction-a defined point of adversity where I would admit
the trade was wrong. My managersinsistedonthis. Imaintainedthejour
nal daily over the five-plus years of trading. After each transaction was
completed, Idida postmortemanalysis. Thispracticemadeithardforme
to rationalize myfailures. There theywerestaringmeintheface. This ac
celeratedmyfall from faith insubjectiveTA.
Because my results were for onlyone person, Iconsidered the possi
bility I was not properly implementing the dictates ofthe TA texts I had
been studying for the past 30+ years. Yet, I also began to wonder ifwhat
theysaidwascorrectorifwhattheysaidwasevensubstantive.
When I discussed my growing skepticism with colleagues, their re
sponse was thatTAwas obviouslyvalid. Trends and patternsin historical
charts were simply too evident to be illusory. Awidely used TA text6 as
serts that TA is valid on these grounds. This did not satisfy me. When I
learned that the same patterns and trends,7 to which TA attributes such
significance, alsoappearwithregularityinpurelyrandom data, myfaith in
chartanalysis was shaken to the core. Moreover, it came to my attention
that studies have shown that expert chart readers cannot reliably distin
guish actual market charts from charts produced by a random process.s
Such charts, which are generated by random draws from a sample ofac
tualprice changes, are, bydesign, devoidofauthentic trendsandpatterns
and, therefore, impossible to predict, but, to experienced chartists, they
looked just like authentic price charts. On this basis, it would seem that
predictionsbasedonauthenticchartscould notbetrusted. Clearly, "obvi
ous validity" is an inadequate standard forjudging the validity ofmarket
patterns.
With inspiration from two books, How We Know What Isn't So by
ThomasGilovich,9and WhyPeopleBelieve Weird Things byMichaelSher
mer,1OIcametothe realizationthatTAmustbeapproachedwiththeskep
ticismand rigorofthescientificmethod.



==================================================
                     PAGE 55                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 39
TIlE MIND: A NATURAL PATTERN FINDER
Wearepredisposedto lookforandfind predictivepatterns. Humannature
is repulsed by the unpredictable and the unexplained, so our cognitive
machinery tends to perceive order, pattern, and meaning in the sensory
stimuli we receive from the world regardless of whether it truly is or
dered, patterned, ormeaningful. "In many instances, whatwe experience
is nothing more than the vagaries ofchance at work." "The appearance
II
ofa face on the surface ofthe moon, the perception ofSatanic messages
when rock music is played backwards, or seeing the face ofJesus in the
woodgrainofahospitaldoorareexamplesofthemindimposingorderon
randomvisualsensorystimuli."12
The tendency to perceive order evolved because this ability was cru
cialfor oursurvival.13The earlyhumans who were bestatitproducedthe
most offspring, and we are descended from them. Unfortunately, evolu
tion did not endow us with an equal ability to distinguish valid from in
valid patterns. AE a consequence, along with the valid knowledge came
manyfalsehoods.
Acquiring knowledge, whether in prehistoric times or today, can fail
in two ways: learning a falsehood orfailing to learn a truth. Ofthese two
mistakes, we seem to be more prone to adopting falsehoods than we are
to overlooking vital truths. Evolutionary biologists speculate that the ac
quisition of false ideas was less detrimental to the survival of early hu
mans than failing to learn something vital. The beliefthat a ritual dance
prior to a hunt would promote success is a learned falsehood with mini
malcost-alittlewasted dancing-butthefailure to learnthe importance
ofstanding downwind ofan animal during a huntwasan error with a sig
nificantsurvival cost.
AE a result, our brains evolved to have a voracious, although indis
criminate, appetite for predictive patterns and causal relations. The false
beliefs thataccumulated along the way were a priceworthpaying. Some
times, a successful hunt did follow a ritual dance, so the superstitious
practice was reinforced. Also, performinga ritual reducedanxiety bygiv
ing an illusory sense ofcontrol over outcomes that were largely a matter
ofchance.
Modern civilized life has changed this picture considerably. Not only
aredecisionsmore complextoday, butthe costsoflearnedfalsehoods are
greater. Consider the debate on global warming. Are rising temperatures
indicative of a real long-term danger or are they a false alarm? Ifthose
convinced that it is a false alarm are later proven wrong, future genera
tionswillpaydearly. However, thecostoftreatingglobalwarmingasase
riousthreatwould bemore modestifthatviewturns outto bemistaken.



==================================================
                     PAGE 56                      
==================================================

40 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The evolution ofhuman intelligence has been a slow process. Ourin
tellectualcapacitiesdevelopedovera several-million-yearperiod, mostof
which took place under conditions referred to as the ancestral environ
ment of adaptation. In this environment, the mandates were few and
clear: Survive and reproduce. Human intellectual capacities and thinking
strategies were adapted for those conditions, not for the complexities of
modem civilization, which date back a mere 10 to 15 thousand years. In
otherwords, 99 percentofhumanevolutiontookplaceinanenvironment
dramaticallyless complexthan the oneweface today. Itis notsurprising,
then, thatourintelligencewould be maladaptedto the complexjudgment
and decision-making tasks we face today. We seem to be as superstitious
todayasthe cave dwellerwhobelievedinritualdances.
THE EPIDEMIC OF WEIRD BELIEFS
Avoracious butindiscriminate appetitefor knowledge inevitablyleadsto
weird beliefs. Our susceptibility to the adoption offalse beliefs was evi
denceddramaticallyina 1990Gallupsurveyof1,236adultAmericans.The
proportion of people believing in all manner of nonsense including the
paranormalisfrightening. Thesearestatisticspresented byShermer.
14 15
Astrology52percent
Extrasensoryperception46percent
Witches 19percent
Aliensfrom spacelandingonearth22 percent
ApreviouslyexistingcontinentcalledAtlantis33percent
Humansand dinosaurs livingatthesametime41 percent
Communicatingwiththespirits ofthe dead42 percent
Ghosts35percent
Personalpsychicexperiences67percent
AstronomerCarl Sagan16lamented that more peoplebelieve in astrol
ogythanthe theoryofevolution. He attributed this widespreadirrational
ity and superstition to a high rate of scientific illiteracy, which surveys
estimate to be over 95 percent. "In such an atmosphere, pseudo science
thrives. Strange beliefs, such as New-Age health practices, are supported
in ways that purport to be scientific but really are not. The evidence of
feredisinsufficient, andfactsthatpointinotherdirectionsaregivenshort
shrift. However, the ideastheyoffersurviveandthrivebecausetheyspeak



==================================================
                     PAGE 57                      
==================================================

The JIIusory Validity ofSubjective TechnicalAnalysis 41
to powerful emotional needs that science often leaves unfulfilled."17 As
Sagan said, it's no fun to be skeptical. It is a burdensome attitude that
leavesourneed tobelieveinfun and comfortingideas, likethe toothfairy
andSantaClaus, unsatisfied.
COGNITIVE PSYCHOLOGY: HEURISTICS, BIASES,
AND ILLUSIONS
Cognitive psychology is concerned with how we process information,
draw conclusions, and make decisions. Itstudiesthe mentalprocessesby
which sensory input is transformed, reduced, elaborated, stored, and re
covered.I8
Overthe last30 years cognitive psychologies have beeninvestigating
the originsofunreliable knowledge. Thegoodnewsis thatcommonsense
and intuitive interpretations of experience are mostly correct. The bad
news is that human intelligence is maladapted to making accurate judg
ments insituations characterized by uncertainty. Underconditions ofun
certainty, intuitive judgments and informally acquired knowledge are
often wrong. Becausefinancial market behavioris highly uncertain, erro
neous knowledgeinthis domainisto beexpected.
The pioneering research ofDaniel Kahneman, Paul Slovic, and Amos
19
Tversky showed that illusory knowledge originates in two ways. First,
people are plagued by various cognitive biases and illusions that distort
what we experience and how we learn from that experience. Second, to
compensateforthemind'slimitedabilitiestoprocessinformation, human
intelligence hasevolvedvarious mental shortcuts calledjudgmentheuris
tics. These rules of thought, which operate quite automatically beneath
our conscious awareness, are the basis of our intuitive judgments and
probabilityassessments. Theyare amarkofhumanintelligence crucialto
everydayliving. Though these quick and dirty rules ofthinkingare gener
allysuccessful, in certain kinds ofsituation they cause us to make biased
decisionsandacquireerroneousknowledge.
Erroneous knowledge is especially problematic because of its re
silience. Studies have shown that once a beliefhas been adopted, it can
survive the assault of new evidence contradicting it or even a complete
discreditingofthe originalevidencethatledtothebelief'sformation.
The sections that follow will discuss a variety ofcognitive errors re
sponsible for erroneous knowledge. For purposes of presentation, each
will be discussed separately. In actuality, however, they operate con
jointly,feeding intoand offoneanother, creatinganillusionofvalidityfor
subjectiveTA. ThisisillustratedinFigure2.3.



==================================================
                     PAGE 58                      
==================================================

42 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Overconfidence,
Self-Attribution Hindsight
Optimism,
Bias Bias
Bias
Knowledge
Illusion
Illusion
of Illusory
Control Correlation
Biased Illusory Sample
Secondhand Trends Size
Knowledge & Neglect
Patterns
Representativeness
Heuristic Bias
FIGURE 2.3 Illusion ofvalidity.
HUMAN INFORMATION PRO~ESSING LIMI1f\TIONS
Despiteitsawesomepower,the information-processingcapabilitiesofthe
human brain are limited. Nobel Laureate Herbert Simon called this the
principle ofboundedrationality. "The capacityofthe humanmindfor for
mulatingand solving complexproblems is very small compared withsize
ofthe problemswhose solutionis required."20 Consequently, we attend to
onlya small fraction ofthe torrent ofinformation the world presents and
processitinsimplisticways.
The number of separate chunks of information that can be held in
consciousmemoryata given time is estimatedto be seven, plus orminus
21
twO. Even morelimitedisthemind'sabilitytohandleproblemsrequiring
configural thinking. A configural-thinking problem requires that a multi
tudeoffactors (variables)beconsideredsimultaneouslyasaninseparable
configuration. Research indicates that the mind is only able to handle a
maximum of three factors when they must be evaluated in a configural



==================================================
                     PAGE 59                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 43
22
fashion. Medical diagnosis is typically a configural-thinking problem. A
set ofsymptoms and lab results, ifconsidered conjointly, can distinguish
onediseasefrom another, buttaken inisolationtheymaybenothingmore
thanasetofdisconnecteduninformativefacts.
Configural thinking is a more demanding mode of thought than se
quential or linear thinking. In a sequential/linear problem, the relevant
variables can be analyzed independently, so even though there may be a
multitude of variables, the message conveyed by each variable is unaf
fected bywhatothersvariables are saying. Therefore, once each variable
has been interpreted on its own, the set of individual messages can be
combined in a linear fashion (i.e., added algebraically23), to derive their
collective meaning. Suppose, for example that a sequential problem in
volves seven variables, each ofwhich can assume a value ofeither +1or
-1. Further, assume that five ofthe variables have the value +1 and two
24
have the value -1. The linearoradditive combination would be equalto
+3 or [(+5) + (-2)]. Research has shown that, when experts make multi
factored decisions in a subjective fashion, they primarily rely on a linear
combining rule,25 though they do it less effectively than formal linear re
26
gression models. These studies have shown thathuman experts are less
effective than linear regression models because they fail to combine the
informationin the consistentmannerofaformal mathematicalmodel.
The combining ofinfonnation in a linearfashion thatsatisfies the re
quirementsofasequential-thinkingproblemfalls shortwhenthe problem
demandsa configuralsolution. Ina configuralproblem, the relevantinfor
mationiscontainedin the webofrelationships (interactions) betweenthe
variables. This means that the variables cannot be evaluated in isolation
as they can in a sequential/linearproblem. To clarifywhat is meant by in
teractions, imaginea configuralprobleminvolvingonlythreevariables,A,
B, and C. Further suppose that each variable can assume only two read
ings, high orlow (i.e., the variablesare binary). Ina configuralproblem, a
highreading onfactorA maymean onethingwhenB islowand Cis high,
whereas a high reading onA can mean something entirelydifferentwhen
B is high and C is low. In a sequential/linear problem, a high reading on
factorA carriesthesamemessageirrespectiveofthereadings onB and C.
The difference between the two configurations-Configuration 1 (A
high,B-Iow, C-high) and Configuration2 (A-high,B-high, C-Iow)-are illus
trated in the Figures 2.4 and 2.5. The eight cells ofthe three-dimensional
spaceshowthat there are eightdistinctlydifferentpossible configurations
of three binary variables. However, a linear combination of three binary
variablescanonlyassumefour7distinctvalues.
The type ofthinking problemfaced bya subjective marketanalystat
tempting to make a forecast by combining the readings onfive indicators
(variables), a relatively modest nun1ber, is likely to be configural rather



==================================================
                     PAGE 60                      
==================================================

44 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
A-High, B-Low, (-High
High
CI'---+-+--
Low
Low High
A
FIGURE 2.4 Configuration 1.
than sequential.28 Simply determining which ofthe five indicators should
be combined (some may be redundant or irrelevant) to produce an
informativepredictionisa hugeconfiguralprobleminitself. Withonlyfive
indicators,thepredictivepowerof26possiblecombinationsmustbeeval
uated (10 pairs, 10triplets, 5quadruplets, and 1quintuplet). Then, once a
good combination has been identified, applying the multi-indicator rule
would also entail configural thinking. As seen earlier, even if the indica
tors are binary, the simplest possible, 3 binary variables can have eight
distinct configurations. Four binaries have 16 possible distinct configura
tions and 5binaryvariables have32. Subjective analystswho believe they
A-High, B-High, (-Low
High
Ct<--+-+---JL---a__-y
Low
Low
A
FIGURE 2.5 Configuration 2.



==================================================
                     PAGE 61                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 45
canperformsuch a feat are too confident-waytoo confident! Thisis not
surprising. Itis merelyone exampleofthe overconfidencebias.
TOO DANG CERTAIN: THE OVERCONFIDENCE BIAS
Ingeneral people are too confident. The overconfidence bias is the docu
mented29 tendencyfor people'sself-assessments to erronthe highside of
the truth. Theytend to view theirpersonal attributes and abilities as bet
ter than objective evidence would indicate. People view themselves as
aboveaverageinmanyways, includinghowmuch they lrnowandhowac
curately they lrnow it. In other words people tend to be arrogant about
theirlrnowledge.
Thetendencyisso pervasive thatpsychologiststhinkoverconfidence
is inborn, and they are quite confident about that. Biologists have specu
lated that high confidence is a feature ofsuccessful mating strategies, so
we are most likely the offspring of swaggering parents. High confidence
confers benefits onhumanityas a whole, evenifthe daring individual try
ing something new and dangerous dies. After enough attempts, someone
succeeds, and we all benefit. In addition, overconfidence is reinforced by
yetothercognitivebiasesthatdistortwhatislearnedfrom experience, in
cluding the self-attribution bias, the confirmation bias, and the hindsight
bias, whicharediscussed later.
Columbia University psychologist Janet Metcalfe summed up re
searchonhumanoverconfidencewiththese humblingwords:30
People think they will be able to solve problems when they won't;
they are highly confident that they are on the verge ofproducing the
correct answer when they are, infact, about to produce a mistake;
they think they have solved problems when they haven't; they think
they know answers to information questions when they don't; they
have the answeron the tip oftheir tongue when there is no answer;
they think they produced the correct answer when they didn't, and
furthermore, they say they knew it all along; they believe they have
mastered learning materialwhen they haven't; they think they have
understood, even though demonstrably they arestillin the dark.
''31
Somespecificfindings aboutoverconfidence:
• Overconfidenceismostextremefordifficultorimpossibletasks.32Stud
ies haveshown extreme overconfidence in taskssuchaspredictingthe
outcome of horse races,33 distinguishing European from American



==================================================
                     PAGE 62                      
==================================================

46 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
handwriting, and predicting rising versus falling stock prices. In the
domainofTA, theanalogoustaskwould bethatofcombininga multi
tude ofindicatorreadings into a marketforecast bysubjectiveconfig
ural reasoning.
• Overconfidence is related to an inability to properly assess the diffi
culty ofvarious tasks and is more pronounced the more difficult the
task. TA analysts may not be aware of the configural reasoning de
manded bysubjective dataanalysis.
• Confidence increases as the number ofpieces ofinformation are in
creased, thoughprediction accuracydoes not.34 Increased confidence
would only be justified if the individual bits of information were
nonredundant, were useful, and could besuccessfullyintegrated.
• Confidenceincreasesasthe levelofagreementamongmultipleinputs
increases. When inputs are independent (uncorrelated) and have pre
dictive information, increased confidence may be justified. When in
puts are redundant, it is not. Many TA indicators that appear to be
distinct because ofnaming conventions and specific calculations ac
tually measure similar market attributes. This can encourage unwar
ranted confidence.
• Doctorswho were 88 percentconfidentthey had correctly diagnosed
pneumoniawere correctin only 20 percent ofthe cases.35 Physicians
weresimilarlyoverconfidentindiagnosingskullfractures.36
• Many other professionals were overconfident about their specific
areasofexpertise. Forexample, when managers ofa chemicalcom
pany were 90 percent confident about company facts (i.e., they ex
pected to be wrong only 10 percent of the time) they were right
only 50 percent of the time.37 Executives of a computer company
were 95 percent confident about general business facts but were
correct80 percent ofthe time. With respect to specifics about their
own company, they were 95 percent confident but correct only 58
percent ofthe time.38
• The overconfidenceofWallStreet'sanalystswithrespectto theirabil
ity to forecast quarterly earnings is attested to by the frequency of
earnings surprises. According to one study, average forecast error
was 44 percent. This statistic was based on over 500,000 individual
earnings predictions.39 People are surprised when outcomes fall out
side ofpredicted ranges, and setting ranges too narrowly is a sign of
overconfidence.
• Individualinvestors are overconfidentintheirability to predictshort
term market trends and pick stocks that will do better than the mar
ket, asevidencedbythe leveloftheirtradingactivity.40
• Wall Streetstrategists exhibit overconfidence in the forecasts for the
general stock market and are frequently surprised by actual out-



==================================================
                     PAGE 63                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 47
comes. However, they maintain theirconfidenceinspite ofthese mis
takes.41 If strategists were indeed learning to curb their confidence,
they would make subsequentpredictions with wider bands ofuncer
tainty, and outcomes would be less likely to fall outside of those
bands. Theydo notappearto do this.
Given the pervasiveness ofoverconfidence, itis likely thatpractition
ers ofsubjective TA would be afflicted with itas well. Itseems likely that
their overconfidence would manifest with respect to three areas; (1) the
predictive power of specific methods, (2) the efficacy of subjective data
analysis and informal inference as a means of knowledge discovery, and
(3) the ability toperform configuralreasoning as insynthesizinga market
forecastfrom amultitudeofindicatorreadingandpatterns. Boththecom
plexity of the financial markets and the known limits of unaided human
intelligence suggest that high confidence with respect to any of these
areasisunwarranted.
According to one study, there are two forecasting professions that
have managed to avoid overconfidence: meteorologists and horse-race
handicappers. Their estimates of their predictive abilities are well cali
bratedbecause: "Theyface similarproblemseveryday; theymakeexplicit
(falsifiable) probabilistic predictions; and they obtain swift and precise
feedbackonoutcomes. Whentheseconditionsarenotsatisfied, overconfi
denceshould be expectedforboth expertsand non-experts.
"42
It is impossible for subjective TA practitioners to obtain accurate
feedback in the context of historical research simply because their
methods for generating forecasts and evaluating forecast accuracy are
not well defined. Feedback is only possible with objective methods.
However, inthe contextofreal-timeforecasting, subjectivepractitioners
could obtain feedback ifthey were willing to make falsifiable forecasts.
Afalsifiable forecast is one that has cognitive content. That is to say, it
sets up a clearly discernable difference between the outcomes that
would indicate the forecast had been correct and outcomes that would
indicate the forecast has been in error. Considerthe following as an ex
ample ofafalsifiable forecast:
The S&P500 will be higher twelve monthsfrom today, and will not
decline more than20percentfrom currentlevels within that time.
Ifthe marketisnothigher12monthshenceorifitdropsmorethan 20
percent from the current level within the next 12 months, it is clear that
the forecast was in error. Unfortunately, falsifiable forecasts are rarely
given by subjective practitioners, thereby preventing feedback for them
and theircustomers. Overconfidencepersists.



==================================================
                     PAGE 64                      
==================================================

48 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
It's Going to Be Great: Optimism Bias
The optimism basis is overconfidence that extends to a generalized and
43
unwarrantedhopefulness aboutthefuture. Asa consequence, mostpeo
plethinktheirliveswillbesprinkledwithmorefavorable andfewerunfa
vorable eventsthanthe livesoftheirpeers.
The optimism bias suggests that TA analysts' beliefs about their pre
dictive prowess will persist even if their prior predictions have not
worked out well. This is borne out by data provided by the Hulbert Di
gest,44 a newsletter that tracks the performance ofmarket forecasters by
translating their recommendations into a portfolio whose value can be
tracked. Some newsletter writers have attempted to explain away their
poor performance ratings by claiming that Hulbert's evaluation methods
areflawed. Accordingto HUlbert, these claimsare leveledprimarilyatthe
assumptions he is required to make to turn a newsletter's vague recom
mendations into specific recommendations whose performance can be
measured. Manynewsletterstracked byHulbertgiveclearadvice, making
suchassumptionsunnecessary.
Despite theirdismallong-term performance, newsletterwriters con
tinue to makeconfidentpredictions, andsubscribers continueto believe
theywillbehelpful. Bothgroupsareoptimisticallybiased.Ifsubscribers
were realistic, they would cancel their subscriptions and the writers
couldnotcontinue in business. Obviously, something otherthan predic
tivesuccess keepsfaith alive. Onepossible explanationisthatbothsub
scribers and newsletter authors have bought into a story that explains
why the underlying method should work, even if it does not work in
practice. As is explained later, a compelling story can overcome statisti
cal evidence because the mind has evolved a greater taste for stories
thanabstractfacts. (Seethesecondhandinformationbiasandthepower
ofnarratives.)
Self-Attribution Bias: Rationalizing Failure
Thetendencytoward overconfidenceisamplifiedbyotherbiasesthatdis
tortwhatwe learnfrom experience. Theself-attributionbiasrefers to the
tendency to interpretpastsuccesses and failure in a biased and self-serv
ing manner. "Numerous studies across a wide range of situations have
found that people attribute positive outcomes to their abilities while at
tributingfailures to external circumstances."45Asa result, we come away
with a falsely optimistic assessment of our strategies and abilities.
Whereas the feedback of past failures might motivate change and im
proved performance, self-serving interpretations short-circuit such learn
ing. This may explain how newsletter writers can maintain confidence in



==================================================
                     PAGE 65                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 49
the face of performance that, by any objective standard, would be re
gardedasnegative.
In addition to making us feel good, self-serving explanations make
intuitive sense. When we exert ourselves and succeed, the causal chain
linkingeffortto agoodoutcome iseasyto understand and hasthe ring of
truth. Attributingsuccesstogood luckorsomeotherfactornotunderour
controlnotonlyhaslessemotionalappeal,italsoseemslessplausible. We
all preferthe plausible. However, a failure despite our best efforts seems
more likely to be attributable to bad luck. Common sense tends to over
look the role ofluck (randomness) in favorable outcomes. That's thejob
ofthestatistician.
Studiesoftheself-attributionbiasamonggamblersare revealing.46By
evaluatingpastlossesinabiasedmanner, theyareabletomaintainafaith
intheirabilitiesintheface ofgrowinglosses. Surprisingly, pastlossesare
notignored. Actuallythe gamblersdevote agreatdeal ofcognitive energy
to failures thereby recasting them in a more favorable light. Losses are
treatedasnearwinsorattributedtobadluck. Wins, ontheotherhand, are
credited to genuine betting skill.47 In the end, both wins and losses wind
up feeding thegambler'ssense ofcompetence.
AsatraderIfrequentlyheardself-servingrationalizations. Iwasguilty
ofitmyselfuntil Istarted keepinga trading logearly in my trading career
with predefined exit points where I was forced to admit my earlier fore
castwasinerror. Eventhough myforecasts were based onsubjectiveTA,
myevaluation criteriawere objectivelydefined inadvance. Idid notstart
to question myabilities until a considerable amount ofnegative feedback
jarredmyfaith.
Sure, I Know That!: The Knowledge Illusion
The knowledge illusion is a false confidence in what we know-both in
termsofquantityand quality.Itisbasedonthefalse premisethatmorein
formationshouldtranslateintomoreknowledge.48When horse-race hand
icappersweregiven moreitemsofinformation, they grewmoreconfident
in their forecasts, but their actual accuracy did not improve.49 The addi
tionalfacts createdthe illusion ofmore knowledge.
The knowledge illusion is relevant to TA because it is most likely to
occurin situations characterizedbylargeamounts ofdata. Notonlydo fi
nancial markets generate a torrentofindividualtime series, but each one
can be transformed into a far greater number of derivative time series
calledtechnicalindicators.ThisprovidesampleopportunityforTApracti
tionerstoseethemselvesasmore knowledgeablethantheyreallyare.
The knowledge illusion stems in part from an overconfidence in
the mind's ability to perform configural reasoning, the simultaneous



==================================================
                     PAGE 66                      
==================================================

50 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
interpretationofa multitude ofvariables. Infact the unaided mind isable
to deal with only two or three variables in a configural fashion (see Part
Two). Byfailing to appreciate these limits, people easilyassume thatcon
sidering more variables (indicators) will lead to greater knowledge and a
moreinformedopinion.
ICan Handle That: The Illusion ofControl
The illusion of control is an unwarranted belief in our ability to control
outcomes. People who feel in control are happier and more relaxed than
peoplewhodo not. Thiscognitivedistortionisfed bytheself-attribution
50
biasand it, intum, feeds the overoptimismbias.
Accordingto Nofsinger, activitiesthatare mostlikelyto inducethe il
lusionofcontrolhave thefive following characteristics51
:
1. Ahighlevelofpersonalinvolvement
2. Alarge range ofchoice
3. A great amount of information available to consider (Knowledge
Illusion)
4. Ahighleveloffamiliarity withthe task
5. Earlysuccessinthe activity
The first four clearly apply to subjective TA. Ahigh level ofpersonal
involvement is engendered by the frequent analysis of market data, the
creation ofnew indicators and new ways to interpret them, the drawing
and redrawing oftrend lines, the counting and then recounting ofElliott
waves, andsoforth. Thereisalsoa largedegree ofchoice: which markets
to follow, which indicators to use, where to place a trend line, and so on.
The various methods used become familiar as they are studied and used
regularly. The fifth factor, earlysuccess, is a matter ofchance. Some will
experience initial success and because of the self-attribution bias are
likelyto attribute itto their expertise and the efficacy ofthe TA methods
ratherthan chance. All ofthesefactors caninduceand maintainan unjus
tifiedsenseofcontrolandanabilityto earnmarket-beating returns.
The Hindsight Bias: IKnew Things Would Turn
Out That Way
The hindsight bias creates the illusionthat the prediction ofan uncertain
eventis easierthan it really is when the eventis viewed in retrospect, af
terits outcome is known. Oncewelearnthe upshotofanuncertainsitua
tion, suchaswhichteamwonafootball game orin which directionprices



==================================================
                     PAGE 67                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 51
moved, subsequent to a TA pattern, we tend to forget how uncertain we
reallywerepriorto knowing theoutcome. See Figure2.6.
Thiscognitive distortioncanbe understood in terms ofthe waymem
oriesarestoredinthebrain. Pasteventsarestoredincategoriesofsimilar
events (e.g., sports events) rather than as a sequential temporal record.
Forthis reasonsinlilartypesofeventsthatoccurred atdifferenttimes be
come intermixed. As a reSUlt, after the outcome of a football game be
comes known, the pregame state of knowledge (uncertainty) becomes
intermixed with definitive postgame knowledge. Our state ofknowledge
prior to the game was uncertain because ofthe ambiguous nature ofthe
pregameevidence. Onecouldhavemadethecaseforeitherteamwinning.
The internlixing of pregame knowledge with postgame knowledge hap
pens immediatelyand unconsciously, leaving us unable to recall whatwe
knew before the game or our state ofdoubt. Consequently, the pregame
evidence,whichwasinfactquite uncertain, seemslessuncertainafterthe
winner became known. This creates a false sense of confidence in our
ability to make predictions. Itis because ofthis verypitfallthatscientists
are especially careful about defining procedures for making predictions
andevaluatingtheiraccuracy.
Subjective TA is especially prone to the hindsight bias because it
lacks clearly defined rules for pattern identification, forecast generation,
Iwonder ifI I knew thejet's
should bet on would win.
thejets? It was so obvious!!
The other team Why didn't Ibet?
is pretty good.
-----------------~--;--"--tV~
NYjets Win
--~
Football Game ----------------------------
'-- ---J
October 1 October 2 October 3
Time
FIGURE 2.6 The hindsight bias.



==================================================
                     PAGE 68                      
==================================================

52 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
and prediction evaluation. In real time, the practitioner ofsubjective TA
faces a task ofoverwhelming ambiguity. Asingle chart contains a mind
boggling number of possible predictive cues that range from the ex
tremelybearishtotheextremelybullish. However, when thesamechartis
viewed in retrospect, the ambiguities that had faced the analyst trying to
makeapredictionfade awaybecauseoutcomesare known. Thiscreatesa
false credibilityfor subjectivechartanalysis.
Let's consider a hypothetical example illustrating how outcome
knowledge might minimize the true ambiguities in a chart, thus overstat
ing the predictive powerofsubjective chartanalysis. ConsiderFigure 2.7
and whatwas knowable as oftime A. The pattern could be interpretedas
bullish: a prioruptrendwitha bullishflag consolidationtemporarilyinter
ruptingthe uptrend.
Or the same chart could be interpreted as bearish: a head-and
shoulderpattern, with a neckline breach and then a return rally provid
ing an ideal time for a short sale. This interpretation is illustrated in
Figure 2.8. Note that both charts are identical up to pointA, exceptfor
the price swings that have been highlighted and the forecast made
based onthe pattern.
r"'"
CC!l)
Price
,
,
,
,
Time
A
FIGURE 2.7 Bullish conjecture.



==================================================
                     PAGE 69                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 53
r "'"
~~~--------------
Price
Time
A
FIGURE 2.8 Bearish conjecture.
In reality, at point A the chart is an ambiguous mixture of technical
features that could support either a bullish or bearish forecast. The true
uncertainty, which isdepictedinFigure2.9, explainswhychartistsshown
thesamehistorywilloftenexpressavarietyofpredictions
Now let's alter the situationbyimagining how chartists mightlook at
the same chart at a later time, point B. We will look at two different out
comes paths subsequentto pointA. This is intended to illustrate how the
hindsight bias can obscure the an1biguity that existed at time A, thereby
creatinganillusion ofvalidityforsubjective chartanalysis, irrespectiveof
whichpathpricesactuallytook.
Firstconsiderananalyst'simpressionlookingatthe chartforthefirst
timeattime BinFigure2.10. Thisisthesamechartshownpreviouslywith
an uptrend tacked onafterpointA. This observerknows an uptrend took
place betweenAand B. Itis my contention that the hindsight bias would
encourage the subjective analysts to notice the bullish flag pattern rather
than the bearish head-and-shoulder pattern. In other words, the posses
sion of outcome knowledge tends to make patterns that gave incorrect
predictions less noticeable while making patterns that predicted accu
rately more noticeable. What was in fact an ambiguous pattern to an ob
server only possessing knowledge up to pointAappears as an obviously



==================================================
                     PAGE 70                      
==================================================

54 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
p'kec@S
?
•
Time
A
FIGURE 2.9 The true uncertainty offoresight.
.
.
.
.. .
•• •• I
.
Bullish Flag .- •••• I
Price Obvious in Hindsight : •• : I
.:.. r
i '"
._--------------~
--------------~
I
I
I
I
I
I
I
I Time
A B
FIG RE 2.10 The false certaintyofhindsight.



==================================================
                     PAGE 71                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 55
bullish bull flag to an observer looking back from point B. Similarly, the
outcome knowledge possessed by the observer at point B renders the
bearish head-and-shoulders pattern less noticeable. The observeratpoint
B comes away with the impression that the evidence on the chart sup
portsthevalidityofchartanalysis.
Alternatively, ifthepricepathfromtimeAtotime Bhadbeenadown
trend, as depicted in Figure 2.11, I contend that the hindsight bias would
cause an observer at point B to notice a head-and-shoulders top pattern
thatpredicted successfully, butthe observerwould not notice the bullish
flag whoseforecastprovederroneous.
Theseillustrationswerecontrivedtomakea point. Whateverthe out
come, itis always possible to selectivelynotice chartfeatures thatseem
inglypredicted correctly, amongstwhatis trulyanambiguous mixture of
bullish and bearish cues. The vague manner in which subjective chart
patterns are defined, the lack ofobjective evaluation criteria, andthe op
eration ofthe hindsight bias create an illusion ofvalidity for subjective
chartanalysis.
Who among us, upon first learning ofthe head-and-shoulder pattern,
,......,,,,,,
,
Bearish H&S ---t----------------~~
Obvious in Hindsight
: I
Price ,
: I
I
o
o
•• • I
.
-... e.. .-. I
• •• I
o ••0 0•• I
J
00
I
I
I
I
I Time
A B
FIGURE 2.11 The false certaintyofhindsight.



==================================================
                     PAGE 72                      
==================================================

56 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
did not peruse historical charts to find examples of the head-and-shoul
ders pattern to see if it worked as claimed? And who among us did not
come away with the impression, "This stuff actually works!" Were we
merelyvictimsofthe hindsightbias?
There is persuasive experimental evidence52 that people are afflicted
with hindsight bias. In one typical study, students were asked to rate the
probability ofvarious outcomes in advance of President Nixon's trip to
Chinain 1972. Forexample,theywereaskedtopredictifa meetingwould
takeplacebetweenPresidentNixonandchairmanMaoTse-tungandifthe
United Stateswould establisha diplomatic missioninChinabutnotgrant
the diplomaticrecognitionthatChinawanted. Botheventswere uncertain
priorto the trip. Two weeks after the trip, when the events that did take
placewere known, thestudentswereasked to recallwhattheyhadprevi
ously predicted. After a two-week interval, 67 percent ofthe students re
membered their predictions as being more accurate than they actually
were. In other words, they were unable to recall their prior uncertainty.
After several months, the percentage ofstudents afflicted with hindsight
biasjumpedfrom 67percentto84percent.
The hindsightbiashas beenfound to operatepowerfullyin trial testi
mony. Witnessesbelievetheyare givingaccurateaccounts, buttheirrecall
ofthe order ofevents and ofspecific details are altered by knowing how
mattersactuallyturned out.
53
The hindsight bias infects historical accounts. Historians, having the
benefit ofhindsight, will often point out that the rise ofthe Third Reich
was quite predictable. They claim that the seeds ofNazism were obvious
in various writings thatpreceded the Third Reich. In actuality, the rise of
the Third Reich was only one ofnumerous possible interpretations that
could have been read into these accounts. The outcome, the rise ofthe
Third Reich, which seems so inevitable to the historians, when looked at
in retrospect, was merely one amongan infinitenumberofpossiblepaths
thathistorycouldhave taken.
Otherexperimental evidence showsthatstrategies aimed specifically
at reducing the hindsight bias are not effective.54 Even when people are
warnedabout hindsightbias and told to avoid it, itstill occurs. Itappears
to be beyond rational control. Not even professional expertise is helpful.
In one study, a group ofdoctors were askedto evaluatethe diagnosticer
rors made by other doctors. The doctors doing the evaluations were
armedwiththe knowledgeofthe diseasethatwasultimatelyconfirmedby
a pathology report. The evaluators were unable to understand how such
errors could have been made by a trained physician. Again, outcome
knowledge makes the past appear as if it should have been more pre
dictable thanitreallywas.
What cognitive processes are responsible for hindsight bias? Though



==================================================
                     PAGE 73                      
==================================================

The IIlusorv ValiditvofSubjective Technical Analvsis 57
thematterisnotsettled,itseemsto gobeyondadesiretoseeourselvesas
smartand in control. One conjecturethathas receivedsome supportsug
gests thatithas to do with the waymemoryworks, or, bettersaid, fails to
work.55Insimplifiedterms, therecordingofmemoriesbythebrainisnota
passive process in which events are stored in proper temporal sequence
ready for replay in that sequence. Rather, memory involves an active de
construction of events for storage and then an active reconstruction of
events when theyare recalled. As events are experienced, they are sliced
and diced andstored byassociative categories, ratherthan bytime ofoc
currence. An encounter with a dog last month at a friend's house in the
countryisdissectedandstoredindistinctcategoriesandneurallocations;
memories of dogs, memories of friends, and memories of trips to the
country, allofwhichmayhaveoccurredatvarioustimesinthepast. When
we attemptto recall the last month's visit to the country, the mind recon
structsthememorybysplicingtogetherbitsofinformationstoredinthese
separate associative locations. Psychologists have speculated that our
brains evolved this way because it is generally effective and efficient.
However, because time ofoccurrence is not an important feature ofthe
storage system, it becomes difficult to recall the orderofevents. And itis
the orderofevents, such as when we received a bitofknowledge, that is
mostcriticalwhenweare assessingourpredictiveabilities.
In the process of deconstructing events for storage, new experience
gets mixed in with old experience. Justas a drop ofinkina glass ofclear
waterbecomes thoroughly and irreversibly intermixed, recently acquired
knowledge, such as the outcome ofan uncertain event, becomes inextri
cablyintermixedwithwhatwasknownpriortotheoutcome. Post-outcome
knowledge becomes indistinguishable from pre-outcome knowledge and
seems as ifitwere known allalong. Thisexplainswhy itis so difficultfor
peopletoreconstructpriorstatesofuncertainty.
Can subjective TA analysts overcome the hindsight bias? To answer
this question we mustconsiderthe analystin the contextoftwo different
tasks: (1)patternresearch-thesearchforpatternswithpredictivepower
in historical data, and (2) real-time forecasting-applying the patterns in
currenttime to make newpredictions. The task ofpatternresearch is ex
emplified by the early investigations ofCharles H. Dow, originator ofthe
Dowtheory, orofRalphN. Elliott, originatorofthe ElliottWave Principle.
In the process of formulating their ideas, they informally proposed and
tested various hypotheses about which chart patterns had predictive
power. The task of making real-time predictions by applying those pat
terns in current time would be exemplified by Dow theorist Richard Rus
sell applying the Dow theory today or Robert Prechter, an Elliott-wave
expert, makingaforecasttoday.
In the context ofhistorical pattern research, I contend that hindsight



==================================================
                     PAGE 74                      
==================================================

58 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
bias is unavoidable because it is impossible to shield oneself from out
come knowledge. Simplyknowing thepaththatpricestookbiasesthe an
alyst's perception of the predictive power of whatever method is being
evaluated. Only objective TA methods offer the opportunity of avoiding
hindsightbiasbecause only information known ata given pointintime is
usedtogeneratesignals, andsignalsareevaluatedinan objectivemanner.
In the context of current-time forecasting, subjective analysts could
protectthemselvesfrom hindsightbiasiftheywerewillingtomakefalsifi
able forecasts. Aforecast is falsifiable if, at the time a forecast is made,
the analyst specifies (1) outcomes that would constitute a forecast error,
or (2) the procedure that will be used to evaluate the forecast as well as
whentheprocedurewillbeemployed. Forexample:
The maTket will be highe1" six months fr-om now, and within that
timefTame the maTket will not decline mOTe than 20 percentfr-om
currentlevels, aT
The maTketwill advance 20pe1"centfrom current levels before it de
clines20pe1"centfTom currentlevels.
A buysignalhas beengiven. Hold longpositionuntilasellsignalis
given.
The first two forecasts specify outcomes that would clearly define a
forecast error. Forexample, ifthe marketwere to drop 20 percent before
itadvanced 20 percent, the prediction would be wrong. End ofstory! The
third forecast, a signal, implies a clear procedure for evaluation-com
pute the gain orloss from date ofthe buy until the date ofthe sell signal.
Falsifiableforecasts would provide the analysts and theircustomers with
valuablefeedback. Unfortunately, few subjectiveforecasters dothis.
SECOND-HAND INFORMATION BIAS: THE POWER OF
A GOOD STORY
No one has the time, much less the expertise, to obtain all necessary
knowledge via direct experience. Therefore, by necessity, most of what
we knowwelearnsecondhandfrom those who purportedlydo know.
Among the myriad ways knowledge is communicated, the narrative
account or story is by far the most popular. BiologistStephen Jay Gould
calledhumans "thestory-tellingprimate." We have beensharingideasthis
wayforthousands, perhapsmillionsofyears. Consequentlymuchofwhat



==================================================
                     PAGE 75                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 59
we know, even including how to do arithmetic, is stored in the mind in a
narrativeformat.
56
For this reason, good stories are more powerful persuaders than ob
jective facts. Psychologists speculate that concrete, colorful, and emo
tionally interesting tales have a powerful impact on our beliefs because
such stories call to mind pre-existing mental scripts. Concepts and ab
57
stractions simply cannot light up the brain's story networks the way a
lively tale can. Philosopher Bertrand Russell said that when we learn
things informally (nonscientifically), we are impacted by the "emotional
interest ofthe instances, not by their number"58 It is for this very reason
thatscientiststrainthemselvestoreactinexactlytheoppositeway, thatis
to discount dramatic stories and pay attention to objective facts, prefer
ablythose thatcan bereducedto numbers.
Notall stories are equally compelling. To commandattention, a story
must be interesting and understandable yet not bore us with what we al
ready know. We are most captivated byvivid accounts about real people,
preferably people we know. Stories that speak to our emotional needs
whilebeingentertainingand informativesellthe best.Agoodtalenotonly
enlivens us, but it gains a life ofits own, as it is told and retold over and
overagain.
The Conflict between Truth and Tale
Aconflict exists between our desire for knowledge and our desire that it
be delivered in the form of a good story. Humorist H. 1. Mencken put it
eloquently; "What ails the truth is that it is mainly uncomfortable and of
ten dull. The human mind seeks something more amusing and more ca
ressing." Reality tends to be full ofinconsistencies, so an audience must
rely on the integrity of the storyteller to tell it like it is rather than how
theywouldliketo hearit.
Regrettably, even when the primary intent ofa secondhand account
is the delivery of knowledge, it is often biased to satisfy the audience's
appetite for an engaging story. Storytellers know full well that informa
tion delivered with too many qualifiers is unappealing. Consequently, in
consistencies and ambiguities are minimized while cohesive aspects are
amplified. Although these editorial modifications make the account
59
more digestible, they canrob itofits essentialtruth. Inthe end, the audi
ence comes away with an exaggerated impression of the information's
clarity and validity. This even occurs when the findings of scientific pa
pers are summarized and communicated. Over numerous retellings, the
truth gets left further and further behind. What started out as a result
with possible significance may end up being reported as a discovery of
highsignificance.



==================================================
                     PAGE 76                      
==================================================

60 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Especiallypersuasiveareaccounts that linkeventswith the chainof
cause and effect. The perception ofcausal relationshipsisa natural cog
nitive capacity60 and causal chains satisfy our need to explain events in
61
the world around US. Studies of jury decisions show that the best
62
causalnarratives winin the courtroom. The side offering thestory that
best ties the evidence into a coherent and credible sequence often car
ries the day.63
The problem is thatcause-effect explanations that are, in fact, falla
ciousare hard to detectwhen theyare plausible andappealto asense of
the ironic. One story that has been making the roundssince the 1950sis
as follows: "The Ten Commandments contain 297 words. The Declara
tion of Independence has 300 words, Lincoln's Gettysburg Address has
266 words, but a directive from the government's Office of Price Stabi
lization to regulate the price of cabbage contains 26,911 words." The
truth is the Office ofPrice Stability never made such a directive. Never
theless, the tale had such appeal, it remained alive despite the agency's
effortsto convincethepublicitwasfalse. Even the dissolutionofthe Of
fice of Price Stability did not stop the story. It was merely modified so
that the directive was described as a "federal directive."64 The story
would not die because of its irony and the plausibility of long-winded
bureaucrats.
Elliott's Tale
The power ofa good story may explain the enduring appeal ofthe Elliott
Wave Principle (EWP), one of TA's more grandiose conjectures. The El
liott Wave Principle holds thatprice waves express a universal orderand
fOffil that is found not only in the fluctuations of financial markets but
throughout the naturalworld, includingthe shapes ofseashells, galaxies,
pine cones, sunflowers, and numerous othernaturalphenomena.
According to EWP, market trends are fractal-a nested hierarchy of
waves sharing the same form but ranging in magnitude and duration
from microwaves that lastonly minutes to grand millennialmacrowaves
65
thatcanlastforthousands ofyears. Thissharedform, calledthe Elliott
wave, is an eight-segment configuration ofrising and falling price move
ments. Infactthisuniversalpatternofgrowthand decayisalleged to de
scribe not only the evolution of prices in financial markets; it is also
manifested in the evolution of trends in mass psychology, the rise and
fall ofcivilizations, culturalfashions, and othersocialtrends. Thetheory
claims to describe just about anything that goes through cycles of
growth and change. Even the business career of Elliott wave's leading
advocate RobertPrechter, according to Prechterhimself, hasfollowed a
series ofups and downs that conform to the Elliott Wave Principle. This



==================================================
                     PAGE 77                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 61
all ties in with a sequence of numbers called the Fibonacci series and
the golden ratio phi, which does have many fascinating mathematical
properties.66
However, thatwhich purportsto explain everything explains nothing.
The ElliottWave Principle, as popularly practiced, is nota legitimate the
ory but a story, and a compelling one that is eloquently told by Robert
Prechter.67 The account is especially persuasive because EWP has the
seeminglyremarkableability tofit anysegmentofmarkethistorydownto
its most minute fluctuations. I contend this is made possible by the
method's loosely defined rules and the ability topostulate a large number
ofnested waves ofvarying magnitude. This gives the Elliott analyst the
same freedom and flexibility thatallowed pre-Copernican astronomers to
explain all observed planetmovements even though their underlying the
ory of an Earth-centered universe was wrong. The analogy between the
medieval astronomerfitting epicycle upon epicycleto theirdataand EWP
analysts fitting wave within nested wave to market data is strong. Thus,
even if the fundamental notion ofEWP is wrong, the method ofanalysis
will stillbeable to obtain a verygoodfitto pastdata.68
In fact, any sufficiently flexible model can fit a prior set of observa
tionswithperfection. Forexample, apolynomialfunction withasufficient
numberofterms (equal in number to the numberofdatapoints) can also
produce a perfectretrofit. However, a model ormethod withan unlimited
ability to fit past observations but which cannot make testable (falsifi
able69 predictionsoffuture observationsisneithermeaningfulnoruseful.
)
Though the Elliott Wave Principle is said to hold even the heavens,
wheregalaxiesconformto thelogarithmicspiral, itsperformancehere on
earthhas beenless thanstellar.7oWhatthen, explainsitsenduringappeal?
I contend that it is attributable to the fact that EWP is a comprehensive
cause-effect story that promises to decipher the market's pastand divine
its future better than any otherTA method. Some stories are too good to
letdie.
Stories Shaped by Self-Interest
Self-interestcan motivate distortion insecondhandaccounts. Peoplewith
an ideological ortheoreticalpositiontend to selectivelysharpensome as
pects ofa storyand minimize othersto bringitinto betteralignmentwith
theirpoint ofview. Purveyors ofspecific TA methods or ofTA in general
have clearideologicalandfinancial interestsatstake.
This charge can be leveled at me71 or any author arguing a position.
However, when the storyteller is constrained by objective evidence and
repeatable procedures, there is less leeway for the distorting effects of
self-interestto operate.



==================================================
                     PAGE 78                      
==================================================

62 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
CONFIRMATION BIAS: HOW EXISTING BELIEFS FILTER
EXPERIENCE AND SURVIVE CONTRADICTING EVIDE CE
"Once a beliefforms, we filter information in ways that sustain it."72 The
confirmation bias is the tendency to view as credible new evidence that
confirms our prior beliefs butto view as incredible evidence that contra
dicts them.73 This tendency inhibits learning from new experience and
stifles the elimination ofincorrect ideas. The confirmation bias explains
whywegetstuckwitherroneousbeliefs.
The Confirmation Bias Has a Rational Basis
In many instancesitis rational to give weightand preference to evidence
that supports an existing belief and to view with skepticism that which
contradictsit. Beliefswould behighlyunstablewithoutsuch arule.
The use ofprior knowledge as an interpretive filter is a mark ofhu
manintelligence, solongas the priorknowledge iswellsupported. Scien
tists were justifiably skeptical of a claim that nuclear fusion had been
achieved at room temperatures in an apparatus constructed from parts
available at the local hardware store.74 Their doubts were subsequently
borne out by objective tests that were unable to reproduce the so-called
cold-fusion effect. Few ofus would acceptatface value the supermarket
tabloid headline: "Elvis Returns on UFO & Will Build an Alien Theme
Park." However, when prior beliefs are unjustified because they lack the
supportofsoundinferencesfrom solid evidence, itis notrational togrant
them the status ofintellectualgatekeeper. The problem is that people are
often not aware of which of their held beliefs are unjustified. Conse
quently, the confirmationbiasoperatesevenwhenitshouldnot.
Biased Perception
The confirmationbiasis a consequence ofthe way perception works. Be
liefs shape expectations, which in tum shape perceptions, which then
shapeconclusions.Thusweseewhatweexpecttoseeand concludewhat
we expectto conclude. As Henry David Thoreau putit, "We hear and ap
prehend onlywhatwe alreadyhalfknOw." The truism, I'll believe itwhen
I seeitmightbebetterstatedI'll seeitwhenI believeit.
The potenteffectofexpectations onperception was demonstrated in
the following experiment. When subjects were given a drink that they
thought contained alcohol, but in fact did not, they experienced reduced
social anxiety. However, other subjects who were told they were being
given nonalcoholic beverages when they were, in fact, alcoholic did not
experiencereducedanxietyinsocialsituations.75



==================================================
                     PAGE 79                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 63
Notonlydowefailtoperceivenewinformationthatisinconflictwith
prior beliefs, we are also reluctant to draw conclusions that are at odds
with what we already believe to be so. As a result we tend to accept at
face value information that is consistent with pre-existing thinking while
we critically scrutinize and discount informationthat is inconsistentwith
priorbeliefs. Theconfirmationbiasexplainswhyourbeliefsfail tochange
76
in responseto new informationasmuchastheyshould.
It gets worse. The confirmation bias, operating in conjunction with
the laws of chance, predicts that erroneous beliefs will strengthen with
timeandfamiliarity.77ATAmethodwithnopredictivepower(e.g., asignal
based on a coin flip) will enjoy occasional success due to chance. Over
time, these confirminginstanceswillaccumulateand, because ofthe con
firmation bias, will be given greater weight than instances where the
method fails. The result is a vicious cycle ofgrowing self-deception. This
suggests that the belief in a flawed method's efficacy will increase over
time irrespective ofits actual merit. It also implies that TA practitioners
most experienced with a useless method will be the least able to recog
nize its flaws because ofmore lengthy exposure to the method's chance
basedsuccesses.
Motivational Factors
The confirmation bias is also driven by motivational factors. TA practi
tioners have a large emotional and financial investment in their favored
method. This is especially true of practitioners whose professional lives
are tiedto aparticularmethod.
There is also a strong motive to maintain consistencywithin oursys
tem beliefs and attitudes. The theory ofcognitive dissonance formulated
78
byFestinger contendsthatpeople are motivated to reduce oravoidpsy
79
chological inconsistencies. The discomfort provoked by evidence that
contradictswhatwe believemakesithardto digestsuchevidence.
Biased Questions and Search
The confirmationbiasalsoslantsthe wayquestionsareframed, thereby
biasing the search for new evidence. This search bias increases the
chance of encountering new evidence that supports the prior belief
while reducing the possibility ofencounteringnonconfirming orcontra
dictory facts. This occurs in the context ofsubjective TA research. By
definition, it is confined to the search for supportive anecdotal exam
ples. Contradictory examples are difficult, ifnot impossible, to find be
cause subjectively defined patterns do not specify the conditions of a
prediction error.



==================================================
                     PAGE 80                      
==================================================

64 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
People trained in the scientific method do two things to combatthis
tendency. First, at the inception ofa test, they establish objective criteria
forevaluatingoutcomes. Second,theyactivelypursueevidencethatmight
contradictpriorbeliefs and assumptions. Scientists are guided by the no
tion that ideas that can survive a vigorous search for contradictory evi
dence have greater validity than ideas that are merely backed by
confirmatoryevidencediscovered byasearchdirectedatexactlythat.
The confirmation bias can occur in the context of objective TA as
well. Consideran analystsearching for profitable rules with back testing.
Ifthefirst rule thatis tested gives unsatisfactoryperformance, thesearch
for a rule with good performance continues. This involves testing a se
quence ofruleswithalteredparameters, logic, indicators, andsoforth un
til a good one is found. This is data mining. Because the decision about
when to end this process is entirely up to the analyst, the scope of the
search is unbounded. This guarantees that ultimately a rule with good
pastperformance will be discovered. The researcher's initial beliefthata
good rule will befound isthus confirmed.
Infact, thepastperformanceofarule discoveredbydataminingover
statesitslikelyperformanceinthefuture. Thisoverstatementiscalledthe
data-mining bias and is discussed in Chapter 6. This problem is not data
miningperse. Infact, whendone properly, dataminingisaproductivere
search method. The error is the failure to take into account the upward
bias caused by data mining. As will be explained in Chapter 6, by taking
intoaccountthe extentofthe searchthatledto the discoveryofthe good
rule, it is possible to draw sound inferences about the rule's future profit
potential.
How Vague Evaluation Criteria Contribute
to the Confirmation Bias
Because subjective TA is vague about how its forecasts should be evalu
ated, evidence ofits success and failure is ambiguous. Thisfacilitates the
analyst's ability to point to prior predictions that worked (confirmation
bias) whileatthesametimepreventingthe identificationofpredictioner
rors. This effectively immunizes subjective practitioners from the nega
tivefeedback thatmightcausethemto changetheirbeliefs.so
Predictionerrorsofasubjectivemethodcanbeobscuredinanumber
ofways. One is what I refer to as pattern renaming. An upside breakout
pattern thatfails is relabeled a successful bull trap. According to TAdoc
trine, upside breakouts are alleged to predictupward trends. This is illus
tratedinFigure2.12.
When an uptrend does not materialize it should be counted as a pre
diction error. Ifinstead, the pattern is renamed a bull trap,S1 we wind up



==================================================
                     PAGE 81                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 65
Price
Predicted
Outcome
•
•• •
• •••
•• • ••
Breakout Signal • •• •
• ••
'\. ••
• Actual
Outcome
Time
FIGURE 2.J2 Breakoutfailure.
with a successful signal from a different pattern as shown in Figure 2.13.
Objective pattern definitions and clearly predefined evaluation criteria
wouldpreventsuchafter-the-factevidencefiddling.
Another way prediction errors are obscured is when forecasts lack a
clear endpoint. The endpoint ofprediction is a future time or event that
calls for the prediction to be evaluated.. A forecast or signal that does
Price
Successful
Bull-Trap
Signal
Time
FIGURE 2.13 Prediction errorobscured with pattern renaming.



==================================================
                     PAGE 82                      
==================================================

66 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
specify or imply a clear endpoint is effectively meaningless. The talking
heads ofTAtypicallyissue predictions withoutclearendpoints: "I'mbull
ish for the intermediate-term." This vague pronouncement provides the
forecasterwithenoughpostpredictionwiggleroomtoavoid gettingnailed
with a bad call. Either a defined time period or a specific subsequent
event, such as a certainpercentage ofadverse price movement, prevents
the forecaster from engaging in an after-the-fact, free-for-all hunt for any
evidence that portrays the forecast in a good light. However, the state
ment "I expectthe marketto advance at least 5percentfrom currentlev
els before it declines 5 percent" leaves no room for fudging. Objective
signaling methods imply a clear endpoint-the subsequent signal to exit
theposition.
The freedom to alter the forecast evaluation criterion after the fact
makes it likely that supportive evidence will be found. This is precisely
why science has a dual personality. It is flexible and receptive to all
testable ideas, yet rigidly orthodox about how testing and evaluation are
to becarriedout. Infact, muchofwhatscientistsdo canbe understoodas
the use ofstrictprocedures for determining when an ideais wortWess. If
peoplewere to use similarprocedures in theireverydaylives, theywould
bemuchlesslikelytoadopterroneousbeliefs.82Peoplearehighlyadeptat
concocting ideas, theories, and explanations for their experience that
haveasemblanceofplausibility.83However, theyare notnearlyasgoodat
testingthosebeliefsoncetheyhave takenroot. Oneofthebiggestreasons
isthefailuretopreciselydefinewhichoutcomesqualifyassupportiveevi
dence and which do not. Withoutobjective evaluation criteria, the oppor
tunity and motivation to find supportive evidence for what we already
believeisleftuncheckedand runs rampant.
The Confirmation Bias and Vague Evidence
The precise way in which the confirmationbias functions to substantiate
priorbeliefsdependsuponthe clarityofthe newevidence. When the new
evidence is vague, the confirmation bias operates unfettered. Because
vagueevidencecanbeinterpretedaseithercontradictoryorsupportive, it
issimplytakenassupportive.
Subjective TAmethodsare vague ontwo counts: with respectto how
patterns and signals are defined, and with respect to how predictions
basedonsaidpatternsandsignalsaretobeevaluated. Thisinvitesthedis
coveryofevidencethatis weaklysupportive. Weakevidenceisnotstrong
enoughto compelthe beliefthatthe methodworks, butbecauseweakev
idence is opento interpretation, it can be interpreted as being consistent
with a belief that the method works. The hallmark of weak evidence is
thatitcan be interpreted as beingconsistentwithseveral differentpoints



==================================================
                     PAGE 83                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 67
ofview. Forexample, the appearance ofa certainportion ofa price chart
could beconsistentwithafive-wave Elliottimpulsepattern, oranentirely
different subjective pattern, or purely random price movement.84 Objec
tively defined patterns and evaluation criteriapermit no such leeway. Ei
ther the data is an example ofa particularpattern or it is not. Either the
outcomeisconsistentwith thepattern'sprediction oritisnot.
The Confirmation Bias and Clearly
Contradictory Evidence
Let's now considerhow the confirmation bias operates when there is evi
dence that clearly contradicts a prior belief. In many situations, the evi
dence is a mixture. Some evidence clearly confirms the beliefand some
clearly contradicts it. In these situations the confirmation bias still oper
ates, butinamore complexmanner.
It might be thought that the dissonant evidence would simply be ig
nored or twisted into being supportive. But, this tends not to happen be
cause people value seeing themselves as rational and cognitively
consistent. "Theyarereluctanttosimplydisregardpertinentevidencethat
is contradictoryin orderto see what theyexpectto see and believe what
they expect to believe."85 However, what they do is to alter the dissonant
information in orderto reduce its conflictwith priorcherished beliefs. In
otherwords, the confirmationbias encouragessubtle cognitive manipula
tions to the clearly dissonant evidence to reduce its importance and re
duceitsclashwithconfirmingevidenceand cherishedbelief.
One way people discount discordant evidence is by applying a
harsher standard ofacceptance.86 The standard applied to evidence that
supportsa favored positionis the gentle requirementthatithave a ring of
plausibility or the possibility ofbeing valid. In contrast, the standard ap
plied to evidence that conflicts with cherished beliefs is that it must be
convincing beyond any possible doubt. For believers in faith healing, a
colorful corroborating account is accepted without question. However, a
controlled scientific study denying the efficacy offaith healing would be
faulted onanyandeverypossibleground, reasonableorotherwise. Byde
manding that dissonant evidence be so strong that it be compelling be
yond any doubt, whileonlyrequiringthatharmoniousevidencebeweakly
consistent, holdersoferroneousbeliefsareable to keep theirfaith alive.
Oneofthemostsurprisingfindings aboutthe confirmationbiasisthat
evidence that contradicts a prior belief can actually have the effect of
strengtheningthat belief. One would think that a mixture ofpro and con
evidence should at least reduce the strength of the prior belief. Studies
have shown otherwise. For example, in one experiment, subjects were
presented with clearand convincing evidence that was both pro and con



==================================================
                     PAGE 84                      
==================================================

68 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
capital punishment.87 The subjects were oftwo types: those with a prior
beliefthatcapitalpunishmentwas goodand those with a priorbeliefthat
capital punishmentwas bad. Both groups were given summaries and cri
tiques to two different studies on capital punishment. One study pre
sented evidence showing that capital punishment was an effective
deterrentagainstcrime while the otherprovided evidence showingitwas
not effective.88 The experiment showed that participants perceived the
study thatwas consistentwith theirpriorbeliefto bevalid and important
evidence, whereas the study that conflicted with their view was seen as
flawed research and weak evidence. After being exposed to evidence on
bothsides ofthe issue, participantscame awayfrom the experimentwith
their prior beliefs strengthened. In otherwords a mixture ofpro and con
evidencecausedthe two groupsto becomemorepolarized.
Itisalsoinformativetoconsiderwhatsubjectsinthestudydidnotdo.
Hostile evidence was neither misconstrued to be favorable nor ignored.
Rather, it was scrutinized for flaws, thus reducing its import. In other
words, significantcognitive effortwas expendedto explain away ormini
mize contradictoryevidence.89
An experiment that is closer to TA examined how the confirmation
bias encouraged unsuccessful bettors to maintain their delusions of
gambling greatness despite swelling losses. Losing money would seem
to be unambiguous evidence offailure, yet bad gamblers remained opti
mistic. Thomas Gilovich showed they accomplished this by evaluating
their wins and losses in a biased manner.90 As was true for subjects in
the study on capital punishment, the gamblers did not forget or ignore
discordant evidence (losses). Rather, they applied significant cognitive
effort to transform their losses into less derogatory evidence. This was
displayed in journals kept by the gamblers. It was found that they de
voted more commentary to losses than gains, and gains and loses were
interpreted differently. Gains were easilyattributed to the gambler's bet
ting acumen (self-attribution bias) whereas losses were attributed to
bad luck and, with a bit offine tuning to the betting system, would have
been wins.
These findings predict how subjective TA practitioners might react if
presented with objective evidence contradicting the validity ofa favored
method.91 For example, suppose a subjective method was to be trans
formed into an objective method, and tests indicated the method was not
effective. Itislikelythatdevotes ofthe method wouldfind any numberof
groundsto criticizethestudy. Thereisnostudyinanyfield ofsciencethat
cannot be criticized on some grounds. Itis also likely that the subjective
practitioner would trot out cherry-picked examples showing instances
where their method was successful. In the end, it is probable that the
practitioners would come away from the exercise more convinced than



==================================================
                     PAGE 85                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 69
everin the value oftheirmethod while mutteringaboutivory-toweracad
emicshavingtheirheadsstuckin the cloudsorsomewhere worse.
Beyond the Confirmation Bias: The True Believer
We have seen that erroneous beliefs thrive on vagueness and can even
growstrongerin theface ofevidencethatwould, attheveryleast, callfor
reduced belief. However, belieflongevity goes beyond even this. Studies
have shown that once a belieftakes root, it can even survive a complete
discreditingoftheevidencethatgaverise tothebeliefinthefirstplace.
Ithasbeendemonstratedthat, whensubjectsaretrickedintoforming
anerroneous belief, theirbeliefsurvivesevenafterthey are told thatthey
had been tricked. One study is particularly relevant to TA because it in
volves pattern discrimination. Subjects were asked to distinguish authen
tic suicide notes from fictitious ones.92 Initially, experimenters gave the
subjects fake feedback, thus deceiving them into believing that they had
learned to discriminate authentic suicide notes from fakes. In reality, the
subjects had not leamed how to perform this task. Later, the experi
menters told the subjects ofthe deceit. Even with such cleardiscrediting
evidence, thesubjectscontinuedto believe, to a considerabledegree, that
theywere able to discriminate realfrom fake suicide notes. Otherexperi
ments conducted along similar lines have found the same effect: a belief
cansurviveatotaldiscreditingofitsoriginalbasis.93
Thesefindingspredicthow believersmightreactifRalph Elliott, W.D.
Gann, or Charles Dow were to return from the grave and announce that
their methods had been intentional scams. It is likely that many current
daypractitionerswouldcontinuetobelieve. Given thatElliottwaveanaly
sis, or at least one version of it, has now been reduced to an objective
algorithm,itwillbeinterestingtoseethereactionsofEWPbelieversifob
jectivetestsfail.
94
Subjective Methods Most Likely to Suffer
the Confirmation Bias
Some subjective TA methods are more likely to encourage the confirma
tion bias than others. These methods will possess three characteristics:
(1) an elaborate causal explanation or compelling story about why the
method works, (2) high retrofit power-the ability to fit or explain past
market behavior accurately, and (3) no ability to generate falsifiable
(testable)predictions.
TA methods fitting this profile include Elliott waves, Hurst cycle the
ory, astrology-based forecasting, and W.D. Gann analysis, among others.
For example, the Elliott Wave Principle is based on an elaborate causal



==================================================
                     PAGE 86                      
==================================================

70 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
explanation invoking universal forces that shape, not only the physical
world, butmasspsychology, culture, and society as well. Moreover, ithas
highretrofitpower. Byemployingalargenumberofnestedwavesthatcan
vary in both duration and magnitude, it is possible to derive an Elliott
wavecount(Le.,fit) for anypriorsegmentofhistoricaldata. However, ex
ceptfor oneobjectiveversion95ofElliottwaves, the methoddoes notgen
erate testable/falsifiablepredictions.
Let's consider the first element, an elaborate causal explanation. TA
methods that are based on intricate causal stories are able to withstand
empiricalchallengesbecausetheyspeakto the deeplyfelt humanneed to
makesenseoftheworld. Becausewearecompelledtoexplainourexperi
ence,wehavehighlydevelopedabilitiesto generateplausiblestoriesafter
the fact. "To live, it seems, is to explain, tojustify, and to find coherence
among diverse outcomes, characteristics and causes. With practice we
have learned to perform these tasks quickly and effectively. "96 Studies in
which people are encouraged to form erroneous beliefs and then re
quested to constructrationales for those beliefs are more resistant to be
liefchangethansubjectswho were notaskedto formulate explanations.97
Subjectswho developed reasons to explain their beliefs became so stuck
in theiropinionsthat, evenaftertheywere told thatthey had beenmanip
ulatedintoadoptingafalse belief, theycontinuedtobelieve.98Infact, they
continued to believe almost as strongly as other subjects who were ma
nipulatedtoform false beliefsbutwere nottold ofthe manipulation.
The second and third elementsthat make a TAmethod immune to evi
dentiarychallengeisahighcapacitytoretrofitpastmarketmovementscom
bined with an inability to make testable (falsifiable) predictions of future
movements. The ability to bring all pastmarket movementinto conformity
with the method has the effect of eliminating any contradictory evidence.
Whenforecast errors do occurbecause outcomes are so completely outof
whackwithforecaststhatevensubjectiveevaluationisunabletohidetheer
ror, the standard explanation is that the error was caused by a misapplica
tion of the method rather than a flaw in the method itself. This fallacious
reasoning is then supported by reapplying the method so itfits the discor
dantoutcome.Theabilitytoalwaysobtainagoodretrospectivefitcombined
withthe inabilitytomaketestablepredictionseliminatesthepossibilitythat
anyevidencewilleversurfacethatcontradictsthemethod.
In science, the rationale for eliminating an erroneous theory is the
fact that its predictions offuture observations are shown to conflict with
those observations. This is the scientist's signal that the theory needs to
be eliminated orreformulated. Ifthe latter, the new version ofthe theory
is then tested againsta subsequentsetoffuture observations. This essen
tialmechanismforpruningawayfalse theoriesisillustrated in Figure2.14
and isdiscussed in depth in Chapter3.



==================================================
                     PAGE 87                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 71
Revise or
Abandon Theory
Prediction
Compared with
New Observation
Conflict
Agreement
Provisional Acceptance ofTheory
FIG HE 2.14 How science eliminates bad theories.
The entire motivationfor revisingorabandoningaTAmethod, orany
theory for that matter, comes from seeing clear examples where its pre
dictions are wrong. If it is easy to make after-the-fact adjustments that
eliminateallerrors, the rationaleto abandonthe methodislost.
Otherresearchsuggests thatsomepeople are more proneto the con
firmation bias than others. Strangely, theyare the mostintelligentpeople.
HarvardpsychologistDavidPerkinsconductedaninterestingstudyshow
ingthatthehigheraperson'sintelligence(measuredinIQpoints),thebet
ter they are able to construct articulate rationales for their beliefs and
defend them.99 However, Perkins also found that because oftheir strong
ability to rationalizeexistingbeliefs, highlyintelligentpeopleare lessable
to consider alternative explanations and points ofview. Hence they are
more apt to form beliefs that are resistant to change. Social scientistJay
Stuart Snelson calls this ideological immunity. He claims that intelligent
and successful adults rarely change their most fundamental presupposi
tions.1OOThis effect is mostpronounced in people who have accumulated
significant knowledge in a particular area. This suggests thatthe mostin
telligentand experienced subjective practitionerswill be the leastable to
abandonamethodin which theypreviouslybelieved.



==================================================
                     PAGE 88                      
==================================================

72 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
ILLUSORYCORRElATIONS
The studies discussed up to this pointhave dealtwith people's erroneous
beliefs aboutthemselves and theirabilities. We now considerstudies that
examinedpeople'sbeliefs aboutthe world atlarge, specificallywhethera
predictive relationship exists between a pair ofvariables. These have
101
shownthatpeople have a tendency to perceive illusory correlation. An il
lusory correlation is the false perception ofa relationship between a pair
ofvariables. Itisa significantproblemforsubjectiveTApractitioners.
Subjective TA Methods Represented
as Binary Variables
A pair ofvariables is said to be correlated when one, referred to as the
predictor, can be used to predictthe other, referred to as the outcome or
dependent variable. Although subjective TA methods are not usually de
scribed in these terms, all such methods can be viewed as asserting the
existence ofa correlation between a pair ofvariables. The predictor is a
TApattern orsignal such as the head-and-shoulders top pattern. The out
come is the post-patternprice movement the pattern is purported to pre
dict. Ahead-and-shoulderstoppatternissupposedtopredictadowntrend
upon the pattern's completion. In fact, the entire enterprise of TA, both
objective and subjective, can be viewed as the search for such correla
tions. Thesearethenusedindividually, thoughmoreoftenincombination,
toforecastmarkettrends.
It can also be said ofsubjectivepatterns, that both the predictorand
the outcome are binary variables. That is to say, they assume only two
possible values. With respect to the predictorvariable, the pattern orsig
nal iseither(1) currentlypresentor(2) notpresent.
102
With respectto the
outcome variable, the future price movement the pattern orsignal is pur
portedto predict, either(1) occursor(2) does notoccur.
For example, the head-and-shoulders top pattern is purported to be
correlated with (predictive of) a postpattern downtrend. At a given point
in time, a head-and-shoulder top pattern and neckline break signal is ei
therpresentonthechartoritisnot. Followingthesignalthedowntrend
103
either occurs orit does not. All subjectivepatterns andsignals can be in
terpretedwithinthisframework.
The central contention ofthis chapter is that subjective TA knowl
edge possesses only illusory validity. This section examines a specific
illusion of validity-illusory correlation. An illusory correlation is the
perception of a correlation between two variables that are not truly
related.



==================================================
                     PAGE 89                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 73
Why We Are Prone to Illusory Correlations
Asubstantial body ofresearch indicates that people easily fall prey to
the illusion ofcorrelation. These illusions seem to result from faulty
104
information processing. The specific fault is paying too much attention
to instances that confirm the existence ofthe correlation while paying
too littleattention to casesthateitherdo notconfirmthepresumedcor
relations orthat outrightly contradictits existence. In TA, confirmatory
instances are those cases in which the pattern/signal occurs and the
outcomethatthe pattern/signal is purported to predictalso occurs. For
example, the head-and-shoulders top pattern occurs and then a down
trend ensues. Ifyoubelievein the efficacyofaparticularsubjectivepat
tern/signal, ask yourself if your belief is not entirely or primarily
founded upon having seeninstances in which the patternhas been suc
cessful (confirmatory examples). Now ask yourselfifyou are similarly
aware of examples where it failed (false positive or false negative
signals105).
However, confirmatory instances constitute only one type out offour
possible types ofoutcomes that can occurbetween a pair ofbinaryvari
ables. The other three types are (2) the pattern/signal does occur butthe
market trend the pattern is supposed to predict does not occur, (3) the
pattern/signal does not occur butthe outcome the pattern is supposed to
predictoccurs anyway, and (4) the pattern/signal does not occur and the
outcome the pattern/signal purports to predict does not occur either.
Types one and four are correctpredictions. Types two and three are pre
dictionerrors. Typetwoiscalledafalse signalorfalsepositive.Typethree
iscalledafailure tosignalorfalse negative.
The Allure ofthe Upper Left Cell
The four possible outcomes between a pair ofbinaryvariables can be il
lustrated with a two-by-two (2 x 2) contingency table (Figure 2.15). This
common dataanalysis tool is composed offour cells, one for each ofthe
fourpossible outcometypes. Into eachcellgoes a countofthe numberof
instances ofthat type of outcome occurring within a sample ofobserva
tions. Such a table would be useful to represent subjective TA patterns
and their outcomes were it not for the fact that their subjectivity pre
cludesobtainingthecounts.
Studies show that people fall prey to illusory correlations because
they pay excessive attention to the number of confirmatory instances,
those falling into the upper-left cell ofthe table.106At the same time, they
fail to take proper account ofthe number of cases falling into the other



==================================================
                     PAGE 90                      
==================================================

74 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Predicted Predicted
Outcome Outcome
Occurs Does NOTOccur
Correct
Pattern/Signal Prediction Error
Occurs Confirmatory False Signal
Instances
Pattern/Signal Error Correct
Does NOT Failure to Prediction
Occur Signal No opportunity
E'IGURE 2.J5 2x 2contingency table.
threecells. We make thismistakefortwo reasons. Thefirst has to dowith
the salience ofthe confirmatory instances, and the second has to do with
a faulty intuitive notion of what constitutes sufficient evidence to con
cludethatacorrelationexists.
The salience of evidence refers to how noticeable, vivid, or out
standingitis. Salientevidenceisattentiongrabbing. Confirmatoryexam
ples are salient because they confirm a suspected hypothesis. In other
words they are rewarding. When subjective technicians think they have
discovered a useful pattern orsignal, theytypically will examine histori
cal datato see iftheir initial supposition was correct. Thus, they are en
gaged in informal hypothesis testing. In this situation, confirmatory
instances, examplesin which the pattern occurredand itmade acorrect
prediction, will be highly noticeable because they reward the analyst's
initial conjecture. Considerwhatitmight have been like for R. N. Elliott
just after formulating his initial hunch that all market movements con
form tothesamefundamentalpattern-afive-segmentimpulsewavefol
lowed by a three-segment corrective wave (5-3 pattern). At that point,
his hypothesis had not yet attained the status of a strong belief. It was
merely a suspicion. However, once that hypothesis was born in Elliott's
mind, it motivated a search for additional confirming cases. In other
words it was Elliott's initial conjecture itself that made confirmatory
cases salient.



==================================================
                     PAGE 91                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 75
Necessary versus Sufficient Evidence
Once the confirmatoryinstances take front stage, a faulty intuitionthat is
quite common enters the picture. This intuition, which is responsible for
producing the illusion ofa correlation, concerns the amount ofevidence
that is thought to be sufficient to establish a valid correlation. It is com
monly thought that confirmatory evidence, by itself, is sufficient. This is
incorrect!
It's not that this intuition is wrong so much as it is incomplete. Intu
itioncorrectlytellsusthatconfirmatoryinstances(eventsintheupperleft
cell) are indeed necessaryto establish the existence ofa correlative rela
tionship. If the head-and-shoulders pattern, or any pattern or signal for
thatmatter, is a valid predictor, thenthere oughtto be instances inwhich
the pattern occurred and the expected outcome also occurred. However,
itis a mistake to assume thata goodly number ofconfirmatoryinstances
are sufficient to establish a correlative relationship. In other words, al
though confirmatoryinstances are necessary, theyare not bythemselves,
sufficientto establishtheexistenceofacorrelativerelationship. Inreality,
the numberofinstances falling into allfour cells ofthe contingencytable
mustbetaken intoaccountto determineifvalid correlationexists.
It is because we fail to realize that a given beliefis not supported by
sufficientevidencethatittakesonanillusionofvalidity. Thuswe cometo
seeitnotasan opinionoranarticle offaith butas a reasonableinference
from objective evidence.107 Numerous studieslO8 have shown that, when
people rely oninformal analysis to determine ifthere is a relationship be
tween two variables, they tend to make two kinds oferrors. They fail to
detectcorrelationsthatare, infact, valid, evenwhenthe correlationis rel
ativelystronglO9 (invisible correlations), and theyfalsely perceive correla
tions thatare invalid (illusorycorrelations). Botherrors are relatedtothe
observer'sexpectationsstemmingfrompre-existingbeliefs.
First, let's consider what happens when people are shown data for
two variables) that are in fact related butwhich are not expected to be
10
correlated. Experiments have shown that they are unable to detect the
relationships unless the correlation exceeds approximately 0.70, on a
scale of0to 1.0, where 1.0 represents a perfect correlation and zero rep
resents no correlation. Acorrelation of0.70 is far strongerthan anything
likelyto be encounteredwithinthe domainofTA.))) Thisfinding suggests
that subjective TA analysts relying on informal/intuitive data analysis
methods will be prone to missing valid correlations that they do not ex
pectto exist.
What is even more relevant to the problem ofillusory TA knowledge
isthe tendency toperceive correlationsthatdo notexist. Whenobservers
haveapriorbeliefthata relationshipexistsbetweenpatternand outcome,



==================================================
                     PAGE 92                      
==================================================

76 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
orthereisamotivation tofind one, the confirmationbias, discussedprevi
ously, makes itlikely thata correlative relationship will be detected even
ifnoneexists.Itishard toimagineaTAanalystwhoisnotimpacted either
bypriorbelief, motivation, orboth.
Even if there is no prior belief or no motivation to believe, people
still display a pronounced tendency to perceive illusory correlations.
Smedslund performed one ofthe earliest experiments showing this ten
dency. Nurses were shown 100 cards that were supposed to represent
Jl2
actual patients. Each card indicatedifthepatientmanifesteda particular
symptom ornot (the binary predictor) and whether thatpatientwas ulti
mately diagnosed with a specific disease (the binary outcome). The
nurses' task was to determine ifthe symptom was truly correlated with
the disease. The data shown to the nurses is summarized in the contin
gency tableinFigure2.16.
Over85 percent ofthe nurses said the symptom was correlated with
the disease. Theywerepersuadedbythe37salientconfirmatoryinstances
falling inthe upperleftcell (symptom presentand diseasepresent). How
ever, theirconclusionwas notsupportedbythe data. Aproperanalysis of
this data, which takesall cell counts into consideration, indicates thatthe
symptom isnotcorrelatedwith the disease.
True Diagnosis
Disease Disease
Present NOT Present
Cell A Cell B
Symptom
Present 37 17
instances instances
Cell C Cell D
Symptom
NOT Present
33 13
instances instances
FIGURE 2.16 Data evaluated by nurses-Smedslund study(1963).



==================================================
                     PAGE 93                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 77
Giving All Cells Their Due
Aformal statistical test that considers all cell counts would be the rigor
ous way to determine ifthe nurses' datawas indicative ofa valid correla
tion. However, one need not even resort to a formal test to see that there
is no correlation between the symptom and the disease. All cells can be
taken into consideration by computing some simple ratios of the cell
counts. Forexample, theproportionofpatientsthatmanifested thesymp
tom and who ultimately proved to have the disease was approximately
0.69 (cell Adivided by [cell A+ cell B], 37/54 = 0.685). However, the pro
portionofpatientsthatdid nothave thesymptomthatultimatelywere di
agnosed with the disease was almost the same or 0.72 (cell C divided by
[cell C + Cell D], 33/46 = 0.717). This quick-and-dirty calculation shows
that likelihood ofhaving the disease was about the same (.69 and .72) re
gardless of whether the symptom was present. Had the symptom really
been correlated with the disease, the two proportions would have been
quite different, such as 0.90 of those with the symptom had the disease
whereas only 0.40 of those without the symptom had the disease. This
would not necessarily suggest that the symptom was a cause ofthe dis
ease butonlythatitwasa validpredictorofit.
In light ofthe fact that even simple ratios showed no correlation be
tween symptom and disease, itis surprising that85 percentofthe nurses
concluded that the symptom was a reliable predictor ofthe disease. The
nurseswerepresentedwithotherdatasetsaswell. Overall,thefactorthat
bestexplainedwhetherthe nursesperceiveda correlationwassimplythe
number ofconfirmatory instances (cell A: symptom and disease both oc
cur). Their judgments were unaffected by the relative proportions that
went into the calculation done earlier. In other words, the nurses did
notpayattention to the numberofcasesin the otherthree cells. As previ
ously noted, many psychological studies have supported Smedslund's
findings.
113
The statistically correct way to detect a correlation between two bi
nary variables is the Chi-square test.114 The test determines whether the
number of instances falling into each cell differs significantly from the
number that would be expected in each cell ifthere were no correlation
between the variables. In other words, the test determines ifcell counts
departsignificantly from a random sprinkling ofthe instances among the
four cells. When cell counts do depart significantly from a random pat
1l5
tern, the Chi-square statistic takes on a large value. This is legitimate evi
denceofacorrelation.
In Figure 2.17 the datafrom the nurses' study have been altered to il
lustrate how cell counts might appear for a symptom that is correlated
with the disease. Each cell also contains the number of instances that



==================================================
                     PAGE 94                      
==================================================

78 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Final Diagnosis
Disease Disease
Present NOT Present
Cell A Cell B
Symptom
54
Present 47 7
Expected =37.8 Expected =16.2
Cell C Cell D
Symptom
NOT Present 46
23 23
Expected =32.2 Expected =13.8
70 30 100
FIGURE 2.17 Evidence ofvalid correlation.
would be expectedifthere were nopredictiverelation betweensymptom
and disease (the random pattern). Forexample, in cellB, there were only
7occurrences ofdisease with the symptompresent (afalse signal). Ifthe
instances conformed to a random pattern, one would expect about 16
false signals. Thus, the relative proportion of disease, ifsymptom is pre
sent, is 0.87versus 0.50when the symptom is notpresent. The difference
in the proportionsshowsthatthesymptomiscorrelatedwiththe disease.
The Role ofAsymmetric Binary Variables
in lIIusory Correlation
Illusory correlations are especially likely to emerge when the variables
involved are binary variables of a particular type-asymmetric binary
variables.116 Binaryvariables can be either symmetric or asymmetric. A
binaryvariable is saidto be ofthe symmetric type ifeach ofits possible
values is associated with the presence of an attribute (redlblue or Re
publican/Democrat) or the occurrence of an event (rain/snow or



==================================================
                     PAGE 95                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 79
flood/fire). However, inthe case ofanasymmetric binaryvariable, one of
the variable's two values depicts the absence of an attribute or the
nonoccurrenceofanevent. Forexample, (attributepresent/attributenot
present) or(eventoccurred/eventdid notoccur). In otherwords, one of
thevariable'sstatesrepresentssomethingnothappeningorafeaturenot
being present.
Researchl17 shows that when both the predictor and outcome vari
ables are ofthe binary asymmetric type, the confirmatory instances (pat
tern occurs and expectedoutcomeoccurs) are extraordinarilysalientand
especially likely to consume the attention of the informal data analyst.
Thisattentionalshifttothe upperleftcellstronglyencouragesthepercep
tion ofillusory correlations. Recall that when the variables are asymmet
ric binaries, the other three cells of the contingency table contain
instancesinwhichoneorbothofthevariablesareregisteringtheabsence
of the pattern, the nonoccurrence ofthe expected outcome or both pat
ternabsenceand outcome nonoccurrence.
The nonoccurrence ofanevent, orthe absence ofanattribute, is eas
ily overlooked, because people have difficulty conceptualizing and evalu
atingthis kind ofevidence.Theresearchjustalludedtohasshownittakes
more cognitive effortto evaluate instances where something does notoc
curthan instanceswheresomethingdoes. Toillustrate the extracognitive
effort required to process a statementinvolving the nonoccurrence ofan
event or the absence of an attribute, consider the following two state
ments. Although they both convey the same information, the meaning of
thefirst, which isgiveninanaffim1ativeform, isintuitivelyobvious. How
ever, the meaning ofthe second, which is given in the negative forn1, re
quiressomethought.
Statement1:AUhumans aremortal.
Statement2:AUnonmortals arenonhumans.
Many of the propositions of subjective TA assert the existence of a
correlation between a pair of asymmetric binary variables: a signal/pat
tern thatiseitherpresentonthe chartornotand an outcomethatthesig
nal/pattern is supposed to predict, which either occurs or does not. Thus
the psychological research described earlier suggests that subjective TA
practitioners will be especially likely to fall prey to illusory correlations.
They will tend to be overly impressed with confirmatory instances, in
which the pattern/signal did occur and the expected outcome also oc
curred, and give too little attention and weightto the three othertypes of
instances. Some examples of subjective patterns along with their ex
pected outcomesare illustratedTable 2.1.



==================================================
                     PAGE 96                      
==================================================

80 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
'J)\JJLE 2.1 Subjective Patterns and Their Expected Outcomes
Pattern/Signal Expected Outcome
Head-and-shoulders top Downtrend
Completed 5th Elliott wave Corrective wave
Strong uptrend, then flag Continuation ofuptrend
Extremely bullish sentiment readings Bear market
Hidden or Missing Data Compounds the Problem
ofIllusory Correlations
Thediscussionsofarshowshowpeoplecometobelieveinboguscorrela
tions because ofan insufficient consideration ofall relevant information
in a contingency table. However, the subjective technician's plight does
notend here. Further compoundingthe propensityto detectillusory cor
relations is the problem ofmissing data. The cell counts needed to fill in
the contingency table are not even readily available. The missing data
problemisyetanotherconsequence ofsubjectiveanalysis.
Without objectively defined signals/patterns and without objectively
defined standards for evaluating their outcomes, it is impossible obtain
the requisite counts that are needed to fill the cells of the contingency
table. Withoutthese counts, the true efficacy ofthe signal/pattern cannot
beevaluated. Thisapplieseventothe confirmatoryinstancesfound inthe
upper left cell ofthe table, although subjective practitioners seem never
to run short of confirmatory examples.1I8 Objective definitions are the
only solution to the missing dataproblem. Until TA adopts objectivity, it
willcontinueto operateonthe basisofmagical thinkingand myth.
Once an illusory correlation is accepted as fact, two previouslymen
tionedcognitiveerrorsassurethemisconception'slongevity.Themerebe
lief in the specious relationship increases the likelihood that additional
supportive evidencewill benoticed (confirmationbias). Moreover, the al
leged correlation is easily assimilated into one's system of beliefs. As
Gilovichpoints out, once a phenomenonis suspectedto exist, people are
easilyableto comeupwithanexplanationaboutwhyitexistsandwhatit
means. Inotherwordshumansareverygoodatinventingstoriesafterthe
fact. Studies show that when people are falsely informed that they are
119
either above average or below average at some task, they can readily in
vent a reason that explains why.120 Part ofthe appeal ofsubjective TA is
the fascinating story offered about why it works. The story itself helps
sustainbelief.



==================================================
                     PAGE 97                      
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 81
ABehaviorist's Explanation ofIllusory
Correlations
Thus far, the discussion has explained the birth and longevity of erro
neousbeliefsintermsofcognitivepsychology.Aseparatesubdisciplineof
psychology, behaviorism, offers a somewhat different account of why
erroneous beliefsas to the efficacyofsubjectiveTAshouldbesodurable.
Behavioralpsychologyisconcernedwith roles ofreward (positive re
inforcement) and punishment (negative reinforcement) in the learning of
habitual behaviors. Ifa pigeon is put in a cage and rewarded with a food
pellet each time it pecks at a button, it will eventually learn the habit of
pecking the button. Once this behavior has been acquired, the bird will
continue pecking the button so long as the food pellets keep coming.
However, ifthe reward is stopped, the bird eventually learns that the be
haviornolongerpays, andthe learnedbehavioris extinguished.
One areaofinterestwithin behavioralpsychologyis the relationship
between the strength ofa habit, indicated by the degree to which it re
sists extinction, and the specific type of reinforcement schedule that
originallyled to the habit'sformation. The questionis which type ofrein
forcement schedule produces the strongest habits. The simplest type of
reinforcement is rewarding every action-each peck earns a pellet. An
other is partial reinforcement, in which rewards come only atfixed time
intervals, such as every 60 seconds, or fixed behavior intervals, such as
every tenth peck. It has been discovered that partial reinforcement
schedules produce slower learning (habit takes longer to form) but
strongerhabits.
Most relevantto the domain ofTAis the fact thatofall the partial re
inforcement schedules investigated, random reinforcement121 produces
habits thatare mostresistantto extinction. Underrandom reinforcement,
the organism's behavior is not truly associated with the receipt of a re
ward. Behavior is rewarded by chance. In one experiment, a pigeon
peckedatacoloredbuttonandreceivedrandomreinforcementfora mere
60 seconds. However, the pecking behavior continued for 3.5 hours after
the reward was stopped. In other words, the pathetic bird engaged in a
fruitless activity for a period that was 210 times longer than the random
reinforcementperiodthatproducedthehabit.
122
Random reinforcement is precisely the type of reward received by
someone who believes in an illusory TA correlation. Every so often, by
chance,thesignal/patternisfollowed bytheexpected outcome, thusrein
forcing the belief. Behaviorist studies would predict that such a belief
would be extremely resistant to extinction. Superstitions and compulsive
gambling, two habits that rest on illusory correlations, are known to be
extremelydifficultto cure.123



==================================================
                     PAGE 98                      
==================================================

82 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
When I worked on the floor ofthe American Stock Exchange, I met
several traders who displayed superstitious behavior. One fellow insisted
onwearingthesametie everyday, whichcontained anaccurate historical
record ofhistastesinsoup.Anotherwasadamantaboutenteringthetrad
ingfloor through one particulardoor ofthe several available. Another re
fused touse a redpen. Eachoftheseoddhabits wasprobablyinitiatedby
an accidental pairing of the behavior with a desired or undesirable out
comeand thenperpetuatedbyrandom reinforcement.
MISPlACED FAITH IN CHARTANALYSIS
Subjective technicians, irrespective ofthe specific method they practice,
believe in the efficacy ofvisual chart analysis. Price charts are presumed
to be a valid method for detecting exploitable order in financial market
data. The pioneers ofTA used charts to discover the discipline's founda
tional principles, and today they remain the primary analysis tool. This
sectionexplainswhyconfidencein chartanalysisis misplaced.
This is not to say that charts have no role in TA. They can serve as a
source of inspiration that leads to the forn1ation of testable hypotheses
about market behavior. However, unless and until these conjectures are
vigorously and objectively tested, they remain mere suppositions rather
thantrustworthy knowledge.
An Informal Search for Order Is Sure to Find It
Humans haveboththe need and the capacitytofind orderand meaningin
their experience. "It may have been bred into us through evolution be
cause ofits general adaptiveness: We can capitalize on ordered phenom
enain ways that we cannoton those thatare random. The predisposition
to detect patterns and make connections is what leads to discovery and
advance. The problem is that the tendency is so strong and so automatic
thatwesometimes detectcoherenceevenwhenitdoes notexist."124
A trait ofintuitive, nonscientific thinking is the tendency to accept
without question surface impressions and obvious explanations. Unfor
tunately, what is obvious at first blush is not necessarily valid. To the
ancientsitseemed obvious the sun orbited the earth. To the pioneers of
TA it seemed obvious that financial market prices formed patterns and
trends. Just as early observers of the heavens perceived mythical fig
ures like Leo the Lion and Orion the Hunter in the random arrange
ments of stars, the first technicians saw heads-and-shoulders, double
and triple tops, and other patterns in the meandering movements of



==================================================
                     PAGE 99                      
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 83
stockand commodityprices. These patternsform the lexicon ofsubjec
tive chartanalysis.
Our nervous system seems to be wired to see shapes regardless of
whether they are real. Dr. John Elder,125 a noted authority on predictive
modeling and data mining, calls this the "bunnies in the clouds" effect.
There is, however, good reason to question the validity ofchart analysis,
and byextension, to question thevalidityofknowledge basedonit. Icon
tend thattrendsandpatterns onchartsthatappearrealto the eyemayof
ten be illusions born of the mind's voracious, though indiscriminate,
appetite toperceiveorder.
Thisisnotmeanttoimplythatfinancialmarketsaredevoidofauthen
tictrendsand patterns. Infact, there issoundempiricalevidencethatpat
126
terns exist which have predictive power. It does mean, however, that
visualinspection ofchartsisan inadequate meansfor discoveringorveri
l27
fying the authenticity ofpatterns. Studies have shown that even expert
chartists cannot reliably distinguish between authentic stock charts and
simulated charts produced by a random process. A method of analysis
that cannot discriminate real charts from fakes cannot provide reliable
analysis of the real. Ajeweler who can't tell the difference between au
thentic diamonds and dime-store costumejewelryis in no positionto ap
praiseadiamondnecklace.
Illusory Trends and Patterns in Financial Data
Statistician HarryRoberts saidthattechnical analystsfall victim to the il
lusion of patterns and trends for two possible reasons. First, "the usual
method ofgraphingstockprices gives a picture ofsuccessive (price) lev
els rather than of price changes and levels can give an artificial appear
ance of pattern or trend. Second, chance behavior itself produces
patternsthatinvitespuriousinterpretations."128
Roberts showed that the same chart patterns to which TA attaches
l29
importance appear with great regularity in random walks. A random
walk is, by definition, devoid ofauthentic trends, patterns, or exploitable
orderofany kind. However, Roberts' random-walk chartsdisplayedhead
and-shoulder tops and bottoms, triangle tops and bottoms, triple tops and
bottoms, trend channels, and soforth.
You can create a random-walk chartfrom a sequence ofcoin flips by
starting with an arbitrary price, say $100, and adding one dollar for each
head and subtracting one dollar for each tail. In otherwords, the face of
the coinrepresentsapricechangeandthe chartshowsasequenceofsim
ulated price levels. Each simulated price in the sequence is equal to the
cumulative algebraic sum ofall priorrandom changes added to the start
ing value. Ifyou carry out this process for three hundred or so flips, you



==================================================
                     PAGE 100                     
==================================================

84 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
maybeamazedatthepatternsthatappear. Manywillbesimilartothepat
terns found on actual charts of financial assets. Clearly the patterns or
trendsthatappearona coin-flip chartcannotpredictitsfuture evolution.
This raises an importantquestion: ifchartist-like patterns can appear
with such regularityin random walks, and itis knownthatthese patterns
cannotbepredictive, howcanthesamepatternsbeconsideredpredictive
simplybecausetheyhappento appearona realchart?
PerhapsweshouldnottrusttheeyeofRoberts. Hewasastatistician,
not an expert in TA. Ifan expert chartist were able to spot the patterns
occurring in random-walk charts as fakes the way a competentjeweler
canspot a counterfeitdiamond, then Roberts's random-walk experiment
would not prove anything other than his lack of chart expertise. Cer
tainly, expert chartists should claim that they can tell the difference be
tween patterns occurring in random walks and those occurring on real
charts. Afterall, ifthe patterns on actual markets charts form as a result
ofchanges in supply and demand and/or oscillations in market mood, as
TAclaims, theyshouldlookdifferentfromtheaccidentalmeanderingson
a random-walkchart!
130
Unfortunatelyexpertchartistshave beenunable to tell the difference
between real and fake charts. Arditti put them to the test. When pre
131
sentedwithanassortmentofreal charts mixed with random-walk charts,
the chartistsexaminedbyArdittiwere nomoreaccurateattellingthereal
from the fake than a guess. This result was subsequently confirmed in an
informal study by Professor Jeremy Siegel of the Wharton Business
l32
School.133Siegelpresented the graphsshown in Figure2.18 (fourreal and
four random) to a group oftop Wall Street stockbrokers who considered
themselvescompetentchartreaders. Again, theexpertswereunabletore
liably distinguish the real from the random. I conducted a similar experi
ment using graduate business students who hadjust completed a course
in technical analysis. Their accuracy was consistent with guessing. The
real chartsare indicatedbytheX.
What can we conclude from these results? First, they do not imply
that financial markets are random, and hence devoid ofvalid trends and
patterns. But they do call into question the efficacy of subjective/visual
chart analysis as a means of knowledge acquisition and forecasting. If
chartreadingwereavalidskill,itshouldatleastbepossibleto distinguish
anactualchartfrom a randomfake.
Evidence ofIllusory Trends in Sports
Financial markets are not the only arenain which observers are plagued
by the illusion oforder. Many sports fans and athletes believe in perfor
mance trends, periods ofhotand cold performance. Whatbaseball fan or



==================================================
                     PAGE 101                     
==================================================

The Illusory ValidityofSubjective Technical Analysis 85
~ ~
1 2 1
~
k\d@
~Jl\~l\~.l'tJl\
..
'\rJ
~ ~
1~IGUKr~2.18 Real and randomly generated stock charts.
Siegel, Jeremy J.. Stocks for the Long Run, 3rd Edition, Copyright 2002, 1998,
1994, McGraw-Hili; the material is reproduced with the permission ofThe McGraw
Hill Companies.
playerdoesnotthinkbattingslumpsorhittingstreaksare real?Basketball
fans, players, coaches, and sports commentators speak of the so-called
hothand, a trend ofabove-averageshootingaccuracy.
These beliefs are widely held because trends in athletic performance
seem so obvious, However, rigorous statistical analysis ofplayer perfor
mancel34 indicates that the hot hand phenomenon is an illusion. The fans
have been duped by their intuition. This erroneous belief stems from a
faulty notionabouthow randomphenomenaoughtto behave. Common
135
sense would have us believe that streaks (asequence ofstrike-outs, or a
sequence ofsuccessful shots, and so forth) shouldnotoccurin a random
process. Rather, intuition incorrectly tells us that random processes
should display something resembling an alternating pattern of positive
and negative outcomes.
The truth is that sports trends are, by and large a cognitive illusion.
What appears to be a batting slump or a hitting streak is often nothing
more thana runofsimilaroutcomes, aphenomenonthatisquitecommon
in random processes. A valid trend is a streak of longer duration than
those that are ordinary in random data. That is to say, a real trend is a
streakofsufficientlengththatitwouldberare ina randomprocess.
Gilovich points out that "although a player's perfonnance does not
displaylongerstreaksthanthoseallowedbychance,itdoesnotmeanthat
the player's perfornlance is chance detem1ined. Itis not. Whethera given



==================================================
                     PAGE 102                     
==================================================

86 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
shot is hit or missed is determined by a host ofnon-chance factors, fore
mostamongthembeingtheskillofthe offensiveand defensiveplayersin
volved. However, one factor that does not influence or predict the
outcomeistheplayer'srecenthitrate."136
To understand why sports fans, athletes, and chart analysts fall prey
to the illusion oftrends and patterns in data that may in fact be random
we mustdelvea bitdeeperintothe nature ofintuitivejudgment.
THE INTUITIVE JUDGMENT AND THE ROLE
OF HEURISTICS
"Tosimplify,therearebasicallytwotypesofthoughtprocesses: automatic
and controlled."137 "Intuition is automatic. It is our capacity for direct
knowledge, for immediate insight without observation or reason."l38 It is
perception-like, rapid, and effortless, notes Princeton University psychol
ogist Daniel Kahneman. In contrast, "deliberate or controlled thinking
is reasoning-like, critical, and analytic."139 The prototype of controlled
thought is scientific reasoning. The prototype of intuition is the brilliant
medical diagnostician who always seems to sniffthe underlying disease.
Subjective technicians rely primarily on intuition, and they do so to their
detriment.
Human intelligence has limits. We can only pay attention to an
extremely small fraction of the information that comes to us through
our sensory organs. Moreover, the mind is not well suited to complex
tasks such as estimating probabilities and making decisions in a logi
cally and probabilistically correct manner in situations characterized
byuncertainty.
To cope, we must simplify. We reduce the complexities ofestimating
and decision making by resorting to a variety ofquick and dirty rules of
reasoning known as judgment heuristics. They focus our attention on a
limited portion ofthe total information picture and utilize relatively sim
ple forms of infonnation processing and reasoning to arrive at conclu
sions quickly. All ofthis is done withoutawareness orexertion. We make
intuitive judgments as automatically and effortlessly as we recognize
someone'sface, a taskforwhichthe mindisverywellsuited.
We acquire judgment heuristics from life experience. The term
heuristic refers to the notion of discovery by trial and error, and this is
howthese rules ofthoughtareacquired. Moreover, theyslip into usageso
subtly that we are unaware of how or when we learned them. And we
adopt them because, in general, they work. However, they work imper
fectly andwhentheyfail ourjudgmentsuffers.



==================================================
                     PAGE 103                     
==================================================

The JIIusory Validity ofSubjective TechnicalAnalysis 87
The good news is that heuristicjudgmenttends to errin a consistent
manner. Inotherwords, thejudgmentsare biased-theytendtobewrong
inthesamewayoverandoveragain.140Thisconsistencymakesitpossible
to predict the conditions under which heuristic judgments will fail and
how they will fail-whether the deviation from true value will be a posi
tiveoranegativeone.
Ingeneral, heuristicjudgmentstend to errinsituations characterized
byuncertainty.141 Financialmarketsarehighlyuncertain, sointuitivejudg
ments in this arena are likely to be biased. The specific bias that afflicts
subjective technicians is the propensityto seetrendsandpatterns indata
wheresuchstructuresare nottrulypresent. Inotherwords, there isasys
tematic tendency to perceive order where only random behavior exists.
This may explain why chartists are unable to distinguish real charts from
pseudochartsproducedfrom random data.
In the words ofProfessor Burton Malkiel, "Randomness is a difficult
notion for people to accept. When events come in clusters and streaks,
people look for explanations and patterns. They refuse to believe that
such patterns-whichfrequently occurin random data-could equallybe
derivedfrom tossingacoin. Soitisinthestockmarketaswell."142Malkeil
takes the extreme position that the market is random. I do notshare this
convictionbutagreethatpeoplemisperceiveorderindatathatisrandom.
Forthis reasonvisualinspection ofpricechartscannotbetrusted.
The illusionoftrendsandpatternsindatathatistrulyrandommaybe
attributable to the failure ofa specificjudgment heuristic called reason
ingbyrepr-esentativeness. Thatistosay, afaulty applicationofthe gener
ally useful representativeness rule biases us toward the perception of
orderwhereitdoes notexist.
Heuristic Bias and the Availability Heuristic
To recap, heuristics help us make complex decisions rapidly in spite of
the limitations ofhuman intelligence, but they can cause those decisions
to be biased. The notion ofheuristic bias is easily explained by consider
ingtheavailabilityheuristic.
We rely onthe availability heuristicto estimatethe likelihood offu
ture events. Itis based on the reasonable notion that the more easily we
can bring to mind a particular class of events, the more likely it is that
such events will occur in the future. Events that are easily brought to
mindaresaid to becognitively available. Forexample, plane crashes are
aclassofeventswithhighcognitiveavailability.
Theavailabilityheuristic makesacertainamountofsense. Theability
torecallaclassofeventsisindeedrelatedtohowfrequentlytheyhaveoc
curred in the past, and it is also true that events that have happened fre-



==================================================
                     PAGE 104                     
==================================================

88 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
quentlyinthe pastare generallymore likelyto occurin thefuture. Thisis
inkeepingwithonetheoryofprobabilitythatassertsthatthefuture likeli
hood ofan eventis related to its historical frequency.143 Taken as a class,
thunderstormshavebeenmorefrequentinthepastthanasteroidimpacts,
andthey do indeedhave a higherfuture likelihood.
The problem with the availability heuristic is that there are factors
that can enhance anevent's cognitive availabilitythat have nothingto do
with its historical frequency and are, therefore, irrelevant to estimating
its future likelihood. Consequently, our judgments of likelihood are
sometimes falsely inflated by the intrusion of these irrelevant factors.
Twofactors thathaveno relevancetoanevent'slikelihoodbutthatdoin
crease its cognitive availability are recency and vividness. That is to say,
howrecently144 aneventofthe type inquestion tookplace and howvivid
the eventwas both impacthow easilywe can bringsuch eventsto mind.
Consider plane crashes as a class of events. A plane crash thatjust oc
curred is both vivid and recent. As a result, in the period right after a
well-publicized plane crash many people tend to overestimate the likeli
hood offuture plane crashes and are inordinately fearful offlying. Note
the biasinthejudgmentis one ofoverestimatinga probability. The avail
ability heuristic never causes us to underestimate an event's likelihood.
The erroris always one ofoverestimatingthe probability.
The Representativeness Heuristic:
Reasoning by Similarity
The representativeness heuristic, which was first identified by Tversky
145
and Kahneman is ofparticular relevance to subjective TA. We use this
rule to make intuitive classificationjudgments. In otherwords, it is used
to estimate the probability that a particular object, for example the dog
before me, belongs to a particular class ofobjects, for example the class
ofpoodles. The underlying premise ofthe representativeness heuristic is
that each member of a class should display the key common attributes
thatdefine the class. Itisquite reasonable to expectthatobjectsfrom the
same class should possess a similarsetofcharacteristics. In arriving ata
judgmentbyrepresentativenessaboutwhetheragivenobjectbelongstoa
particular class, we unconsciously consider the degree of similarity or
match between the attributes ofthe object and the set ofattributes pre
sumed to be representative ofthat class. This is why this heuristic is re
ferred asa reasoningbyrepresentativeness.
Inthissectionandthenext, Iwillarguethatvisualchartanalysisisin
herentlyflawed because ofa biastoward the perception oforderandpat
ternevenindatathatistrulyrandom. Iwillcontendthatthisbiasisdueto
a misapplication of the representativeness heuristic. In other words, a



==================================================
                     PAGE 105                     
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 89
bias stemming from a faulty application ofthe representativeness heuris
tic can explain whysubjective techniciansfall preyto an illusion oforder
inchartsthatwere, infact, produced bya randomprocess.
Toexplainhowchartistsmakethismistake, whichisrathersubtle,we
will take a briefdetour by considering a more easily understood case of
heuristic biascausedbyrepresentativeness reasoning.
As mentioned earlier, reasoningby representativeness ispremised on
the notion that a class ofobjects orevents can be represented by a class
stereotype-atypical example that possesses the class's key distinguish
ing characteristics. Often this line of reasoning works well. However, it
fails when an object's most noticeable features are not reliable indicators
ofthe object's class. In other words, when the characteristics that most
l46
draw ourattention are notrelevantinjudgingthe object's class reason
ing by representativeness tends to fail. Unfortunately, intuitivejudgment
tends to be based on the most obvious features ofan object or event re
gardlessofwhetherthesefeatures are trulyuseful indicatorsofitsclass.
Reasoning by representativeness estimates the probability than an
object belongs to a given class by considering the degree of match be
tween the object's most salient attributes and the mostsalient attributes
of the class stereotype. The greater the number of matching attributes,
the higherthe adjudged probabilitythatthe objectisindeeda memberof
the class in question. "Modern psychologists hypothesize that our con
cepts of categories (classes) like bank tellers, feminists, microcomput
ers, skunks, and all kinds ofthings are represented cognitively as lists of
attributesthatwe believeare definingcharacteristicsofthoseentities."147
This is illustrated in Figure 2.19. In many instances this fast and frugal
classification rule gives accurate results. People would not use such a
rule itifitdid not.
Let's consider an example where the application of the representa
tiveness rule leads to an erroneous judgment about an object's class.
Imagine that you havejusttaken your seat on a commercial airline flight
and you become concerned thata person (the object) seated next to you
may be a terrorist (the class). Your apprehension is aroused because the
person has the following obvious characteristics: man ofapparently Mid
dle Eastern origin, mustache, 25-30 years ofage, wearing a turban, read
ing the Koran, and appearing nervous. Your conclusionthat the man may
very well belong to the class of terrorists occurred quite spontaneously
because his obvious characteristics are similar to a set ofcharacteristics
that was also possessed by (i.e., is representative of) the terrorists who
conducted the attacks of September 11, 2001. Your inference was based
on the representativeness heuristic. Now consider how you would have
reactedifthe passengerwere a young Middle Easternman butwas wear
ing blue jeans and a tee-shirt with an obscene logo, had on a New York



==================================================
                     PAGE 106                     
==================================================

90 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
C K Q
• •
,
Object
of
Uncertain
Class
Stereotypical Class Member
Class
FIGURE 2.19 Classification by numberofmatching attributes.
Yankee baseball cap instead ofa turban, was reading a copyofPlayboy
instead ofthe Koran, and was listening to rock music on an iPod while
blowing chewing-gum bubbles. You would not have been worried. Intu
itive judgments of class rest on the assumption that the most obvious
characteristics of an individual are reliable indicators of that individ
ual's class.
Judgments based on representativeness can err because they fail to
take into accountstatistical and/orlogicalelements ofa situationthatare
truly relevant to an accurate judgment about its class. The probabilisti
cally correct way of estimating the class of an event or object is called
Bayes' theorem,148whichis derivedfrom theaxioms ofprobabilitytheory.
Ifitwere applied to the airlinepassengersuspected ofbeinga terrorist, it
would take into account such things as the proportion ofterrorists in the
world (Le., the base rate ofthe class ofterrorists), the proportion ofpeo
ple in the world that possess the set ofobserved characteristics (i.e., the
base rate of the pattern), and the proportion of terrorists that have this
patternofcharacteristics (Le., the conditionalprobability that these char
acteristicsarepresentgiventhattheindividualisinfactaterrorist). When
these values are plugged into the equationthat expresses Bayes' theorem
we obtain the conditional probability that the person is a terrorist, given
thattheypossessthe patternofcharacteristics.



==================================================
                     PAGE 107                     
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 91
Because intuition operates on the basis ofrepresentativeness, rather
than Bayes' law, we tend to commit an error called the base ratefaUacy.
Thisisthefailure to considerthe baserate statisticscharacterizinga situ
ation.Terrorists,fortunately, areveryrare (i.e., haveaverylowbaserate).
Thus,amongtheentiresetofpeoplewhopossessthesamesalientcharac
teristicsas the terroristsof9/11/01,thefraction thatareactuallyterrorists
is extremely small. Perhaps only one person in a million with these traits
is actually a terrorist. Thisfact is highly relevanttojudgingthe likelihood
that the person seated next to us is dangerous. However, we tend to ne
glect base-rate statistics because intuitivejudgmentis focused on the at
tention-grabbingfeatures ofasituationorobjectratherthanrelevantfacts
suchasabase-rate.
An example of fallacious reasoning by representativeness was dis
played by market analysts who predicted that a deflationary depression
would follow the stock market crash of October 1987. They were capti
vated bythe very noticeable similarity between the behaviorofthe stock
marketin 1929,whichdid leadto adeflationarydepression, and itsbehav
ior in 1987. Both witnessed market crashes. This glaring similarity took
analysts' attention awayfrom significantdissimilarities between 1987and
1929. Inotherwords, 1987and 1929were differentwithrespecttofactors
thatdo havesomepredictiveinformationwith respectto deflationarycol
lapses. An examination ofmarket crashes over the last 100 years shows
thataprice crashis nota reliablepredictorofa deflationarydepression.
Another error associated with representative reasoning is called the
conjunction fallacy, a failure to take into account a basic law of logic
telling us that a proper subset must contain a smallernumber ofobjects
thanthelargersettowhichitbelongs. Thesethorsesisapropersubsetof
the larger set, animals. It follows that horses must be fewer in number
than animals ingeneral, whichincludeshorses, bluebirds, aardvarks, and
so forth. However, when people rely on the representativeness rule, they
seem to ignore this law oflogic and committhe conjunction149fallacy. In
one experiment conducted by Tversky and Kahneman,150 subjects were
presentedwith thefollowing description:
Linda is 31 years old, single, outspoken, and very bright. She ma
joredinphilosophy. As astUdent, shewas deeply concernedwithis
sues ofdiscrimination and socialjustice, and also participated in
antinuclear demonstrations. Which possibility is most likely? (1)
Lindais a bank teller, or(2) Lindais a bank teUerANDis activein
thefeminist movement.
Astoundingly, 85 percent ofthe subjects responded that it was more
likely that Linda was a bank teller AND active in the feminist movement



==================================================
                     PAGE 108                     
==================================================

92 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
than that Lindawassimplya bankteller. This conclusionignores the logi
cal relationship between a set and a proper subset. Subjects became so
fixated onthefactthatLindapossessedasetofsalientcharacteristicsthat
matched an intuitive stereotype of a feminist, that they committed the
conjunction fallacy. Clearly, the set ofwomen who are both bank tellers
andactiveinthefeministmovementisasubsetofa largercategorywhich
contains all female bank tellers including those who are feminists and
those whoarenot. Otherstudiesconfirmedthesefindings and ledTversky
and Kahneman to concludethatpeople'sprobabilityjudgmentsare biased
by the addition of salient detail. Even though each additional detail de
scribing an object reduces the probability that such an object exists,
\5\
when peoplereason byrepresentativeness, additionaldetailincreasesthe
number of characteristics that match a class stereotype, thus increasing
an intuitive estinlate ofprobability. The conjunction fallacy is illustrated
inFigure2.20.
The conjunction fallacy can have disastrous consequences when it
seeps intojuryverdicts. For example, considerwhich oftwo possibilities
wouldseem morelikely
152:
1. Thedefendantleftthescene ofthe crinle.
2. The defendantleftthescene ofthe crime for fear ofbeingaccused of
murder.
BankTellers
Who Are
Feminists
FIGURE 2.20 The conjunction fallacy.



==================================================
                     PAGE 109                     
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 93
The second hypothesis involves a conjunction of"leftthe scene" and
"fearofaccusation."Becausethesecondalternativeisasubsetofthefirst,
it is by definition less probable. Clearly, there are reasons for someone's
leaving a crime scene otherthan fear ofbeing accused, like going to pick
up the dry cleaning. However, theadditional detail ofthesecondhypothe
sislendsa ring ofplausibilitybecause itincreasesa sense ofmatch to ac
tions thatwould berepresentativeofaguiltyparty.
THE REPRESEN11\TIVENESS IIEURISTIC AND THE
ILLUSION TRENDS AND PA1'TERNS IN CHARTS:
REAL AND FAKE
Sofarthe representativenessheuristichas beendiscussed with respectto
its use in classifying objects. In fact, the heuristic is also involved in a
more abstract type ofclassificationjudgment that is central to the prac
tice ofsubjective TA: judging that a sample ofdata evidences trends and
patterns and is, therefore, worthy offurther analysis oralternatively that
the dataisa random hodge-podge notworthyoffurther analysis.
Bearin mind thejudgmentaboutwhethera datasetappears random
or nonrandom is not made with conscious awareness or intention. It oc
curs as automatically and unconsciously as thejudgmentinvolved in rec
ognizing the face ofa friend. All heuristicjudgments, including reasoning
by representativeness, occurautomaticallyand withoutconsciousaware
ness. As a result, whena chartistconcludesthata sample ofpricehistory
contains authentic patterns and trends (i.e., that the data is nonrandom)
and is, therefore, worthyofanalysis, itis nota matterofcontrolled think
ing. That is to say, the chartist does not consciously pose the question:
"Doesthisdataappearas ifitcontainsexploitable orderand structure."
Although heuristic judgments slip into our thinking unnoticed, they
can have a profound influence on ourbeliefs. The problem is that heuris
tic thinking fails to take into account important features ofthe data that
are relevant to determining if it does contain authentic patterns and
trends thatmightbe exploitedwithfurther analysis.
In a previous section, I asked you to create an artificial price history
(random-walk chart) by flipping a coin 300 times. If you did not do so
then, I urge you to do so now. Not only will it make this section more
meaningful, it may permanently change the way you look at charts. To
quote MarthaStewart, "that'sagood thing."
Ifthiswasyourfirst exposure to datagenerated bya randomprocess,
you may be surprised by how nonrandom it looks. You may not have ex
pectedtoseeformationsthatlooklikechartpatterns(head-and-shoulders,



==================================================
                     PAGE 110                     
==================================================

94 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
doubletop, etc.). Thequestionwe mustconsideristhis: Whydoesthisse
quence, which we know to be random in origin, give the appearance of
having authentic patterns andtrends. Or, said differently, why does data
generated by a purelyrandom process appearas ifit were generated by
a process governed by rules, which, if discovered, would make predic
tionpossible?
Yourchartmayhave lookedsomethinglikeFigure2.21.
The bottom line is this: That which appears to be nonrandom is not
necessarily nonrandom. My contention is that the perception of trends
andpatternsinrandomlygenerateddataoccurs because ofafaulty appli
cation of the representativeness heuristic. Specifically, the heuristic cre
atesafaultyintuitivenotionofhowrandomdataissupposedtolook. That
is to say, it produces a false stereotype of randomness. Thus, when we
viewdatathatdoesnotappeartomatchthisfalse stereotype,weautomat
ically and erroneously conclude that the data is nonrandom. From this
flawed conclusion, itis a shortleap to the equallyfalse perception ofpat
terns and the equally false notion that the patterns contain predictive
information.
One consequence of representativeness-based reasoning is the ex
pectation that an effect should resemble its cause. This is often reason
able. The Earth (big cause) is responsiblefor gravity (big effect), while a
mere pebble in our shoe causes only a minor annoyance. However, the
rule that an effect should resemble its cause does not always hold. The
1918 flu pandemic, a big effect, was caused by a creature of submicro
scopicproportions.
Howdoesthis relateto chartsofassetprices?Asetofdataona chart
can be seen as an effect of a process, the activity of a financial market.
The process is the cause whereas the data is the effect. A chart of our
Price
Time
FIGURE 2.21 Data from a random process that appears ordered.



==================================================
                     PAGE 111                     
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 95
body's hourly temperature over a 24-hourperiod is an effect caused by a
processthathappenstobeourbody'smetabolism.
Because representativeness-basedthinking operates involuntarilybe
neath conscious awareness, when we view the chart of an asset's price
history, we tend to draw an automatic conclusion about the nature ofthe
process that generated the chart. Perhaps, if stock charts looked some
thinglikethetraceinFigure2.22, thepioneersofTAwouldhavebeenless
likely to see exploitable structure, and the practice of TA would never
have been born. This data issupposed to approximate an intuitive stereo
type ofrandomness: haphazard, disorderly, and withoutanyhintoftrend.
Onthe basis ofthe representativeness heuristic, we presume that ifa
process(thecause)israndom, itseffect(thedata)shouldalsoappearran
dom. As a consequence ofthis assumption, any sample ofdata that devi
ates from the presumed stereotype ofrandomness, because it manifests
some sense ofpattern ortrend, is concluded to be the product ofa non
random process. Stock charts give the appearance ofhaving been gener
ated by a nonrandom process. In other words they match an intuitive
notionoforderandpredictability. Bearinmind thatfinancial marketsmay
indeed be nonrandom to some degree and hence be amenable to predic
tion. However, that is not the issue here. Rather the issue is can non
random structure that is amenable to prediction be detected by visual
inspectionandintuitive appraisal?
As mentioned previously, the deficiencies ofreasoning byrepresenta
tivenessaredue to anexcessivefocus onobviousappearancesandthesi
multaneous disregard ofimportantstatistical and logical features. As will
beexplainedinChapter4, one ofthe mostimportantfeatures ofa sample
ofdata,fromthestandpointofdataanalysis,isthenumberofobservations
comprising the sample. Valid conclusions about a sample of data must
take the number of observations into account. Moreover, the statistical
Price
Time
FIGURE 2.22 Intuitive stereotype of random data: haphazard, disordered,
nontrending.



==================================================
                     PAGE 112                     
==================================================

96 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
characteristics of a large number of observations are required to accu
rately represent the true nature of the process that generated the data
(e.g., random or nonrandom). Conversely, small samples of data often
convey the illusion ofpatterns and trends because the statistical charac
teristics of a small sample of data can be wildly unrepresentative of the
datageneratingprocess.
Icontendthatthebeliefthatvisualchartanalysisisavalidmethodfor
detecting trends and patterns is based on a flawed chain ofthinking that
goessomethinglikethis:
1. On the basis of the representativeness heuristic, it is erroneously
thought that data generated by a random process should look ran
dom-aformless haphazard flip-flop withoutany hint ofpatterns, or
ganizedshapesortrends.
2. This stereotype is erroneously assumed to manifest in all samples of
random data, irrespective ofthe number of observations comprising
thesample.
3. Asmallsampleofdataisexaminedanditappearsnottomatchthein
tuitivestereotype ofrandomness.
4. The datais thus deemed to betheproductofa nonrandom process.
5. Nonrandom processes are amenable to prediction, hence it would be
reasonable to lookforpatternsand trends in the datato makepredic
tions.
ThisisillustratedinFigure 2.23.
The principal flaw in this chain ofreasoning is the failure to consider
the matter ofsample size (step 2ofthe preceding list). The presumption
thata sample ofdatacanaccuratelyreflectthe nature ofthe datagenerat
ing process, irrespective of the number of observations comprising the
sample, violatesanimportantprincipleofformal dataanalysis, the Lawof
Large Numbers. Reasoning byrepresentativeness fails to respectthe Law
ofLarge umbers. In otherwords, itcommitsthe crimeofsmall numbers
orsamplesizeneglect. Thepenaltyisautomatic membershipinTheFools
ofRandomnessl53club.
The Law of Large Numbers and the Crime
of Sample Size Neglect
The Law ofLarge Numbers tells us that sample size is ofparamount im
portance because itdetermines the confidence we can have ininferences
basedonasampleofdata. Also knownastheLawofAverages, the Lawof
Large Numbers tells us that only samples comprised oflarge numbers of



==================================================
                     PAGE 113                     
==================================================

The Illusory Validity ofSubjective TechnicalAnalysis 97
Does The Data Contain Predictable Order?
Yes: Data sample does not match intuitive stereotype of randomness
7
Stereotype
Data Sample
Random
Data
Non-Random
Process
FIGURE 2.23 Data is presumed to be amenable to prediction because it does
not match an intuitive stereotype ofrandomness.
observations reliably and accurately reflect the characteristics of the
processthatgenerated thesample. Or, said differently, onlylargesamples
can inform us about the characteristics ofthe populationfrom which the
samplecame. Thus, the largerthenumberofcointosses, the moreclosely
the proportion ofheadsin the samplewill reflectan essentialtruthabout
fair coins: the probabilityofheadsis0.50. The bottomline is we canhave
greater confidence in what we learn from large samples than from small
ones. TheLawofLargeNumberswill bediscussedingreaterdepthincon
nection with statistics in Chapter 4. For the present discussion, what is
important is the connection between the Law ofLarge Numbers and the
appearanceofillusorypatternsandtrendsinsmallsamplesofdata.
Small samples can be misleading because they can manifest charac
teristicsthatareverydifferentfromthetrue characteristicsoftheprocess
thatgeneratedthesample. Thisexplainswhyasmallsegmentofapseudo
stockchart-onethatwasactuallygeneratedbyarandomprocess-gives
the appearance of authentic patterns and trends. Of course, by design,
suchachartcannothavereal trendsandpatterns. Thus, a reliable conclu
sion about whether a sample ofdata was the result ofa random or non
randomprocesscannotbe obtainedfrom asmallsample. Thisis depicted
byFigures2.24and 2.25. Whentheparts(smallsamples) ofthechartlook
nonrandom, an illusion ofnonrandomness is created in the entire chart,
whentheyareputtogether.
Coin-toss experiments illustrate howsmall samples canproducemis
leadingimpressions. The obvious statistical characteristic ofthis random



==================================================
                     PAGE 114                     
==================================================

98 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Random Process
SmallSample SmallSample SmallSample
IllusoryUptrend IllusoryH&SPattern IllusoryDowntrend
FIGURE 2.24 The illusion oftrends and patterns in small samples-data gener
ated byarandom process.
processis that there are two equallylikelyoutcomes. Thisis the basisfor
an expectation that the proportion of heads is 0.50. This intuition is in
deed correct when the number oftosses is large. In a sample of 100,000
tosses, theproportionofheadswillbeextremelycloseto0.50.Ifonewere
to repeat this experiment many times, the proportion ofheads would al
ways beveryclose to 0.50. Infact, itwould be unlikely to fall outside the
range of0.497-0.503.154
The problem is intuition pays no attention to sample size orthe Law
ofLarge Numbers. So we also expect0.50heads to appearinall samples.
Althoughitis true thatthe expectedproportion ofheadsin anysample is
always0.50, evenforjusttwo tosses, theproportion ofheads thatwill ac-
Price r·······························:
Illusory Up-Trend Illusory Down-Trend
Time
FIGURE 2.25 If small parts ofa chart look orderly, the whole chart assumes a
nonrandom look.



==================================================
                     PAGE 115                     
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 99
tuallyappearinagivensmallsample candepartsignificantlyfrom 0.50. In
ten tosses, for example, three heads (0.30) orsevenheads (0.70) can eas
ily happen. In a sample of 100,000 tosses, 0.30 heads (30,000) or 0.70
(70,000) wouldbevirtuallyimpossiblewithafair coin.
Another faulty intuition about random processes is that outcomes
shouldalternate (up/down, head/tail) morethantheyreallydo. Whenpeo
ple are asked to imagine what a sequence of coin tosses would be like,
they tend to imagine sequences that show far more head Itail alternation
155
than would really occur in a random sequence. In reality, random
process displays less alternation and longer streaks of similar outcomes
(up, up, up) than is widely believed. Therefore, when a set of data does
notmatch the false expectation ofan alternatingsequence, itgives rise to
the illusionofatrend. Forexample, itseemsintuitivelycorrectthatacoin
toss would be more likely to produce the sequence H,T,H,T,H,T than the
sequence H,H,H,H,H,H. However, both sequences are, in fact, equally
probable,156so we come awaywith the erroneousimpressionthatthefor
mer sequence suggests a random process whereas the latter suggests a
processthatdisplaysatrend.
Because random sequences actually alternate less and streak more
than ourintuitive conception, sequencesthatactuallyare random appear
as if they are nonrandom. For example, the average basketball player
makesabout50percentoftheirshots, similarto the probabilityofgetting
heads. Thus, there is a reasonably good chance of making four, five, or
even six shots in a row if20 shots are taken in a game. In other words,
chancefavors theappearanceofwhatsportsfans callahot-hand,157which
is, infact, nothingmore thanastreaktypicalofarandomsequence.
The erroneous notion that random processes should alternate more
and streak less is responsible for two false perceptions: (1) the cluster
ing iUusion, and (2) thegambler'sfaUacy. In this context, the term clus
terrefers to a clump, either in time or space, ofsimilar events. Acluster
ofsimilar outcomes in a random time series gives the illusion ofa trend
that has momentum and this creates the false expectation thatthe trend
should persist. The gambler's fallacy is the equallyfalse expectation that
a cluster of similar outcomes should not occur, and when one does, it
givesrisetothefalse expectationthatareversalisdue. Thegambler'sfal
lacy is exemplified by a person who thinks a tail is more likely after a
stringofheads.
It is an observer's prior beliefs about a process that determines
whether he will fall prey to the clustering illusion or the gambler's fal
lacy. Suppose aperson isobservingaprocessthatis, infact, random but
does not realize that it is. That person will tend to fall prey to the clus
tering illusion if they hold a prior beliefthat the process is nonrandom
and that it should manifest trends. Sooner or later a streak occurs, and



==================================================
                     PAGE 116                     
==================================================

100 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
the priorbeliefis confirmed. The observerhas fallen prey to the cluster
ing illusion.
In contrast, ifthe observer has a prior beliefthat the process is ran
dom, it is more likely that they will fall prey to the gambler's fallacy.
Soonerorlaterastreakoccurs, for example four heads ina row, whichis
then followed by a tail. Although this would be quite normal in a random
process, the appearance of the tail reinforces the false belief that a tail
was due. Therefore, when a process is truly random both the observer
who thinks the process is nonrandom and believes that trends (clusters)
should occurandthe observerwho thinksthe processis random and that
the trends shouldreverse will have their beliefs rewarded. In truth, when
a process is random both observers' expectations are false. Random
walks have no memory, andprioroutcomeshave no effectonsubsequent
events.
Theclusteringillusioncanalsooccurinaspatialcontext.Thepercep
tionofillusoryshapesinrandomdataissimplyanothermanifestationofa
false expectationaboutrandomness. Thefalse expectationisthatrandom
data should not display any hint of organization or form. Any departure
from this expectation is interpreted, falsely, as a sign of a nonrandom
process at work. Acase in which illusory spatial patterns in random data
encouraged the formation of erroneous perceptions occurred during the
World War II bombing ofLondon by German V-I and V-2 missiles. The
158
newspapersranmapsofmissileimpactsites, and readershad theimmedi
ateperceptionthatthestrikeswere clustered. Thisgave rise to an equally
false, after-the-fact interpretation that impact patterns were evidence of
aneffortbytheGermanstoavoidhittingcertainpartsofLondon, whichin
tumspawnedfallacious causalreasoning. Londonerscametobelieve that
these areas were spared V-2 attacks because they housed German spies.
However, a formal analysis ofthe missile impact sites showed that they
wereentirelyconsistentwith a randompattern.
A more recent example of illusory spatial clustering that fostered
erroneous causal reasoning was the so-called cancer-cluster hysteria in
California. People became alarmed if the number of diagnosed cases of
cancerofa particulartype (e.g., lung cancercaused by asbestos) was far
above average in their community. Itturns outthat clusters can occur by
chance. What is not apparent to the statistically unsophisticated is this:
Given5,000censustractsinCaliforniaand80possibleenvironmentalcan
cers, therewillbesometractsinwhichthenumberofcancercaseswillbe
far above average simply due to chance. In some cases, such clusters re
ally are attributable to environmental toxins, but not always and not
nearlyas oftenasintuitionwouldsuggest.
This brings us backto TAand the appearance oforderlypatterns like
double tops, head-and-shouldersbottoms, andsoforth. Theoccurrenceof



==================================================
                     PAGE 117                     
==================================================

The Illusory ValidityofSubjective TechnicalAnalysis 101
suchshapesis notconsistentwithanintuitivenotionofdataproduced by
a random process. Such data is erroneously presumed to be completely
haphazardandshapeless, so,wejumptothe conclusionthepatternsmust
bethe by-productofanonrandomprocessthatisnotonlyamenabletovi
sual analysis but that the patterns themselves are useful for making pre
dictions. As Roberts showed, random walks can easily trace the patterns
thatsubjectivetechniciansholdsodear.
Insummary,peoplehavea hard timetelling whenasetofdataisran
dom ornot. Because the mind is predisposed to the perception oforder
and adeptat inventingstoriesthatexplainwhythat orderexists, itis not
at all mysterious that the pioneers ofTA would find patterns and trends
in price charts and then invent theories about why such patterns should
occur. Methods more rigorous than visual analysis and intuition are
needed to find the exploitable order that may exist in financial market
fluctuations.
THE ANl'ID01'E 1'0 ILLUSORY KNOWLEDGE:
THE SCIENTIFIC METHOD
This chapter examined many ways we can be fooled into adopting erro
neous knowledge. The bestantidote everinventedfor this problem is the
scientificmethod, thesubjectofthe nextchapter.



==================================================
                     PAGE 118                     
==================================================





==================================================
                     PAGE 119                     
==================================================

The Scientific
Method and
Technical
Analysis
T
1\S central problem is erroneous knowledge. As it is traditionally
practiced,muchofTAisabodyofdogmaandmyth,founded onfaith
and anecdote. This is a consequence ofthe informal, intuitive meth
ods used by its practitioners to discover patterns with predictive power.
Asadiscipline, TAsuffersbecauseitispracticedwithoutdiscipline.
Adopting a rigorous scientific approach would solve this problem.
The scientific method is not a cookbook procedure that automates
knowledge discovery. Ratheritis "asetofmethods designedto describe
and interpret observed orinferred phenomena, past orpresent aimed at
buildingatestablebodyofknowledge opento rejectionorconfirmation.
In other words, (it) is a specific way of analyzing information with the
goal of testing claims."l This chapter summarizes its logical and philo
sophical foundations and discusses the implications of its adoption by
TApractitioners.
THE MOST IMPORTANT KNOWLEDGE OFALL:
A METHOD TO GET MORE
"Ofall the kinds of knowledge that the West has given to the world, the
most valuable is the scientific method, a set ofprocedures for acquiring
new knowledge. It was invented by a series of European thinkers from
about 1550to 1700."2 Compared to informal approachesitis unsurpassed
initsabilitytoseparatefact from falsehood. The dramatic increase inour
103



==================================================
                     PAGE 120                     
==================================================

104 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
understanding of and control over the natural world over the last 400
yearsattests to the powerofthescientificmethod.
Themethod's rigorprotectsusfrom thefrailties ofmind, whichoften
impair what we learn from experience using less formal methods. Al
though informal lmowledge acquisition works well for many ofthe obvi
ous truths of daily living, sometimes what seems obvious is not true. It
wasobvioustotheancientsthatthesunrevolvedaroundtheearth.Ittook
science to show this was false. Informal observation and intuitive infer
ence are especially prone to failure when phenomena are complex or
highly random. Financialmarketbehaviordisplaysboth.
Historically, TAhas notbeenpracticedina scientificmanner, butthis
is now changing. In academia and the business world, a new breed of
practitioners, lmown as quants, have been adopting ascientific approach.
Indeed, some ofthe mostsuccessful hedgefunds are usingstrategies that
could becalledscientificTA.
Unsurprisingly, many traditional TA practitioners have resisted this
change. Vested interests and habitual ways ofthinking are hard to aban
don. No doubt there was opposition when folk medicine evolved into
modern medicine, when alchemy progressed into chemistry, and when
astrologyadvanced into thescience ofastronomy. Rancorbetween tradi
tional andscientificpractitionersisto be expected. However, ifhistoryis
anyguide, traditionalTAwill, inthe end, bemarginalized. Astrologers, al
chemists, and witch doctors still practice, but they are no longer taken
seriously.
THE LEGACYOF GREEK SCIENCE: A MIXED BLESSING
The Greeks were the first to make an effort at being scientific, though
their legacy proved to be a mixed blessing. On the positive side of the
ledgerwastheinventionoflogic byAristotle.Theformal reasoningproce
dures he developed remaina pillaroftoday'sscientificmethod.
On the negativesidewerehisfaultytheoriesofmatterand motion. In
stead of being generalized from prior observations and tested against
freshly observed facts, as is the practice in modern science, his theories
were deduced from metaphysical principles. When observations con
flicted with theory, Aristotle and his disciples were prone to bending the
facts rather than altering orabandoning the theory. Modern science does
the opposite.
Aristotle ultimately realized that deductive logic was insufficient for
learning about the world. He saw the need for an empirical approach
based on inductive logic-observation followed by generalization. Its in-



==================================================
                     PAGE 121                     
==================================================

The Scientific Method and Technical Analysis 105
vention was his most significant contribution to science. However, he
failed in the application ofhis own invention. At his institution ofhigher
learning, the famed Lyceum in Athens, Aristotle and his students made
meticulous observations on a wide range ofnaturalphenomena. Unfortu
nately, the inferences they drew from these facts were biased by Aris
totelian dogma and often based on inadequate evidence.3 There was too
muchtheorizingfrom toolittleevidence. Whenfacts contradictedfavored
firstprinciples,Aristotlewouldcontortthefactsto conservetheprinciple.
UltimatelytheAristotelianlegacywouldprovetobeanobstructionto
scientificprogress. Becausehisauthoritywassogreat, hisflawedtheories
were transmitted as unquestioned dogma through the next 2,000 years.
This hindered the growth of scientific knowledge, or at least the sort of
lrnowledge that we now characterize as scientific.4 In a like manner, the
teachings ofTA's pioneers like Dow, Schabacker, Elliott, and Gann have
beenpassedalongunquestionedanduntested.Justasthereisnoroomfor
dogmainscience, neithershouldthere beinTA.
THE BIRTH OF THE SCIENTIFIC REVOLUTION
"Science was the major discovery, or invention, ofthe seventeenth cen
tury. Men ofthat time learned-and it was a very revolutionary discov
ery-how to measure, explain, and manipulate natural phenomena in a
way that we call scientific. Since the seventeenth century, science has
progressed a great deal and has discovered many truths, and it has con
ferred many benefits that the seventeenth century did not know, but it
has not found a new way to discover natural truths. Forthis reason, the
seventeenth century is possibly the most important century in human
history."5
The revolution began in Western Europe around 1500 in an atmos
phere ofintellectualstagnation. Atthattime, all knowledge was based on
authoritarian pronouncements rather than observed facts. The doctrines
ofthe Roman Church and the dogmaofGreek science were taken as lit
eral truths. On Earth, objects were assumed to be governed by Aris
totelian physics. In the heavens, the laws invented by the Greek
astronomer Ptolemy and later endorsed by the Church were thought to
rule. The Ptolemaic system held that the earth was the center ofthe uni
verse and that the sun, stars, and planets orbited about it. To the casual
observer, thefacts seemedto agreewiththe Church'sorthodoxtheory.
Everyone was happy until people began to notice facts that con
flicted with these accepted truths. Artillerymen observedthatprojectiles
hurled bycatapults andshotfrom cannons did notfly in conformitywith



==================================================
                     PAGE 122                     
==================================================

.06 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Aristotle'stheoryofmotion (Figure 3.1). The objectswere repeatedlyob
served to follow arced paths-a distinct departure from the trajectory
predicted by Greek theory, which would have had them fall directly to
earth as soon as they leftthe apparatus. Either the theory was wrong or
thesoldersweredeceived bytheirsenses. The notionoftestingthevalid
ityofatheorybycomparingitspredictionswithsubsequentobservations
isfundamental tothe modemscientificmethod. Thiswasanearlystepin
thatdirection.
Also, at that time, observations ofthe heavens began to collide with
the Church's theory ofan Earth-centered universe. Increasingly accurate
astronomicalmeasurementsrevealedthatplanetsdidnotmoveasthethe
ory said they should. For a time, bewildered astronomers were able to
patch up the Church's theory to make it agree with observed planetary
paths. Asdiscordantobservationsoccurred, newassumptionswereadded
to the theory, in the form ofsmallerorbitsrevolvingabouta planet'sprin
cipal orbit. These ad hoc fixes, called epicycles, permitted the theory to
account for the troublesome observations. For example, epicycles were
able to explain why planets were sometimes seen to move backwards
(retrograde motion) rather than follow a continuous path across the sky.
Over time, a succession of epicycle fixes transformed the fundamentally
incorrect theory of an Earth-centered universe into an unwieldy mon
strosity of complexity with epicycles upon smaller epicycles upon even
smallerepicycles.
A landmark in scientific history took place when the telescope of
Galileo Galilei (1564-1642) showed that four moons circled the planet
Jupiter. This observation contradicted Church orthodoxythatall celestial
objects must revolve around the Earth. Galileo's findings would have
called for the rejection of deeply held beliefs and the destruction of the
Church'sviewofreality. Religiousauthoritieswerenotabouttoface these
facts. Though Galileo had originallybeengivenpermissionbythe Church
to publish his findings, it subsequently withdrew the permission and
Path Observed
~-
Path Predicted
by
Aristotelian Physics
FIG HE 3.. Prediction versus observation.



==================================================
                     PAGE 123                     
==================================================

The Scientific Method and Technical Analysis 107
found him guilty ex post facto. As punislunent, he was forced to give up
astronomyand live outhisdaysunderhouse arrest.6
Eventually,theChurch'stheoryoftheheavenswasreplacedbythefar
simplerand more correctCopernicanmodel. Itplaced the sun atthe cen
ter of the universe and managed to explain astronomical observations
withfarfewer assumptions. Todaywe knowthatthe sunis notthe center
ofthe universe, but at the time the Copernican model represented a true
increaseinman's knowledge oftheheavens.
FAITH IN OBJECTIVE REALllYAND
OBJECTIVE OBSERVATIO S
Science makes a fundamental assumption about the nature of reality;
there is an objective reality that exists outside and independent ofthose
who observe it, and this realityisthesource ofthe observer'ssensoryim
pressions. Thisfaith isthebasisofscience'semphasisonobservation.Itis
properly characterized as a faith because there is no experiment or logic
thatcanprovethe existence ofan objectivereality. Forexample, Icannot
provethatmyperceptionofa red trafficsignaliscausedbyalightthatex
ists outside ofmyselfrather than bysomething thatarose entirely within
myownmind.
Onemightaskwhyweshouldmakeabigdealaboutafact thatseems
soobviousto commonsense. Everyonecanseethere isaworldoutthere.
However, in the eyes ofscience this issue is a mountain and not a mole
hill, and it points to an important difference between knowing scientifi
cally and knowing intuitively. Science does not accept something as true
simply because it seems to be so. The history of science has shown re
peatedly that the obvious is not necessarily true. Science assumes the
truthofanindependentobjectivereality, notbecauseitisself-evident,but
because itisconsistentwith aprinciple thatshapes muchofthescientific
enterprise-theprincipleofsimplicity.
The principle says simpler is better. Thus, when judging which of
several theories is most likely to be true, when all theories are equally
good atfitting a setofobservedfacts, theprinciple ofsimplicityurges us
to accept the simplesttheory. This is the theory that invokes the fewest
and least complicated assumptions. The principle of simplicity, also
known as Okaum's Razor, tells us to cut away a theory's excess com
plexity. The Copernican model of the universe was preferable to the
Church'smodel because itreplaced a complex arrangementofepicycles
with one orbit for each planet. In a like manner, ifa segment ofmarket
history can be explained either as a random walk, which assumes very



==================================================
                     PAGE 124                     
==================================================

108 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
little, orasan Elliottwavepatternthatassumes waves embeddedwithin
waves, golden ratios, Fibonaccisequences, and a hostofothercomplex
ities, the random walk is the preferred explanation. That is, unless the
Elliott Wave Principle is able to make more accurate predictions than
the random-walk theory.
Assuming the existence of an external reality simplifies matters
greatly. To do otherwisewould require afar more complexsetofassump
tions to explain why two people, observing the same phenomenon, will
come away with generally the same impression. Explaining why I see
other cars stopping when I see the traffic signal turning red is simple ifI
assume that there really is a traffic light outthere. However, without that
assumption, many more complicated assumptions would be required to
explainwhyothermotoristsstopwhenIdo.
As a consequence of its faith in an objective reality, science holds
that objective and subjective perceptions are essentially different. My
objective observation of a red traffic signal and my subjective experi
ence ofbeingannoyed because it will make me late for work are not the
same. Objectiveobservations canbesharedwithand confirmedbyother
observers. For this reason, objective observations lend themselves to
the establishment ofknowledge that can be shared with and confirmed
by others. Subjective thoughts, interpretations, and feelings cannot, and
this flaw alone is sufficient to disqualify subjective TA as legitimate
knowledge.
THE NATURE OF SCIENTIFIC KNOWLEDGE
Albert Einstein once said; "One thing Ihave learned in a long life: that all
ourscience,measuredagainstrealityisprimitiveand childlike-andyetit
is the most precious thing we have."? Scientific knowledge is different
than wisdom gained by other modes of inquiry such as common sense,
faith, authority, and intuition. These differences, which account for sci
ence'shigherreliability, are considered in thissection.
Scientific Knowledge Is Objective
Science strives for maximum objectivity by confiningitselfexclusively to
facts about the world out there, although it is understood that perfectly
objective knowledge is never attainable. This eliminates from considera
tion subjective assessments that are inherently private and accessible by
only one person. Inner thoughts and emotive states cannotbe shared ac-



==================================================
                     PAGE 125                     
==================================================

The Scientific Method and Technical Analysis 109
curatelywithothers, evenwhen thatis the intentofanartist, poet, writer,
orcomposer. InNaked Lunch, William Burroughs attempts to conveyhis
personal experience ofheroin. However, Iwill neverknow hisexperience
and may take away from those passages something quite different than
Burroughs's experience or that of someone else reading it. Scientific
knowledge is, therefore, public inthe sensethat itcanbesharedwithand
verified byas manypeople aspossible. This promotes maximum possible
agreementanlong independentobservers.
Scientific knowledge is empirical or observation based. In this way
it differs from mathematical and logical propositions that are derived
from and consistent with a set of axioms but need not refer to the
external world or be confirmed by observation. For example, the
PythagoreanTheoremtells us thatthesquared length ofthe hypotenuse
ofa right triangle, c, is equal to the sum ofthe squares ofthe other two
sides a and bor
However, this truth was not derived by studying thousands ofexam
ples of right triangles and generalizing from those many observations.
Rather, itisderived from asetofaccepted mathematicalpostulates.
Scientific Knowledge Is Quantitative
The notion that the world is best understood when described in quanti
tative terms originated with the Greek mathematician Pythagoras [569
B.c.-approx. 475 B.C.]. It was his contention that both the world and
the mind are essentially mathematical. The importance science places
on quantification cannot be overemphasized. "Wherever mankind has
been able to measure things, which means transform orreduce them to
numbers, it has indeed made great progress in understanding them and
controllingthem. Where human beingshave failed to find a way to mea
sure, they have been much less successful, which partly explains the
failure of psychology, economics, and literary criticism to acquire the
status ofscience."8
Observations must be reduced to numbers ifthey are to be analyzed
in a rational and rigorous fashion. Quantification makes itpossible to ap
ply the powerful tool of statistical analysis. "Most scientists would say
thatifyoucannotdescribewhatyouaredoinginmathematicalterms, you
arenotdoingscience."9Quantificationisthebestwayto ensurethe objec
tivity of knowledge and to maximize its ability to be shared with and
tested byallqualifiedpractitioners.



==================================================
                     PAGE 126                     
==================================================

aa
0 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The Purpose ofScience: Explanation
and Prediction
Thegoalofscienceis the discoveryofrulesthatpredictnewobservations
andtheoriesthatexplainpreviousobservations. Predictiverules, oftenre
ferred to as scientific laws, are statements about recurring process, such
as'eventA tendstopredicteventB,' butlawsdon'ttrytoexplainwhythis
happens.
Explanatory theories go further than predictive rules by telling us
whyitisthatB tendstofollowA, ratherthansimplytellingusthatitdoes.
Chapter 7willdescribe sometheories advanced bythefield ofbehavioral
finance thatattemptto explainwhythe behavioroffinancial marketsis, at
times, nonrandom. These theories give hope to TA and may explain why
certainmethodsofTAcanwork.
Scientificlawsandtheories differwithrespect to theirgenerality. The
mostprized are the mostgeneral-thatis, theypredictand/orexplainthe
widest range ofphenomena. ATArule thatis effective onall marketsand
all time scales would have higher scientific stature than one that works
onlyoncopperfutures onhourlydata.
Lawsalso differwithrespecttotheirpredictivepower. Those thatde
pict the most consistent relationships are the most valuable. All other
things being equal, a TA rule that is successful 52 percent ofthe time is
lessvaluablethan onethatworks 70percentofthetime.
The most important type of scientific law is the functional relation
ship. Itsummarizes a set ofobservations in the form ofan equation. The
equation describes how a variable that we wish to predict (dependent
variable) typicallydenoted bythe letterY, isafunction of(Le., dependent
on) one or more other variables called predictors, usually designated by
theletterX. Thisisillustratedas
y= f(X)
Typicallythe values ofthe predictors are known, but the value ofthe
dependentvariableisnot. Inmanyapplications, thisis becausethe depen
dent variable Yrefers to a future outcome, whereas theX variables refer
to values that are currently known. Once a functional relationship has
beenderived, itispossibletopredictvaluesforthe dependentvariable by
pluggingin knownvaluesofthepredictorvariables.
Functionalrelationships can be derived in two ways. Theycan be de
ducedfrom explanatorytheoriesortheycanbeestimated (induced)from
historical databyfunction fitting (e.g., regressionanalysis). Currently, TA
is primarily constrained to the latter because theories ofTAarejustnow
beingformulated (see Chapter7).



==================================================
                     PAGE 127                     
==================================================

The Scientific Method and TechnicalAnalysis 111
THE ROLE OF LOGIC I SCIENCE
Scientific knowledge commands respect, in part because its conclusions
arebasedonlogic. Byrelying onlogicandempiricalevidencetojustifyits
conclusions,scienceavoidstwoofthe commonfallaciesthatcontaminate
informalreasoning: appeals to authorityand appeals to tradition. An ap
peal to authority offers as proof the statement of a purportedly knowl
edgeable person. An appeal to tradition offers a long-standing way of
doingthingsasproof.
To its detriment, much ofpopularTAisjustified on grounds oftradi
tion orauthorityratherthanfornlallogic and objective evidence. In many
instances, currentauthorities merelyquote priorauthorities who, in turn,
quote yet earlier experts and so on back to an original source whose
knowledge wasprimarilyintuitive. Thus thefallacies ofauthority and tra
ditionare mutuallyreinforcing.
The First Rule of'Logic: Consistency
Aristotle (384-322 B.C.) is credited with the invention of fornlal logic,
which evolved from geometry. The Egyptians had been making accurate
measurementsoflinesandanglesandcalculatingareasforovertwothou
sand years, butit was the "Greeks who extended these basic notions and
transformed them into a compellingsystem ofirrefutable conclusions de
rivedfrom mathematicaldefinitions (axioms)."10
Formallogicisthe branchofmathematicsconcernedwiththe lawsof
correct reasoning that are used to formulate and evaluate the validity of
arguments. In contrast to informal inference, if the rules offormal logic
arefollowed, a true conclusionisguaranteed.
The most fundamental principle offormal logic is the rule ofconsis
tency. Itisexpressedintwolaws: theLawoftheExcludedMiddle and the
LawofNoncontradiction. "Thelawoftheexcludedmiddle requiresthata
thing musteither possess orlack a given attribute. There is no middle al
ternative. Or said differently, the middle ground is excluded."ll A state
ment is either true or false. It cannot be both. However, the law is only
properly applied to situations that are binary and is easily misapplied to
situationsthatarenottrulytwo-state.
"Closely related to the law ofthe excluded middle is the law ofnon
contradiction.Ittellsusthata thingcannotbothbeand notbeatthesame
time."12 Astatement cannot be true and not true at the same time. An ar
gument that allows for its conclusion to be true and not true at the same
tinleissaidto beaself-contradictory.
As will be shown in subsequent sections, these laws of logic are
used to greateffectinscience. Although observed evidence, such as the



==================================================
                     PAGE 128                     
==================================================

112 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
profitable back test ofa TA rule, cannot logically prove that the rule has
predictive power, that same evidence can be used to logically disprove
(contradict) the assertion that the rule is devoid ofpredictive power. By
the LawofNoncontradiction, thisindirectlyprovesthatthe rule doespos
sess predictive power. This method ofproving empirical laws, called the
method ofindirectprooforproofby contradiction, is the logical basis of
thescientificmethod.
Propositions and Arguments
Logical inference takes two distinct forms: deduction and induction. We
will consider each separately and then see how they are used togetherin
the logical framework of modern science, the hypothetico-deductive
method. However, before consideringtheseforms ofinferencesome defi
nitionsare inorder.
• Proposition: a declarative statementthatis eithertrue orfalse some
times referred to as a claim. For example, the statement The head
and-shoulder pattern has more predictive power than a random
signal is a proposition. A proposition differs from other kinds of
statementsthatdo notpossessthe attribute oftruth offalsity suchas
exclamations, commands, and questions. Therefore, only proposi
tionsmaybeaffirmed ordenied.
• Argument: a group ofpropositions, one ofwhich is referred to as the
conclusion, whichisclaimedtofollow logicallyfrom the otherpropo
sitions, calledpremises. Thus, an argument asserts that its premises
providetheevidencetoestablishthe truthofitsconclusion.
Deductive Logic
As mentioned earlier, the there are two forms of logic, deductive and
inductive. This section considers deductive logic; the next considers
inductive.
Categorical Syllogisms. Adeductiveargumentisonewhosepremises
are claimedto provide conclusive, irrefutable evidence forthe truth ofits
conclusion. Acommonform ofdeductive argumentis the categorical syl
logism. Itis comprised oftwo premises and a conclusion. Itis so named
becauseitdealswithlogicalrelationsbetweencategories. Itbeginswitha
premise that states a general truth about a category, for exampleAll hu
mans are mortal, and ends with a conclusion that states a truth about a
specificinstance, for exampleSocrates ismortal.



==================================================
                     PAGE 129                     
==================================================

The Scientific Method and TechnicalAnalysis 113
Premise 1:Allhumans aremortal.
Premise2:Socrates is ahuman.
Therefore:Socrates is mortal.
Notice thatthe argument's first premise establishes a relationship be
tween two categories, humans and mortals: All humans are mortal. The
second premise makes a statement about a particular individual member
of the first category: Socrates is a human. This forces the conclusions
that this individual must also be a member of the second category:
Socrates is mortal.
Thegeneralform ofa categoricalsyllogism isasfollows:
Premise 1:Allmembers ofcategoryA aremembers ofcategoryB.
Premise2:X is amemberofcategoryA.
Therefore:X is amember'ofcategoryB.
Deductive logic has one especially attractive attribute-certainty. A
conclusion arrived at by deduction is true with complete certainty, but
this is so ifand only iftwo conditions are met: the premises ofthe argu
mentare trueandtheargumenthasvalidform. Ifeitherconditionislack
ing, the conclusion is false with complete certainty. Therefore, a valid
argumentis defined as one whose conclusionmustbe true ifitspremises
are true. Or, said differently, itis impossible for a valid argument to have
truepremisesandafalse conclusion.
Insummary, conclusionsarrivedatbydeductionareeithertrueorfalse.
Ifeithervalidform ortruepremisesarelacking,thenthe conclusionisfalse.
Ifbotharepresent,theconclusionistrue. Thereisnomiddleground.
Itisimportantto note thattruth and validityare two distinctproper
ties. Truth, and its opposite, falsity, are properties thatpertain to an indi
vidual proposition. Aproposition is true if it conforms to fact. Because
premisesandconclusionsare both propositions,theyareaptlycharacter
ized as either true or false. The premise All pigs can fly is false. The
premiseSocrates is amanis trueas is the conclusionSocrates is mortal.
Validity is a property that pertains to the form of an argument. In
otherwordsvalidityrefers to the logicalrelationships betweenthepropo
sitions comprising the argument. Validity or lack thereof describes the
correctness of the logical inference linking the premises to the conclu
sion, butvaliditymakesno referencetothefactual truthoftheargunlent's
premises or conclusion. An argument is said to be valid if, when its
premisesare true, itsconclusionmustalso betrue.
However, an argument can be valid even if is composed of false
propositions so long as the logical connections between the propositions



==================================================
                     PAGE 130                     
==================================================

114 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
CARS
Sports
Cars
FIGURE 3.2 Eulercircles.
are sound. The categorical syllogism thatfollows has valid form because
its conclusion is logically compelled by its premises, yetits premises and
conclusionareclearlyuntrue.
AUhumansareimmortal.
Socrates is a human.
Therefore, Socrates is immortal.
Because validity has nothing to do with matters offact, validity can
best be demonstrated with arguments or diagrams of arguments, called
Eulercircles, whichmake nofactual referenceswhatsoever. Ina Eulerdi
agram, the set ofelements comprising a category is represented bya cir
cle. Toshowthatonecategory, suchassportscars, isasubsetofthe more
general category-cars in general-the circle representingsports cars re
sideswithinthe largercirclerepresentingcars. SeeFigure3.2.
Figure 3.3 makes clearwhyargument 1is valid butargument2is not.
Argument 1is valid because its conclusion, X is a B, follows necessarily
from (i.e., iscompelledby) itspremises. However, Argwnent2isnotvalid
because it conclusion, X is an A, is not logically compelled by its
premises. Xmay belong to category Abut not necessarily. The Euler dia
grams portray validity more forcefully than the argument itself because
the argumentalone requiressomethinking.
Argument 1
AllAs areB's.
Xis anA.
Therefore, X is aB.



==================================================
                     PAGE 131                     
==================================================

The Scientific Method and TechnicalAnalysis 115
Argument 1 Argument2
Valid Invalid
All A's are8's. All A's areB's.
Xisan A. Xisa8.
ThereforeXisaB. ThereforeXisan A.
CategoryB Category8
Category A Category A
x
FIGURE 3.3 Valid and invalid categorical syllogisms.
Argument 2
AllA'sareB's.
Xis aBo
Therefore, X is anA.
Conditional Syllogisms: l'he Logic of Scientific Arguments.
Anotherform ofdeductive argument, and the one that is centralto scien
ti:fic reasoning, is the conditional syllogism. It is the logical basis for es
tablishing the discoveryofnew knowledge.
Like the categorical syllogism, the conditional syllogism is also com
posed of three propositions: two premises and a conclusion. It is so
named becauseitsfirstpremiseisa conditionalproposition.
A conditional proposition is a compound statement that combines
two simplepropositions using the words If-then. The propositionfollow
ingifisreferredto astheantecedentclauseandthepropositionfollowing
then is referred to as the consequent clause. The generalform ofa condi
tionalpropositionis
If(antecedent clause), then(consequent clause)



==================================================
                     PAGE 132                     
==================================================

116 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Forexample:
Ifitis a dog, then ithasfour legs.
In the example, it is a dog is the antecedent clause and it has four
legs is the consequentclause. Asecond example thatis closerto the pur
poseathandwouldbe
If the TA rule has predictive power, then its back tested rate ofre
turnwiUexceedzero.
Our ultimate goal will be to establish the truth of the antecedent
clause of these conditional propositions like the preceding example. As
will becomeclear, the route toestablishingitstruthisan indirectone!
Thesecondpremiseofaconditionalsyllogismisapropositionthatei
theraffirms ordenies the truth ofeithertheantecedentclause orthecon
sequent clause of the first premise. Referring to the prior example, the
secondpremisecouldbe oneofthefollowing fourstatements:
It is a dog: Affirms the truthofthe antecedent.
It is nota dog: Deniesthe truthoftheantecedent.
It hasfour legs: Affirmsthe truthofthe consequent.
It does nothavefour legs: Deniesthe truth ofthe consequent.
The conclusion of the conditional syllogism affirms or denies the
truthofthe remainingclause ofthefirstpremise. Inotherwords, the con
clusionreferencestheclausethatisnotmentionedin thesecondpremise.
Forexample, ifthe second premise refers to the antecedentclause ofthe
firstpremise,It's adog, thenthe conclusionwouldreferto theconsequent
clause, It hasfour legs.
An exampleofa conditionalsyllogismisasfollows:
Ifitisa dog, then ithasfourlegs.
It is a dog (affirmsthetruthoftheantecedent).
Therefore, ithasfour legs (affirms the truth ofthe consequent).
When a conditional syllogism possesses valid form, its conclusion is
logicallycompelledbythetwopremises.Moreover, ifithasvalidform and
its premises are factually true, then the conclusion ofthe conditional syl
logismsmustalsobetrue.



==================================================
                     PAGE 133                     
==================================================

The Scientific Method and TechnicalAnalysis 117
Valid FOl'lns 01' the Conditional Syllogism. There are two valid
forms ofthe conditional syllogism; affirming the antecedent and deny
ing the consequent. In an argumentthataffirms the antecedent, the sec
ond premise affirms the truth of the antecedent clause of the first
premise. In an argument thatdenies the consequent, thesecondpremise
asserts that the consequent clause ofthefirst premise is not true. These
two valid forms ofargumentare shown here. They assume all dogs pos
sess four legs.
Affirmingtheantecedent:
Premise 1: Ifitis a dog, then ithasfour legs.
Premise2: It is a dog.
Valid Conclusion: Therefore, ithasfour legs.
In this valid form, the second premise affirms the antecedent clause
bystatingIt is a dog. These twopremises deductivelycompel the conclu
sionIt hasfour legs. Thegeneralform ofa conditionalsyllogisminwhich
theantecedentisaffirmedis
Premise 1: IfA is true, thenB is true.
Premise 2:A is true.
Valid Conclusion: Therefore, B is true.
The othervalidform ofthe conditionalsyllogism isdenial ofthe con
sequent. In this form, the second premise asserts that the consequent
clause ofpremise 1is a falsehood. From this, one may conclude that the
antecedent clause ofpremise 1is also false. This is illustrated by the fol
lowingexample:
Premise 1: Ifitis adog, then ithasfour legs.
Premise 2: It doesNOThavefour legs.
Valid Conclusion: Therefore, itisnota dog.
Denialofthe consequentfollows the generalform:
Premise 1: IfA is true, thenB is true.
Premise2: B isnot true.
Valid Conclusion: Therefore, A is not true.



==================================================
                     PAGE 134                     
==================================================

118 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Thisissometimesshortenedto
IfA, thenE.
NotE.
Therefore, notA.
Notice that this form of argument uses evidence (creature does not
havefourlegs) toprovethatantecedent(It isadog) isfalse. Itisthisform
ofreasoningthatis usedinsciencetoprovethatahypothesisisfalse. The
hypothesis plays the role ofthe antecedent. We hypothesize thatthe crea
ture is a dog. The consequentclausepredictswhatwill be observedifthe
hypothesisistrue. Thatis, ifthecreatureistrulyadog, then, whenitisob
served, itwillbeseentopossessfourlegs. Inotherwords, the conditional
proposition, IfA, then B, predictsthat Bwould be observed inanexperi
ment if the hypothesis (A) were in fact true. The second premise states
that when the observation was made, the observation contradicted the
prediction, that is, Bwas notobserved. Given this, we canvalidlydeduce
thatthe hypothesisAisfalse.
As will be shown in due course, ifwe can prove that hypothesis Ais
false, we can indirectly prove that some other hypothesis is true. That
other hypothesis is the new knowledge that we wish to establish as true,
for exanlple that a new vaccine is more effective than a placebo or that
some TA rule predicts more effectivelythan a randomly generated signal.
Invalid Form of the Conditional Syllogism. An importantreason
for using formal logic is the difficulty people often have when reasoning
informallyaboutconditionalsyllogisms. Consistentwiththe numerousbi
ases and illusions discussed in Chapter 2, psychological studiesl3 have
shown that people tend to commit two fallacies when reasoning about
conditionalpropositions: the fallacy ofaffirming the consequent, and the
fallacy ofdenying the antecedent. An example ofthe fallacy ofaffirming
theconsequentis
Premise 1: Ifitis a dog, then ithasfour legs.
Premise2: It hasfour legs.
Invalid Conclusion: Therefore, it is a dog.
The fact that a creature has four legs certainly does not compel the
conclusionthatthecreatureisadog.Itmaybeadog. Theevidenceiscon
sistentwith the fact thatitis a dog. However, the evidence is also consis
tent with other conclusions. It could just as easily be a cow, a horse, or



==================================================
                     PAGE 135                     
==================================================

The Scientific Methodand TechnicalAnalysis 119
any other four-legged creature. The fallacy of affirming the consequent
has thegeneralform:
Premise 1:IfA is true, thenB is true.
Premise2:B is true.
Invalid Conclusion: Therefore, A is true.
This fallacy is a very common error in poor scientific reasoning and
one committed in many articles on TA. Consider the following syllogism,
whichcommitsthefallacy ofaffirmingtheconsequent.
Premise 1: IfTA ruleX has predictivepower, then it shouldproduce
profits ina back test.
Premise2: The back test wasprofitable.
Invalid Conclusions: Therefore, the TA rule haspredictivepower.
Bothofthesepremisesmaybetrue, butthe conclusionisnotnecessar
ilytrue. With respecttothefirstpremise, itistruethatiftheTAruleXdoes
possess predictive power it should back test profitably. In other words, a
profitable back test would be consistentwith the supposition that the rule
has predictivepower. However, a profitable backtestcould also be consis
tent with the supposition that the rule was merely lucky. Similarly, justas
evidence offour legs is consistentwith a dog, it is also consistentwith all
otherfour-legged creatures. The fallacy ofaffirming the consequentis the
14
reasonthatempiricalevidencecannotbe usedtoprovethatahypothesisis
true. As we shall see, one philosopher ofscience, Karl Popper, contended
that the scientific method must, therefore, rest on denial (falsification) of
theconsequent,which, aspointedoutearlier,isavalidform ofinference.
The otherfallacy associated with conditionalsyllogisms is thefallacy
ofdenyingtheantecedent. Itisillustratedhere:
Premise 1:Ifitisa dog, thenithasfour legs.
Premise2: It is notadog.
InvalidConclusion: Therefore, itdoes nothavefour legs.
The fact thata creature is nota dog does notpreclude its havingfour
legs. Thegeneralform ofthisfallacy is
Premise 1: IfA is true, thenB is true.
Premise2:A isnot true.
Invalid Conclusion: Therefore, B isnot true.



==================================================
                     PAGE 136                     
==================================================

120 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Inlogicalshorthand
IfA, thenB.
NotA.
Therefore, notB.
Invalid arguments can be hard to notice when they concern complex
subjects. Thebestwaytoreveal them istoplugincommonplaceitemsfor
AandBandseeifthe conclusionfollows from thepremises.
Figures 3.4 and3.5summarize the preceding discussion ofthe condi
tionalsyllogism.
As mentioned, the great strength ofdeductive reasoning is its ability
to deliver conclusions that are true with certainty. However, deductive
logichasagreatwealrness; itisunabletorevealnewlrnowledgeaboutthe
world. All that a deductive argument can do is to reveal truths that were
already implicit in its premises. In other words, deductive reasoning can
only tease out truths that were already present in the premises though
theymaynothavebeenevident.
This is not meant to minimize ortrivialize deduction butto clarifyits
role. Itmay well be that a conclusion implied by the premises is far from
obvious. Great mathematical discoveries are exactly that-the revealing
oftruths that had been implicit in the axioms ofa mathematical system
but that had not been understood prior to being proven. Such was the
case with the much-publicizedproofofFermat's lasttheorem. Itwasfirst
Valid Invalid
Affirming the Antecedent
IfA, then B.
A.
Therefore, B.
Denying the Consequent
IfA, then B.
Not B.
Therefore, Not A.
FIGURE 3.4 Conditional syllogisms: general form.



==================================================
                     PAGE 137                     
==================================================

The Scientific Method and TechnicalAnalysis 121
Valid Invalid
Affirming The Antecedent
Ifa dog, then has 4 legs.
It is a dog.
Therefore, has 4 legs.
Denying The Consequent
Ifa dog, then has 4 legs.
Legs not equal to 4.
Therefore not a dog.
FIGURE 3.5 Conditional syllogisms: example.
hintedatin the margin ofa bookin 1665butnotproven until 1994, byAn
drew WilesandRichardTaylor.
Inductive Logic
Inductionis the logic ofdiscovery. Itaims to reveal new knowledgeabout
theworldbyreachingbeyondthe knowledge containedin thepremisesof
an inductive argument. However, this new knowledge comes with a
price-uncertainty. Conclusions reached by induction are inherently un
certain. That is, they can only be true with some degree of probability.
Thus, thenotionofprobabilityisintimatelyconnectedwithinduction.
Inductionproceedsinamanneroppositeto deduction. Wesawthatde
ductionprogressesfrom apremiseexpressingageneraltruththatisheldto
apply to an unlimited numberofinstances,AUmenaremortalto a conclu
sionaboutaspecificinstanceSocrates ismortal.Incontrast, inductiverea
soning moves or, better said, leaps from a premise based on a limited
number ofobserved instances to a general conclusion about an unlimited
numberofsimilar butnot-yet-observed instances. Thus, this form ofinfer
enceisoftenreferredto as inductive generalization. Itis in leaping beyond
what has been directly experienced by observation that induction incurs
uncertainty-thepossibilitythatitsconclusionmaybewrong.
Inductivegeneralizationisillustratedbythefollowing:
Premise:Each ofone thousand healthydogs hadfourlegs.
Generalconclusions:Allhealthy dogs will havefour legs.



==================================================
                     PAGE 138                     
==================================================

122 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The conclusion, in this example, is called a universal generalization,
becauseitassertsthatallmembersofthe classdoghave theattributefour
legs. Universalgeneralizationshavetheform
AllX's are Y's.
or
100percent ofX's have the attribute Y.
Generalizations need not be universal. Nonuniversal generalizations
have theform
PpercentofX's are Y's.
or
PpercentofX's haveattribute Y.
or
X's have attribute YwithprobabilityP.
Again, we see that probability is inextricably tied to the concept of
generalization. In fact, nonuniversal generalizations are also known as
probabilistic generalizations. An example would be: A higherpercentage
of bulldogs are prone to violence than poodles. This statistically sound
nonuniversalgeneralization15does notclaim all bulldogs are prone tovio
lent behavior, nordoes it claimthateven a majorityareso inclined. How
ever, it does say that, as a group, statistics show that bulldogs have a
higherprobabilityofbeingdangerousthanpoodles.
Aristotle's defeat as scientistwas in part attributable to his failure to
appreciate that nonuniversal generalizations convey useful knowledge.
His fascination with the certainty ofdeductive proofs led him to restrict
his search to universal generalizations. He was unable to see that many
importantregularities ofthe naturalworldare inherentlyprobabilistic.
Induction by Enumeration
The most common form ofinductive argument is based on enumeration.
It proceeds from a premise that enumerates the evidence contained in a
set ofobservations, and then draws a general conclusion that pertains to
allsimilarobservationsoutsidethe enumeratedset.



==================================================
                     PAGE 139                     
==================================================

The Scientific Method and TechnicalAnalysis 123
Premise: Over the past 20 years there have been 1,000 instances in
which TA ruleX gave a buy signalandin 700 ojthose instances the
marketmovedhigheroverthenext 10days.
Conclusion: Inthefuture, whenruleXgivesa buysignal, thereisa0.7
probabilitythatthemarketwill behigherattheendof10days.
This conclusion or any conclusion reached by induction is inher
ently uncertain because it extends to observations that have not yet
been made. However, some inductive arguments are stronger than oth
ers, and hence, arriveatconclusionsthatare more certain. Thestrength
of an inductive argument and the certainty of its conclusion depend
upon the quantity and quality of the evidence enumerated in its
premise. The more numerous the instances cited and the higher their
quality, the more likely the conclusion will generalize accurately to fu
ture observations.
Suppose thatthe conclusion aboutTA rule Xhad been based on only
10 instances with a 70 percent success rate instead of 1,000. In such a
case, the conclusion would have been at least 10 times more uncertain.
16
This means we should be lesssurprised ifthesignal'sfuture accuracy dif
fers greatlyfrom itshistoricsuccessrate. Inscience, theevidence offered
in supportofa conclusion is typically evaluated with statistical methods.
This allows one to make quantitative statements about the conclusion's
uncertainty. Thistopic isdiscussed inChapters5and 6.
The strength ofan inductive argument also depends upon the quality
ofthe evidence. Evidence quality is an entire subject unto itself, but suf
fice itto saythatsome observational methods produce higherqualityevi
dence than others. The gold standard in science is the controlled
experiment, where all factors but the one under study are held constant.
TA does not permit controlled experiments, but there are better and
worse ways to make observations. One issue that is relevant to TAis the
matter ofsystematic error or bias. As will be discussed in Chapter 6, ob
jectiveTAresearch isproneto aparticulartype ofsystematicerrorcalled
data mining biasifcarefulstepsare nottaken.
Also relevant to the strength ofan inductive argument is the degree
towhichthe evidence cited is representative ofthe kinds ofobservations
thatare likely to be encountered in the future. An inference about a rule
that generates long/neutral (+1,0) signals that was back-tested only dur
ingrisingmarkets isunlikelyto be accurateaboutthe rule'sperformance
in a declining market environment. Even if the rule had no predictive
power, its restriction to long or neutral positions makes it likely that it
would have generated a profit because it was tested during a rising mar
ketperiod.



==================================================
                     PAGE 140                     
==================================================

124 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The mostcommonfallacy ofinductionis the hastygeneralization
an induction based on too little evidence or evidence of poor quality.
Rule studies that cite a small number of successful signals as a basis
for concluding that the rule has predictive power are likely to be hasty
generalizations.
1'00 .-HILOSOPIWOF SCIENCE
The philosophy of science seeks to understand how and why science
works. It explains such things as: the nature of scientific propositions
and how they differ from nonscientific and pseudoscientific proposi
tions; the way scientific knowledge is produced; how science explains,
predicts, and, through technology, harnesses nature; the means for de
termining the validity ofscientific knowledge; the formulation and use
of the scientific method; and the types of reasoning used to arrive at
conclusions.
17
That the scientific method is one ofman's greatest inventions and is
byfar the mosteffective method for acquiringobjective knowledge about
the natural worldisbeyondquestion. "Thepartofthe world known as the
Industrial Westcould, in its entirety, be seen as a monument to the Scien
tific Revolution.... Humankind's ability to predict and control the nat
"18
ural world has improved more in the last 400 years, since the scientific
revolution began, than it had in the prior 150,000 years that modern hu
mans, homo sapiens, walked theface ofthe Earth.
Strange as itmay seem, the invention ofthe scientific method and its
firstfruits came beforeitwas understood whythemethod worked aswell
asitdid. Thatinsightcamegradually, overthe courseofseveralcenturies,
aspracticingscientistsand theirerstwhile critics, the philosophers ofsci
ence, wrestled with perfecting the method and, in parallel, developed an
understandingofhowand why itworked.
The mere fact that the scientific method did work was not enough. It
was seen as necessary to understand why. What philosophers found so
vexing was the following apparent contradiction. On the one hand were
the greatvictoriesofscience, suchasNewton'slawsofmotionand gravity
andhumanity'sexpandingtechnologicalcontrolovernature. Onthe other
was the fact that scientific knowledge was inherently uncertain, because
conclusions arrived at by logical induction were inherently uncertain.
How could so much useful knowledge result from such a flawed method
ofinference?
This section discusses the keys steps in the method's development
and the milestones in our deepening understanding of how and why it



==================================================
                     PAGE 141                     
==================================================

The Scientific Method and TechnicalAnalvsis 125
works. Thereadermaywishtoskipthis historicaldevelopmentand go di
rectlyto thesummaryofthe keyaspectsofthescientificmethod.
Bacon's Enthusiasm
Withoutinvitation,philosopherspoke theirnoses intoallsortsofmatters.
It'stheirnature. Theytellus how to act(ethics), howgovernmentsshould
rule (political philosophy), what is beautiful (aesthetics), and ofgreatest
concernto us, whatconstitutesvalid knowledge (epistemology) and how
weshouldgoaboutgettingit(philosophyofscience).
Thescientific revolution was, in part, a revoltagainstAristotelian sci
ence. The Greeks regarded the physical world as an unreliable source of
truth. According to Plato, mentor of Aristotle, the world was merely a
flawed copyofthetruthandperfectionthatexistedintheworld ofForms,
a metaphysical nonmaterial realm, where archetypes ofthe perfect dog,
theperfecttree, and everyotherimaginablething could befound.
Whenthe revoltagainstthisviewofrealityfinallyarrived, itwasharsh
and unremitting.19 The new school of thought, empiricism, resoundingly
rejected the Greek paradigm. It contended that, not only was the natural
world worthy of study, but that careful observation could lay bare its
truths. Apioneer of empiricism and perhaps the first philosopher ofsci
ence was Francis Bacon (1561-1626). "Nature, for Bacon was an open
book that could not possibly be misread by an unprejudiced mind."20 In
his famous work theNovem Organum (the new tool) Bacon extolled the
power ofobservation and induction. He held science to be a completely
objective rational practice that could conclusively confirm or falsify
knowledge simplybyobservingwithoutbias orprejudice and then gener
alizingfromthose observations. Inmanysituations, thisapproachseemed
to work.
However, empiricismwas notthe perfecttool Bacon and his disciples
claimed, and philosophers made it their business to say why. First, they
pointed outthat empiricism, an observation-based enterprise, rested on a
crucialassumptionthatcouldnotbeconfirmedbyobservation,theassump
tionthatnaturewasuniformoverall timeandspace. Thatassumptionwas
criticalinjustifyingthe empiricists'position thatifascientific law was ob
served to hold true here and now it would also hold true everywhere and
forever. Because the uniformity of nature assumption could not be con
firmed by observation, it had to be taken on faith. Second, science often
deals with phenomena and concepts that defy direct observation: atomic
structure, the force ofgravity, and electric fields. Though their effects are
observable, these constructs could not be arrived atexclusivelyby obser
vationandinduction.Theyarebetterunderstoodashumaninventionsthat
explainandpredictratherthanobservablephysicalrealities.



==================================================
                     PAGE 142                     
==================================================

.26 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Nevertheless, Bacon's contributions to the development ofthe scien
tific method were important. He promoted the idea of experiment and
made room for doubt by taking special note of discordant observations.
BothideaseludedtheGreeks.
Descartes's Doubt
Ifphilosophers are good for anything it's raising doubt, and no one was
better than Rene Descartes (1596-1650). Regarded as the father ofmod
em philosophy and a key figure in the birth ofscience, Descartes shared
Bacon'sskepticismfortheauthoritarian knowledge ofthe Greeksand the
dogmaofthe Roman Church. However, Descarteswasjustasskepticalof
theclaimsmade bytheempiricistsaboutthepowerofobservationand in
ductive generalization. His famous expression, "I think, therefore I am,"
expressed the positionthatscience muststartbydoubting everythingex
ceptthe existence ofthe personwho experiences doubt. Fromthatpoint
ofsolidity, knowledge must be built purely by deductive reasoning with
outbeingcontaminatedbyerror-proneobservationsmadewiththe imper
fectfive senses.
As a consequence of Descartes' anti-empirical stance and penchant
for theorizing in a factual vacuum, his scientific findings were almost
meaningless.21 However, his contributions to science were lasting. Skepti
cism is central to the scientific attitude. In addition, Descartes' invention
ofanalyticgeometrypavedthe wayfor Newton'sinventionofthe calculus
which, intum, allowedhimtospecifyhisfamous equationsofmotionand
gravity.
"ume's Critique ofInduction
Another dose of doubt was administered by Scottish empiricist and
philosopher David Hume (1711-1776). His seminal work, the Treatise on
Human Nature, published in 1739, grappled with a central problem of
epistemology: how to distinguish knowledge from lesser forms of know
ing, such as opinions thathappen to be true. Priorto Hume's publication,
philosophersgenerallyagreed thatthe distinction was related to the qual
ity of the method used to acquire the knowledge. What justified calling
one bit of wisdom knowledge but not another was the pedigree of the
method ofinquiry.
22
Philosophers couldnot, however, agree onthe bestmethod ofknowl
edge acquisition. Empiricists argued that objective observation followed
by inductive generalization was the route to wisdom. Rationalists, on the
other hand, contended that pure deductive reasoning from self-evident
truthswasthe correctmethod.



==================================================
                     PAGE 143                     
==================================================

The Scientific Method and TechnicalAnalysis 127
Hume took issue with both schools of thought and ultimately even
with himself. As an empiricist, Hume disparaged the rationalists' purely
deductive approach because it was disconnected from observed facts.
Also, in the spiritofempiricism, Hume said that it was wise to adjust the
strength of one's beliefs in proportion to the evidence and that theories
should be evaluated by the degree to which they matched observation.
Butthen Hume wenton to contradicthimselfby attacking the logical ba
sis of empiricism. He denied the validity of inductive generalization and
disparaged the ability ofscience to establish causal laws. His searing cri
tique ofinductionhascometo be knownas Hume'sproblem.
Hume's attack on induction was on both psychological and logical
grounds. First, he said the beliefthat induction could establish correla
tive or causal connections between events was a nothing more than a
by-product of human psychology. Hume asserted that the perception
of cause and effect was merely an artifact ofthe mind. The beliefthat
A causes B or is even correlated to B, simply because A has always
been followed by B was nothing more than a habit ofmind, and a bad
habitat that.
From a logical perspective, Hume claimed that induction was flawed
because no amountofobserved evidence, no matter how objectivelycol
lected, can compelaconclusionwiththeforce ofavaliddeduction. More
over, he said that there was no rule of induction that tells us when we
have evidence ofsufficientquantityorqualitytojustifythe leap from afi
nitesetofobservationstoaconclusionaboutaninfinitenumberofsimilar
butnotyetobserved instances. Arule ofinduction would, itself, have had
tobetheresultofapriorvalidinductionmadeonthebasisofanevenear
lierrule ofinduction, andsoforth and so on, ad infinitum. In otherwords,
an attempt to justify induction inevitably rests on an infinite regress-a
logicallyimpossible absurdity.
In light ofHume's attack, supporters ofinduction retreated to a nar
rower claim saying inductive generalizations were merely correct in a
probabilistic sense. So as evidence accumulates in favor ofa relationship
betweenAand B, the probabilitythatthe relationship isauthenticalso in
creases. However, philosopherswere quickto pointoutthata probabilis
tic justification of induction was also flawed. As will be pointed out in
Chapter4, theprobabilitythatApredictsBisequaltothenumberoftimes
thatAwasfollowed byBdivided bythetotalnumberofinstancesofAre
gardlessofwhetheritwasfollowedbyB.23Becausethefuture holdsanin
finite number ofinstances, the probability will always be zero, no matter
how numerousthepastobservations (anynumberdivided byan infinitely
large numberisstillzero).
Thus, Hume and his allies created a paradox. On one hand were their
seeminglyvalidconcernsabouttheflawsofinduction.Ontheotherhandwas



==================================================
                     PAGE 144                     
==================================================

128 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
the accumulation ofstunningscientific discoveries. Ifscience was based on
suchaflawedlogic, howcouldithavemanagedtobesosuccessful?
William Whewell: The Role ofHypothesis
Ittookphilosophersandscientiststwo hundredyearsofwatchingthesci
entificmethod succeedto understand how itworked and why ithad been
so triumphant. By the middle ofthe nineteenth century, it was becoming
clearthatscience employs a synergistic combination ofinductive and de
ductive logic. In 1840, William Whewell (1794-1866) published The His
tory andPhilosophy ofInductiveSciences.
Whewell was the first to understand the crucial role of induction in
the formulation ofa hypothesis. Whewell called ita happy guess, He said
thatscientificdiscoverystartswitha bold inductiveleap to a newhypoth
esis, but it is followed by deduction. Thatis to say, aftera hypothesis has
been induced, a prediction is deduced from said hypothesis. This predic
tiontakestheform ofa conditionalstatement:
If the hypothesis is true, then specificfuture observations are pre
dicted to occur.
When the observations are made, they will either be consistent with
theprediction, thus confirmingthe hypothesis, orconflictwith thepredic
tion, thus contradictingthe hypothesis.
What is a scientific hypothesis? It is a conjecture ofa suspected pat
tern, for example:
X predicts Y.
or
X brings about Y.
Thisconjectureisspawnedbyascientist'spriorexperience-noticing
repeated pairings ofX and Y. Once the XY hypothesis has been put for
ward, a testable prediction is deduced from it. This takes the form of a
conditionalproposition, whichiscomposedoftwo clauses: anantecedent
andaconsequent.Thehypothesisservesastheantecedentclause,andthe
prediction serves as the consequent clause. In situations where it is
merelyasserted thatXiscorrelated (predicts) withYthefollowing condi
tionalpropositionwouldapply:
IF X predicts Y, then future instances ofX wiU befoUowed by in
stances ofY.



==================================================
                     PAGE 145                     
==================================================

The Scientific Method and TechnicalAnalysis 129
In cases where the hypothesis asserts that Xcauses Y, the following
conditionalpropositionwouldapply:
IFX causes Y, thenifX isremoved, Yshouldnot occur.
Thepredictionsembodiedinthe consequentclause ofthe conditional
propositionare then comparedwithnewobservations. These mustbeob
servationswhose outcome is notyetknown. Thisis crucial! Thisdoes not
necessarilymean thatthe observationsconcernsomefuture event. Itsim
ply means that when the prediction is made, the outcome ofthe observa
tions is notyetknown. Inhistoricalscienceslikegeology, archeology, and
so forth, the observations are aboutevents that have already taken place.
However, the outcomeshave notyetbeenobserved.
Ifitturns outthatfuture observationsofXare notfollowed byYorif
removal ofXdoes prevent the occurrence ofY, then the hypothesis (an
tecedent) isprovento befalse bythevalid deductive form falsification of
the consequent.
IfX, then Y.
Not Y.
Valid Deduction: Therefore, notX.
If, however, future instances ofXare indeed followed by Y, or ifthe
removal of X does cause the disappearance of Y, the hypothesis is not
proven! Recallthataffirming the consequentisnotavaliddeductiveform.
IfX, then Y.
Y.
Invalid: Therefore, X.
Thatthepredictionhas beenconfirmedmerelyofferstentativeconfir
mation of the hypothesis. The hypothesis survives for the moment, but
morestringenttestsofitaresuretofollow.
Whewell agreed with Hume that inductive conjecture was a habit of
humanthought, butthe habitthat Hume so disparaged Whewell regarded
as fruitful though mysterious. He was unable to explain the mental
processes that gave rise to such creative thoughts though he believed in
ductive generalization was partofit. He called the ability to conjure a hy
pothesis an unteachable inventive talent but a crucial one, because,
without a hypothesis, a set of observations remain nothing more than a
disconnected collection of facts that could neither predict nor explain.
However, withone camethebreakthroughsofscience.



==================================================
                     PAGE 146                     
==================================================

.30 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Whewell'sdescriptionofthiscrucialcreativeaspectofsciencereminds
me ofa conversationIhad many years ago with Charles Strauss, a prolific
composer of some of Broadway's most successful musicals. In my early
twenties at the time, short on tactand long on audacity, Iasked him to ex
plain how he managed to be so productive. With more forbearance than I
deserved, Charlesdescribedadailydisciplineofreligiouslysittingatthepi
ano from 8 until 11 each morning and from 2 until 5 each afternoon,
whetherhefeltinspiredornot. Hetoldmehetreatedcomposingmusiclike
ajob. Duringthis time he wouldtesttentative melodies-musicalhypothe
ses. He modestly attributed his high productivity to 99 percent discipline
and1percentcreativetalent. However, asIthoughtaboutthisconversation
years later, it seemed otherwise. With an almost infinite number of note
combinations, his success at creating infectious tunes had to be a creative
talent-somespecialabilitytoseewhichofthose musical col'\iectures had
the potential to be melodies and which did not. This was what Whewell
called the happy guess-that unteachable talent of seeing a theme that
meaningfullyrelatesadisparatesetofobservationsornotesthateludesthe
averageperson.Afewfolks haveit.Thevastmajoritydonot.
Whewell'srealizationthatproposingahypothesiswasanactofinven
tion no less than the creation ofthe steam engine orthe light bulb repre
sented a profoundly important advance in thinking about science.24
Inductioncouldnotproducetruths onitsown, butitwasanecessaryfirst
step. This was aprofound departurefrom thepriornotion ofscience as a
systematic objective investigation followed by inductive generalization.
Whewellsawthescientistasa creatorasmuchasan investigator.
Karl Popper: Falsification and Bringing Deduction
Back into Science
In two landmark works, The Logic ofScienti.f'U; Discovery25 and Conjec
tures andRefutations,26 Karl Popper (1902-1994) extended the insightof
Whewell and redefined the logic of scientific discovery by clarifying the
role ofdeduction. Popper'scentralcontentionwasthatascientificinquiry
was unable to prove hypotheses to be true. Rather, science waslimited to
identifyingwhichhypotheses werefalse. This wasaccomplished by using
observed evidencein combinationwiththevalid deductiveform offalsifi
cationoftheconsequent.
Ifhypothesis His true, then evidence E is predicted to occur under
specifiedconditions (e.g., backtestofaTArule).
EvidenceE did not occurunderthe specifiedconditions.
Therefore, hypothesisH isfalse.



==================================================
                     PAGE 147                     
==================================================

The Scientific Method and TechnicalAnalysis 131
In taking this stance, Popper challenged the prevailing view advo
cated by a school ofphilosophy called logical positivism. Just as Francis
Bacon had revolted against the strictures of the Greek tradition, Popper
was in revolt against the Vienna Circle, the home of logical positivism.
Logical positivists believed that observations could be used to prove hy
potheses to be true. Popper demurred, saying that observed evidence
could only be used toprove a hypothesisfalse. Sciencewasa baloney de
tector, nota truth detector.
Popper justified his method, called falsificationism, as follows. A
givensetofobservationscanbeexplainedbyorisconsistentwith numer
oushypotheses.Therefore, the observeddata, byitself, cannothelp us de
cide which ofthese hypotheses is mostlikely correct.27 Suppose the data
is thatone ofmy shoes is missing. One hypothesis thatmightexplainthis
observationis thatI'madisorganizedhousekeeperwhoisalwaysmisplac
ingthings. Anotherhypothesis, whichisequallyconsistentwiththe obser
vation of a missing shoe, would be that my house was burglarized by a
one-leggedthiefwhoonlyhad useforoneshoe. Infact, aninfinitenumber
of hypotheses could be proposed that are consistent with (explain) the
missingshoe.
We havealreadyseen thatdatacannotbe used logicallyto deduce the
truth of a hypothesis. Attempts to do so commit the fallacy of affirming
the consequent. However, and this isthe keyto Popper'smethodoffalsi
28
ficationism, datacan be used to validlydeduce the falsehood ofa hypoth
esis by denial ofthe consequent. In other words, disconfirming evidence
can be used to reveal a false explanation. For example, finding the other
shoewouldbeevidencethatwouldfalsifytheone-leggedthiefhypothesis.
Thelogicalargumentisasfollows:
Premise 1: Ifa one-legged thiefis responsiblefor my missing shoe,
then I willnotfind the shoeinmyhome.
Premise2: Missing shoeisfound (consequent denied).
Conclusion: Therefore, the one-legged thiefhypothesis isfalse.
Popper's method of falsification runs against the grain of common
sense,whichisbiasedinfavorofconfirmatoryevidence.Aspointedoutin
Chapter2, intuition often tells us to test the truth ofa hypothesis by see
ing ifconfirmatory evidence can be found. We do so under the mistaken
impression that confirmatory evidence is sufficient to establish truth.
However, using evidence in this way commits the fallacy ofaffirming the
consequent. We are not wrong to suspect that confirmatory evidence
should be found ifthe hypothesis is true, butwe are wrong to think that
confirmatory evidence is sufficient to establish its truth. Said differently,



==================================================
                     PAGE 148                     
==================================================

132 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
confirmatoryevidenceisanecessaryconditionofaproposition'struthbut
nota sufficientcondition.29 Popper's pointwas thatthe absence ofneces
sary evidence is sufficient to establish the falsity ofa hypothesis, but the
presence ofnecessary evidence is not enough to establish its truth. Hav
ing four legs is a necessary condition of a creature being a dog, but the
presence offour legsis notsufficientto establishthata creature is a dog.
However,the observationthatthe creaturedoesnothavefourlegsissuffi
cienttofalsifythe contentionitisa dog. Popper'slogicoffalsification can
be seenas aprotectionagainstthe confirmationbiasthat infectsinformal
inference (see Chapter2).
One final example may help clarify the power offalsifying evidence
and the weakness ofconfirmatory evidence. It is the famous problem of
the black swan posed by philosopherJohn Stuart Mill (1806-1873). Sup
pose we wish to ascertain the truth of the proposition: 'All swans are
white.' Mill said, and Popper concurred, that no matter how many white
swanshavebeenobserved-thatis, no matterhowvoluminousthe confir
matory evidence-the proposition's truth is never proven. A black swan
may lurk just around the next corner. This is the limitation ofinduction
that so upset Hume. However, by merely observing a single non-white
swan, one may declare with certitude that the proposition is false. The
conditional syllogism below shows that the falsity ofthe propositionAll
swans are white is based on the valid deductive form falsification ofthe
consequent.
Premise 1:!fitis true thatallswansarewhite, thenallfutureobser
vations ofswanswill bewhite.
Premise 2:A nonwhiteswan is observed.
Valid Conclusion: The proposition that allswans arewhiteisfalse.
The general form of argument used to test a hypothesis under Pop
per'smethodoffalsification is:
Premise 1:Ifthehypothesisis true, thenfuture observationsarepre
dicted to havepropertyX.
Premise 2:An observation occurs thatlackspropertyX.
Valid Conclusion: Therefore, hypothesis isfalse.
Aswillbeseenin Chapters4and5, this is the logicused toteststatis
tical hypotheses.
l'he .-rovisional and CumulativeNatureof'Scientilic I{nowledge.
One implication ofPopper's method offalsification is thatall existingsci-



==================================================
                     PAGE 149                     
==================================================

The Scientific Method and TechnicalAnalysis 133
entific knowledge isprovisional. Whatevertheory iscurrentlyacceptedas
correctis alwaysa targetfor empiricalchallenge and the possibilityofits
being replaced by a more correct theory always exits. Today Einstein's
TheoryofRelativityis takenas correct. Thoughitspredictions have with
stood countless tests, tomorrow a new test may show itto be false or in
complete. Thus, science is a never-ending cycle ofconjecture, prediction,
testing, falsification, and new conjecture. Itisinthisway thatthe body of
scientific knowledge continually evolves toward an ever more accurate
representationofobjectivereality.
In mostinstances when oldertheories are replaced, itis not because
they are provenfalse so much as theyare shown to be incomplete. When
Newton's laws ofmotion were replaced by Einstein'stheories, Newtonian
physicswasstillmostlycorrect. Withinthelimiteddomainofeverydayex
perience, where bodies travel at normal speeds (approximately less than
90 percent ofthe speed oflight), Newton's laws still held true. However,
Einstein'stheory was correctin a widerdomain, which not onlyincluded
the motions ofeveryday objects, but also objects traveling up to and in
cluding the speed of light. In other words, Einstein's theory, which built
upon Newton's, wasmoregeneraland thussubsumedit.
The net result ofbuilding upon prior successful ideas (those whose
predictions have been confirmed) and pruning away wrong ideas (those
whose predictions have been falsified) is a body ofknowledge that con
tinuallyimproves. This cannotbesaidofany otherintellectual discipline,
where new styles are introduced but where newer is not necessarily
better. The fact that a scientistin anyfield ofscience knows more today
than even the best one living just a generation ago is beyond debate.
However, whether gangsta rap is better or worse than Mozart could be
argued endlessly.
l'he Restriction of Science to l'cstable Statements. Another
implication of Popper's method is that science must restrict itself to
testable hypotheses-propositions thatgenerate predictions aboutobser
vations not yet made. To say that a hypothesis has been tested and has
survived orhas been tested and falsified means that predictions deduced
from it have either been confirmed or contradicted by new observations.
Thecomparisonofpredictionswithnewobservationsisthe crucialmech
anismthatfostersthe continualimprovementofscientificknowledge. For
this reason, propositions that do not generate testable predictions must
beexcludedfrom the domainofscientific discourse.
The term prediction, in the context of hypothesis testing, warrants
some clarification, because TA is essentially an enterprise dedicated to
prediction. When wespeakofpredictionas itpertainstotestinga hypoth
esis, itdoes not necessarily mean foretelling the future though the pre-



==================================================
                     PAGE 150                     
==================================================

134 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
dicted observations may indeed lie inthe future. Rather, the termpredic
tion refers to the fact thatthe observations' outcomes are notyetknown.
Predictions made in historical sciences, such as geology, refer to
eventsthathavealreadytakenplaceperhapseonsago. Forexample, geol
ogy's dominant theory, plate tectonics, may predict thatspecific geologic
formations that were created millions ofyears ago would be observed if
an investigation ofsome specific location were to be carried out tomor
row. Infinance, the efficientmarket's hypothesis predicts thatifaTA rule
weretobe backtested, itsprofits, afteradjustmentfor risk, wouldnotex
ceed the riskadjusted return ofthe marketindex.
Oncethepredictionhasbeendeducedfrom thehypothesis,theopera
tions necessary to produce the new observations are carried out. They
may involve a visit to the location ofthe predicted geologic formation or
the back test ofthe TA rule. Then it becomes a matter ofcomparing pre
diction withobservation. Measuringthe degree ofagreementbetweenob
servation and prediction and making the decision about whether the
hypothesis is leftintactorfalsified is whatstatistical analysis isallabout.
Whatis importantfrom the standpointofscience is thatthe hypothe
sis is able to make predictions about observations whose outcomes are
notyetknown. Thisiswhatallowsahypothesistobetested. Observations
whose outcomes are known cannot serve this purpose because it is al
ways possible to fashion an explanation, after-the-fact that is consistent
with said observations. Therefore, activities that are unable or unwilling
to make testable predictions, thus opening them to the possibilityoffalsi
fication, do notqualifyasscientific.
'I'he Demat'cation I-roblem: Distinguishing Science I".'om .-seu
doscience. An important consequence ofPopper's method was that it
solveda keyprobleminthephilosophyofscience-definingthe boundary
between science and nonscience. The domain of science is confined to
propositions (conjectures, hypotheses, claims, theories, andsoforth) that
make predictions that are open to refutation with empirical evidence.
Popperreferred tosuchpropositionsasfalsifiable and meaningful. Propo
sitionsthatcannotbe challengedinthis mannerare unfalsifiable ormean
ingless. In otherwords they do notsay anything ofsubstance, something
thatsetsup a testableexpectation.
Unfalsifiable propositions may appear to assert something, but, in
fact, theydonot. Theycannotbechallengedbecausetheydonotsay,with
any degree ofspecificity, whatcan be expected to happen. In effect, they
are not informative. Hence, the falsifiability ofa proposition is related to
its information content. Falsifiable propositions are informative because
they make specific predictions. They may prove to be wrong, but at least
theysaysomethingofsubstance.Apropositionthatcannotgeneratefalsi-



==================================================
                     PAGE 151                     
==================================================

The Scientific Method and TechnicalAnalysis 135
fiable predictions essentiallysays any outcome canhappen. Forexample,
a weather forecast that says, It will be cloudy or sunny, wet or dry,
windy orcalm, and cold orhot, allows for all possible outcomes. Itcan
not be falsified. The only thing good that can be said about it is that its
lackofinformationiscompletelyobvious.
The problem is that pseudoscientists and pseudoforecasters are
clever about the way they word their predictions so as to obscure their
lackofinformationandnonfalsifiability. Considertheastrologer'sforecast
"You will meet a tall dark stranger, and your life will be changed." This
statementisimpossibletorefuteno matterwhatthe outcome. Shouldyou
evergo backfor arefund, youwillbegivenone oftwo answers: (1) bepa
tient-you will meetthestrangersoon, or(2) you alreadydid meeta dark
stranger and you life is indeed different, but you are oblivious to the
change. These answers cannotbe challenged because the prediction was
vague. It neither stated when your life would change or in what measur
able (verifiable)manner. Thisisafar cryfrom theprediction, "Before 7:00
P.M. next Wednesday you will see a man wearing one red shoe, walking
east on 42nd Street whistling 'Satin Doll.'" By 7P.M. next Wednesday, the
evidence will be in and you will have had the opportunityto evaluate the
prediction as true orfalse. Even ifthe prediction turns out to be false, it
was at least falsifiable allowing you to decide if future visits to the as
trologerare worthwhile.
The infomercial pitchmen that inhabit late-night television are mas
ters of the meaningless claim. "Wearing our copper bracelet will im
prove your golf game." What does improve mean? How is it to be
measured and when? As with the astrologer's forecast, the claim's
vagueness makes it impossible to deduce a testable (falsifiable) predic
tion. However, it is quite easy to imagine after-the-fact anecdotal evi
dence that is seemingly confirmatory. "Holy cow, I feel so much more
relaxed wearing my copper bracelet. I used getall tensed up before tee
ing off. Now my drives seem straighter. LastweekIalmosthad a hole in
one, and I don't curse nearly as much as I used to. My wife even says I
look more handsome in my golf outfit, and I think my hair has stopped
falling out."Althoughmeaninglessclaimsinvite confirmatoryanecdotes,
they are protected from objective falsifying evidence. The claimant
never gets nailed and the claim gains support from seemingly confirma
tionalanecdotal reports.
In contrast, the statement, "Wearing a copper bracelet will increase
the length ofyour drives by 25 yards," is informative and meaningful be
cause itgenerates a testable prediction. You hit 100golfballsnotwearing
the copper bracelet and determine their average distance. The next day,
do the same thing while wearing a copper bracelet and get their average
distance. Repeat this alternating series ofexperiments for 10 days. Ifthe



==================================================
                     PAGE 152                     
==================================================

136 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
drives on copper-bracelet days are less than 25 yards better, evidence re
futing the claimwould beinhand.
Limitations of Popper's Method. Asimportantas Popper'smethod
offalsification is to modemscience, it has beencriticized ona numberof
grounds. Critics assert that Popper's contention that hypotheses can be
definitively falsified overstates matters. Although the observation of a
black swan can neatly and logically falsify the universal generalization
thatallswansare White, the hypothesesofrealscience arefar more com
plex30 and probabilistic (nonuniversal). They are complex in the sense
thata newlyproposedhypothesisrests onnumerousauxiliaryhypotheses
that are assumed to be true. Thus, ifa prediction deduced from the new
hypothesisislaterfalsified, itisnotclearwhetherthenewhypothesiswas
inerrororoneofthemanyauxiliaryhypotheseswasincorrect.Thisproved
tobethe casewhenthesolarsystem'seighthplanet, Neptune, wasdiscov
ered. The aberrant path ofUranus was not due to flaws in Newton's laws
butin the auxiliary hypothesis thatthe solarsystem contained onlyseven
planets. However, when the theory did fail earlyin the twentieth century,
it was indeed because ofimperfections in Newton's laws. Doing real sci
ence is tricky business. TA is a long way from facing these problems be
cause we are still at the at the point of scratching the data for reliable
predictiverules.
Moreover, because many ofthe hypotheses ofscience are probabilis
tic, as would be the case in TA, an observation that contradicts the hy
pothesis can never be taken as certain evidence offalsity. The aberrant
observationmaybea chance occurrence. Itishere thatstatisticalanalysis
entersthepicture.Aswill bepointedoutinChapters4and5, the decision
to reject a hypothesis on the basis of observed evidence runs a certain
probabilityofbeingwrong. Statisticshelps us quantifythisprobability.
Despitetheseand otherlimitationsthatgobeyondourconcernshere,
Popper's contributions to the development ofthe scientific method have
beenenormous.
The Information Content of Scientific Hypotheses
To recap, a hypothesis is informative ifit can make testable predictions.
This opens itto the possibility ofbeingfound false. Thus, the falsifiability
ofa hypothesisanditsinformationcontentare related.
Within the domain ofscientifically meaningful hypotheses, there are
degrees of information content and falsifiability. Some hypotheses are
moreinformationrichandhence morefalsifiable than others.
When Popper referred to a bold conjecture he was speaking of a
highly informative hypothesis from which many falsifiable predictions



==================================================
                     PAGE 153                     
==================================================

The Scientific Method and TechnicalAnalysis 137
could be deduced. The scientist'sjob, therefore, is to continually attempt
to refute anexistinghypothesis and to replace itwithanevenmore infor
mativeone. Thisspursthe improvementofscientificknowledge.
An information-rich hypothesis makes manyprecise (narrow ranged)
predictions abouta broadrange ofphenomena. Eachpredictionpresents
an opportunity to show the hypothesis is false. In other words, the more
informative a hypothesis the more opportunities it presents for falsifica
tion. In contrast, low information, timid hypotheses make fewer or less
precise predictions. Consequently they are more difficult to falsify. For
example, a TArule that claims high profitability on any instrumentin any
time frame makesaboldandinformation-richclaimthatthatcouldbefal
sified byshowing that itis unprofitable on one marketin one time frame.
In contrast, a method thatonly claimsto be marginally profitable on S&P
futures on the one-weekbartime scale is timid, has low information con
tent, and is hard tofalsify. The onlyopportunityto refute it would be lim
ited to back test ofthe S&P 500 on a weekly showing that was not at all
profitable.
SomeTAmethodsthatareseeminglyinformativearenot. ElliottWave
Principle is a case in point. On the surface, it bravely proclaims that all
price movement in all markets over all time scales can be described by a
single unifying principle. This proposition is seemingly confirmed by the
ability of EWP practitioners to produce a wave count for any prior seg
mentofprice data. Infact, EWPistimidto the pointofbeingmeaningless
becauseitmakesnofalsifiable predictionsoffuture pricemotion.31
Asecondcaseinpoint, onemorepleasingtotheTAcommunity,isthe
Efficient Markets Hypothesis (EMH). In this context, the term efficiency
refers to the speed with which prices reflect all known and knowable in
formation that is relevant to the future returns ofan asset. In an efficient
market, relevant information is presumed to be reflected in price almost
instantaneously. EMH comesinthreeflavors. Indescendingorderofbold
ness, information content, and falsifiability they are: EMH strong, EMH
semistrong, and EMHweak.
EMHstrongassertsthatfinancialmarketsareefficientwithrespectto
allinformation, evenprivateinsideinformation. Thisversionpredictsthat
allinvestmentstrategies,betheybasedonaninsidetipfromthepresident
about an impending takeover, or based on public information ofa funda
mental ortechnical nature, will be useless in earningmarket-beating (ex:
cess) returns. This most audacious version ofEMH is also seeminglythe
mostinformativeandfalsifiable becauseanyevidenceofabnormalprofits
from anyinvestmentstrategywhatsoever, irrespectiveofthetype ofinfor
mationorform ofanalysisused, wouldbesufficienttorefute EMHstrong.
However, because information that is known privately can never be con
firmed, thisversionisnottestableinapracticalsense.



==================================================
                     PAGE 154                     
==================================================

138 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The semistrong version of EMH makes a less informative and nar
rowerclaim, sayingthatthemarketisonlyefficientwith respecttopublic
information. This version of EMH can be falsified with any evidence of
market-beatingreturns produced byan investingstrategybased oneither
public fundamental data(PIE ratios, bookvalues, andso forth) ortechni
cal data (relative strength rank, volume turnover ratios, and so forth). In
effect, EMH semistrong denies the utility of fundamental and technical
analysis.
Finally we have EMH weak, which makes the least bold and leastin
formative claim. Itasserts thatthe marketis onlyefficientwith respect to
pastprice, volume, and othertechnical data. Because EMH weakonlyde
nies the utility oftechnical analysis, it presents the smallest and hardest
to-hit target by would-be falsifiers. Their only hope would be to present
evidence that shows excess returns generated by an investment strategy
based onTA.
Because EMH weak is the hardest version to falsify and is thus the
least likely to be proven false, its falsification would also generate the
mostsurprise. Inotherwords, ofallversionsofEMH,falsification ofEMH
weakwouldgeneratethe biggestincreasein knowledge. This pointsouta
general principle of science: the largest gains in knowledge occur when
the mosttimidand hardesttofalsify hypothesesarefalsified. Atestshow
ing that inside information, like the tip from a corporate president, was
able to generate excess returns (i.e., falsification of EMH strong) would
not be surprising nor would we learn very much from it. Big deal, so in
sideinformationgeneratesprofits. Whatelseisnew? Incontrast,thefalsi
fication of EMH weak would be a highly informative event for both TA
practitioners and EMH supporters. Not only would it mean the final de
struction ofthe EMH, an importantprinciple offinance for over40years,
butitwouldbeanimportantconfirmationofTA'svalidity. Bothwouldrep
resentlarge changesin the currentstate ofknowledge.
Thus, itcan be saidthatthe gainin knowledge thatoccurs uponfalsi
fication of a hypothesis is inversely related to its information content.
Likewise, it can be said that the knowledge gained by confirmation ofa
hypothesis (observations consistent with its predictions) is directly re
lated to the information content ofthe hypothesis. The most informative
hypotheses make the most audacious claims ofnew knowledge. They at
tempt to bring within the realm of understanding the broadest range of
phenomenawith the greatest degree ofprecision while at the same time
involvingthe fewest assumptions. Whensucha hypothesisisfalsified, we
are not very surprised nor do we learn very much. Few would have ex
pectedthe hypothesis to be confirmed, exceptperhapsfor the bold scien
tistproposingit. Forexample,supposeaboldnewtheoryofphysicsisput
forward, one of whose predictions is that it is possible to build an anti-



==================================================
                     PAGE 155                     
==================================================

The Scientific Method and TechnicalAnalysis 139
gravity device. Iftrue, such a theory would represent a majorincrease in
knowledge. However, if the device fails to work, no one would be sur
prisedbytheprediction'sfailure. However, itisexactlythe oppositewhen
a timid hypothesis is falsified. For example a timid hypothesis would be
onethatmerelyassertsthatthe currentlyacceptedtheoriesofphysicsare
true and predicts that the antigravity device should fail. The falsification
ofthisweakhypothesisviaobservationsoftheantigravitydeviceworking
would result in a very significant gain in knowledge-the verification of
newphysics.
Themost timid hypothesis thatcanbe putforward is one thatasserts
that there have been no new discoveries. In other words, it says that all
thatiscurrentlyknownisall thatthere is to know. This hypothesisdenies
the truth of any other hypothesis that asserts that something new has
been discovered. The timid hypothesis that asserts that nothing new had
beendiscoveredhasaspecialnameinscience. Itiscalledthenullhypoth
esis and itis the startingassumptioninthe investigation ofany claim that
a new discovery has been made. Whether that claim asserts that a new
vaccine will cure a dreaded disease, that a new principle ofphysics tells
ushowtonullifygravity, orthataTArulehaspredictivepower,wealways
startbyassumingthatthe nullhypothesisis true. Then, ifevidencecanbe
producedthatfalsifiesthe null hypothesis, amosttimidclaim, itgenerates
a biggainin knowledge.
Thus, scienceproceeds as follows. Every time a bold new hypothesis
is putforward, itspawns an opposing claim, the null hypothesis. The null
isastimidasthenewhypothesisisbold.JonasSalk'sboldhypothesis that
hisvaccinewould preventpolio betterthana placebo spawneda compet
ing claim, the null hypothesis. It made the timid prediction that the vac
cine's ability to preventinfectionwould be no betterthan a placebo. This
was a timid prediction because every prior attempt to develop a vaccine
against polio had failed. These two competing claims left no middle
ground. If one hypothesis could be falsified, by logic's Law of the Ex
cluded Middle, we know that the othermust be true. Salk's experin1ental
evidencemadeitclearthatthe rateofinfectionamongthosereceivingthe
real vaccine was significantly less than those receiving the placebo. In
otherwords itwassufficienttofalsifythenull'sprediction. Thiswasasur
prisingresultthatrepresented ahugeincreasein medical knowledge!
How Scientists Should Respond to Falsification
Howshoulda scientistrespond whena hypothesis ortheory thathassur
vived many prior tests is ultimately falsified because recent observations
conflict with predictions? The proper response is whatever leads to the



==================================================
                     PAGE 156                     
==================================================

140 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
biggestincrease in knowledge. Becausescientists are human beings, they
sometimesfail to dowhatisscientificallycorrect.
There are two possible responses that increase knowledge. The first
is to preserve the existing hypothesis by using it to predict new, previ
ously unknown facts. Ifthese new facts are confirmed and can explain
whythe observationsthathadbeenin conflictwiththehypothesisare no
longer in conflict, then the hypothesis deserves to be retained. The new
facts represent an increase in what we know about the world. Asecond
proper response is to throw out the old hypothesis and propose a new
one that not only accounts for all observations that had been explained
by the prior hypothesis but also explains the new discordant observa
tions. Thisalso representsan increasein knowledge in the form ofa new
hypothesis with greater explanatory orpredictive power. However, in ei
ther casethe correctresponse is to do whateveradvances the frontier of
knowledge the most.
Unfortunately the interests ofscience sometimes take a back seat to
personal agendas. Human nature gets in the way of good science. Emo
tional, economic and professional ties to a prior hypothesis can motivate
attempts to explain away the discordant evidence in a way that reduces
the information content and falsifiability of their cherished hypothesis.
This moves the frontier ofknowledge backward. Fortunatelyscience is a
collective self-correcting enterprise. The community ofscientists happily
takes theirfallen brothers to task when theystray from the path ofright
eousnessin this way.
Someexampleswillclarifytheseabstractconcepts. First, Ipresentan
example of a proper response to predictions being contradicted by new
observations. In this case, new facts were predicted to rescue an estab
lished theory from falsification. This high road was taken by two as
tronomers during the nineteenth century, and itled to new knowledge in
the form ofa newplanetbeingdiscovered. Atthattime, Newton's laws of
motion and gravity were the accepted physics ofplanetary motion. They
had been confirmedand reconfirmed by countless observations, butthen,
to the surprise of astronomers ofthat day, new and more powerful tele
scopes showed the planetUranus was deviating from the orbit predicted
by Newton's laws. A rigid and improper application of falsificationism
would have called for an immediate rejection of Newtonian mechanics.
However, all theories rest on a bed of auxiliary assumptions. A key as
sumptioninthiscasewasthatthesolarsystemcontainedonlysevenplan
ets with Uranus being the seventh and furthest from the sun. This led
astronomers Adanls and Leverrierto boldly predictthe existence ofa yet
undiscovered eighth planet (the new fact), lying beyond Uranus. If this
were true, that new planet's gravitational effects could explain the aber
rantmotionofUranus, whichseemedtobeinconflictwith Newton'slaws.



==================================================
                     PAGE 157                     
==================================================

The Scientific Method and TechnicalAnalysis 141
In addition, ifan eighthplanet did exist, one ofNewton's laws would pre
dictthe exactspotin the sky where the new planetwould be observable.
This was a bold, informative, and highly falsifiable prediction that put
Newton's laws to a most stringent test. The astounding prediction made
byAdams and Leverrierabout where the new planetwould appearin the
sky was indeed confirmed in 1846 with the discovery of Neptune. They
saved Newton's laws from falsification by demonstrating that the theory
was notonly able to explainthe deviant behavior ofUranus butwas also
able to makea highlyaccurateprediction. Thisis the kosherwayto retain
a theory whenitisconfrontedwithdissonantobservations.
Intheend, however, Newton'slawsprovedto beprovisionallytrue, as
is ultimately the case for all laws and theories. Although Newton's laws
had worked perfectlyfor more than 200 years, early in the twentieth cen
tury more precise astronomical observations were found to be truly in
consistent with the theory's predictions. The old theory had finally been
falsified and itwas timefora newone. In 1921,AlbertEinsteinresponded
properly byputtingforward his newand moreinformativeTheoryofGen
eral Relativity. Today, almost one hundred years later, Einstein's theory
hassurvived allattempts tofalsify it.
ewton's theory qualified as scientificbecause itwas open to empiri
cal refutation. In fact, Newton's theory was not wrong so much as it was
incomplete. Einstein'sGeneralTheoryofRelativitynotonlyaccountedfor
all ofthe phenomena covered by the Newtonian model, but it accommo
dated the new observations that conflicted with Newton's more limited
theory. This is how science progresses. Longevity and seniority mean
nothing. Predictiveaccuracyand explanatorypowerareeverything.
The case ofAdamsand Leveriermakeclearwhyclaimsmustbeopen
to empirical refutation. Falsifiability alone gives scientific knowledge a
large advantage overconventionalwisdom. The ability tojettisonfalse or
incomplete ideas and replace them with evermore informative ones pro
duces a body ofknowledge that is self-correcting and in a continualstate
ofimprovement.Thisintumprovidesastablebaseuponwhichnewideas
can be erectedthatreacheverhigherlevels ofunderstanding. Intellectual
activities thathave noprocedurefor eliminatingerroneous knowledge in
evitably get bogged down in nonsense. This is preciselythe problemwith
the popularversionofTA.
Now we will consider an example ofan improperresponse to falsi
fication. Itoccurs in the field offinance. The injection ofscience intofi
nance is relatively recent. Perhaps this explains the defensive,
unscientific response of those who support the efficient markets hy
pothesis. When observations collided with their favorite theory, they
tried tosave itfrom falsification byreducingitsinformation content. As
mentioned earlier, its least informative version, EMH weak, predicts



==================================================
                     PAGE 158                     
==================================================

142 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
thatinvestmentstrategiesbasedon TA32 will not be able to earnrisk ad
justed returns that beat the market index. When EMH supporters were
faced with studies showing that TA-based strategies were able to earn
excess returns,33 they responded by trying to immunize their theory
from falsification. They did so by inventing new risk factors and
claimed that the excess returns earned by TA were merely compensa
tion for risks inherentin pursuing such a strategy. In otherwords, EMH
defenders claimed thatinvestorswhofollowed theTAstrategywere ex
posing themselves to a risk that was specific to that strategy. This al
lowed the EMH supporter to characterize the TA strategy's returns as
non-market-beating. Recall EMH does not claim that earning returns
higherthan the marketis impossible. Itonlysays thathigherreturns en
tail the assumption of additional risk. Ifthe returns earned by the TA
strategy were indeed compensation for bearing higher risk, then EMH
would remain intactdespite the studiesthatshowTAstrategy earninga
return higherthan the marketindex.
There was one problem with the way EMH supporters did this. They
34
cooked up the riskfactor afterthe TAstudieshad beenperformed. This
is not kosher science. It would have been scientifically correct if EMH
had defined the risk factor in advance ofthe study and predicted that a
testoftheTAmethodwouldgeneratewhatappearedtobemarket-beating
returns. Had they done this, the status of EMH would have been en
hanced with a successful prediction. Instead, EMH supporters took the
low road to save their favored hypothesis by giving themselves the li
cense to invent a new risk factor any time they needed to explain away
findings that conflicted with their favored theory. In so doing, EMH sup
porters rendered theirhypothesis unfalsifiable, therebydraining itofany
infornlationcontent.
The precedentfor this knowledge-regressive method ofimmunizing
the EMH hypothesis against falsification had already been established
byearlierflawed defensesofEMH. Theseearlier, andsimilarlymisguided,
efforts to save EMH were in response to studies that had shown that
public fundamental information, such as the price-to-book ratio and PE
35
ratio, could be used to generate excess returns. In response to this in
convenientevidence, EMH defenders claimed that low price-to-book ra
tios and low PE ratios were merely signals of stocks with abnormally
high risk. In other words, the fact that a stock's price is low relative to
its bookvalue is an indication that the company is facing difficulties. Of
course, this reasoning is circular. What is key here is the fact that EMH
advocates had notdefined lowprice-to-book orlowPE as risk factors in
advance ofthestudiesshowingthatstockswith these traitswereableto
earn excessreturns. Had EMH theorists done so, itthey would have bol-



==================================================
                     PAGE 159                     
==================================================

The ScientificMethod and TechnicalAnalysis 143
stered the information content ofEMH with an additional dimension of
risk. Instead, EMH theoristsinventedthese riskfactors afterthefact, for
the specific purpose of explaining away discordant observations that
had already been made. Such explanations are termed ad-hoc hypothe
ses-explanationsinventedafterthefact for the specificpurpose ofsav
ing a theory or hypothesis from being falsified. Popper referred to this
knowledge regressive, save-the-theory-at-any-cost behavior as falsifica
tion immunization.
Had Popper known of it, he would have chastised the die-hard sup
porters of EMH, but he would have probably applauded the efforts of
those advocating behavioral finance. This relatively new field has pro
posedtestablehypothesesthatexplaintheprofitabilityofstrategiesbased
onpublic technical andfundan1ental dataasarisingfrom the cognitivebi
asesand illusionsofinvestors. Itis ironicthaterroneousbeliefsintheva
lidity of subjective TA and the valid profitability of some forms of
objectiveTAmay bothbethe result ofcognitivefoibles.
The Scientific Attitude: Open yet Skeptical
Falsificationism makes a clear distinction between two phases of scien
tific discovery: proposal and refutation. These phases demand different
mindsets-openness and skepticism. The coexistence of these opposite
mindsetsdefines thescientific attitude.
An attitude of openness to new ideas is vital when hypotheses are
being formulated. The willingness36 to see things in a new way, to ad
vance newexplanations and take boldinductive leaps characterizes the
proposal phase. Most practitioners of TA function well in this mode.
New indicators, new systems, and new patterns get proposed all the
time.
However, once a bold conjecture has been made, receptivity must
morph into skepticism. Doubt about the new idea motivates a relentless
search for its flaws. Thus, there is an ongoing tension in the scientist's
mind between speculative curiosity and hard-nosed disbelief. However,
the doubt is not an unremitting skepticism but one that yields to persua
sive new evidence. This is the quasi-schizoid state that defines the scien
tificattitude.
Beyond the distrust of the new hypothesis, another form of skep
ticism thrives in the mind of a scientist: doubt about the mind itself.
This springs from a profound awareness ofthe all-too-human tendency
to generalize hastily and leap to unfounded conclusions (see Chapter
2). The procedures of science can be seen as safeguards against these
tendencies.



==================================================
                     PAGE 160                     
==================================================

144 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
THE END RESULT:
THE HWOTHETICO-DEDUCTIVE METHOD
37
Somewouldsaythereisnosuchthingasthescientificmethod. "Thesci
entific method, as far as it is a method is nothing more than doing one's
damnedest with one's mind no holds barred."38 At its essence it is intelli
gentproblemsolving.
The problem-solving method used in science today has come to be
knownasthehypothetico-deductivemethod. Itiscommonlydescribedas
having five stages: observation, hypothesis, prediction, verification, and
conclusion. "In actualscientific work these stages are so intertwinedthat
it would be hard to fit the history ofany scientific investigation into this
rigid scheme. Sometimes the different stages are merged or blurred, and
frequentlytheydonotoccurinthesequencelisted."39Rather, itisa useful
waytothinkabouttheprocess.
The hypothetico-deductive method was initiated by Newton in the
seventeenthcentury, butwasnotformally nameduntilafterPopper'scon
tributionswereintroduced.Itisthe outgrowthofseveralhundredyearsof
tusslingbetweenscientists andphilosophers. The methodintegratesboth
inductive and deductive logic, payingheed to their individuallimits while
leveragingtheirrespectivepowers.
The Five Stages
1. Observation: Apossiblepattern orrelationship is noticed in a set of
priorobservations.
2. Hypothesis: Based on a mysterious mixture ofinsight, prior knowl
edge, and inductive generalization, itis hypothesized that the pattern
is not an artifact of the particular set of observations but one that
should be found in any similar set of observations. The hypothesis
maymerelyassert thatthe patternis real (scientific law) oritmaygo
further and offeran explanation about why the pattern exists (scien
tifictheory).
3. Prediction: Apredictionisdeducedfrom the hypothesisandembod
ied in a conditional proposition. The proposition's antecedent clause
is the hypothesisand its consequentclauseis theprediction. Thepre
diction tells us whatshould be observed in a newsetofobservations
ifthehypothesisisindeedtrue. Forexample: Ifthehypothesisistrue,
then X should be observed if operation 0 is performed. The set of
outcomes defined byXmakes clearwhichfuture observationswould
confirm the prediction and, more importantly, which future observa
tionswouldbeinconflictwithit.



==================================================
                     PAGE 161                     
==================================================

The Scientific Method and TechnicalAnalysis 145
4. Verification: ew observations are made in accordance with the op
erationsspecified and compared to the predictions. Insome sciences
the operationisa controlledexperiment. Inothersciencesitisan ob
servationalstudy.
5. Conclusion: An inference about the truth orfalsity ofthe hypothesis
ismadebasedonthe degreetowhichthe observationsconformtothe
prediction. This stage involves statistical inference methods such as
confidence intervals and hypothesis tests, which are described in
Chapters4and 5.
An Example (Tom TA
The following is an example of the hypothetico-deductive method as it
would beapplied to testinga new ideaabouttechnicalanalysis.
1. Observation: It is noticed that when a stock market index, such as
theDowJonesaverageortheS&P500, rises aboveits200-daymoving
average, it generally continues to appreciate over the next several
months (probabilisticgeneralization).
2. Hypothesis: On the basis of this observation, inductive generaliza
tion, and priorfindings oftechnical analysis, we propose the follow
ing hypothesis: Upward penetrations of the 200-day moving average
by the DJIA will, on average, produce profitable long positions over
the nextthree months. I'll referto thishypothesisas 200-H.
3. Prediction: On the basis ofthe hypothesis, we predictthat an obser
vational investigation, or back test, will be profitable. The hypothesis
andthepredictionareturnedintothefollowing conditionalstatement:
If200-Histrue, then thebacktestwiU beprofitable. Howeverthispre
diction creates a logical problem. Even ifthe back test is profitable it
will notbehelpfulinprovingthe truth of200-H because, aspreviously
pointed out, while a profitable backtest would be consistentwith the
truth of200-H, it cannotprove that 200-H is true. An attempt to do so
wouldcommitthefallacy ofaffinningthe consequent(seeargument 1
below). If, on the other hand, the back test turns out to be unprof
itable, it would bevalid to conclude 200-H is false by the valid logical
formfalsification ofthe consequent. SeeArgument2below.
Argument 1
Premise 1:If200-His true, then a back testwiU beprofitable.
Premise2: The back testwasprofitable.
Invalid Conclusion: Therefore, 200-Histrue. (fallacyofaffirming
the consequent)



==================================================
                     PAGE 162                     
==================================================

146 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Argument 2
Premise 1:If200-His true, then a back testwillbeprofitable.
Premise2: The back test wasNOTprofitable.
Valid Conclusion: Therefore, 200-Hisfalse.
However, our objective is to prove the truth of200-H. To get around
this logical problem, suppose we had formulated a null hypothesis at
stage2, specifically, Upwardpenetrationsofa200-daymovingaver
age do notgenerateprofits over thefollowing three months. Let's re
fer to this as Null-200. From this we can formulate the following
conditional proposition: If Null-200 is true, then a back-test will
NOT be profitable. Ifthe back test does turn out to be profitable, we
canvalidlyargue thatthe null hypothesishas beenfalsified (falsifying
the consequent). By the Law ofthe Excluded Middle, either 200-H is
true or Null-200 is true. There is no middle ground; there is no other
hypothesis possible. Thus by disproving the null we will have indi
rectly proven that 200-H is true. Thus we have the following condi
tionalsyllogism:
Premise 1: IfNull-200 is true, then the back test will be un
profitable.
Premise 2: The back test was NOT unprofitable (i.e., it was
profitable).
Valid Conclusion: Null-200isfalse, therefore200-His true.
4. Verification: The proposed rule is back tested and its profitability is
observed.
5. Conclusion: Determining the meaning of the results is a matter of
statisticalinference, thetopictreatedoverthe nextthreechapters.
RIGOROUS AND CRITICAL ANALYSIS OF
OBSERVED RESULTS
Thefifthphaseofthehypothetico-deductivemethodpointsto anotherim
portant difference between science and nonscience. Inscience, observed
evidence is not taken at face value. In other words, the obvious implica
tionoftheevidencemaynotbeitstrue implication. Theevidencemustbe
subjected to rigorous analysis before a conclusion can be drawn from it.
The evidence of choice in science is quantitative data, and the tool of
choicefor drawingconclusionsisstatisticalinference.
An important scientific principle is the preference for simpler expla-



==================================================
                     PAGE 163                     
==================================================

The Scientific Method and TechnicalAnalysis 147
nations(Okam'sRazor). Assuch,astoundinghypothesesaregivenserious
consideration only after more commonplace hypotheses have been re
jected. Sightings ofa UFO are not in1mediatelyinterpreted as evidence of
analienvisit. More mundane accountssuch as ball lightning, weatherbal
loons, ora new aircraft mustfirst be discredited before an invasion from
outerspaceis takenseriously.
Thus a scientific attitude toward an extraordinarily profitable rule
backtestwould first considerand rejectotherexplanations before enter
taining the possibilitythat a significantTA rule has been discovered. The
possibleexplanationsofgoodperformanceunrelatedtoarule'spredictive
poweraregoodluckduetosamplingerror(seeChapters4and5) andsys
tematicerrordue to data-miningbias(see Chapter6).
SUMMARYOF KEYASPECTS OF TIlE
SCIENTIFIC METHOD
Thefollowing isasummaryofthe keypoints ofthescientificmethod:
• No matter how voluminous the evidence, the scientific method can
neverconclusivelyprovea hypothesis tobetrue.
• Observedevidence used incombinationwiththe deductiveform falsi
fication ofthe consequentcan be used to disprovea hypothesiswitha
specified degree ofprobability.
• Science is restricted to testable hypotheses. Propositionsthatare not
testable are notwithin the domainofscientific discourseandare con
sidered to bemeaningless.
• A hypothesis is testable if and only if predictions about yet-to-be
made observations canbe deducedfrom the hypothesis.
• Ahypothesis that can only explain past observations but that cannot
makepredictionsaboutnewobservations is notscientific.
• Ahypothesisis tested bycomparingitspredictionswithnewobserva
tions. If predictions and observations agree, the hypothesis is not
proven, but merely receives tentative confirmation. If they don't
agree, the hypothesisistakento befalse orincomplete.
• All currently accepted scientific knowledge is onlyprovisionallytrue.
Its truth is temporaryuntila testshowsitto befalse, atwhichpointit
is replacedorsubsumed bya more completetheory.
• Scientificknowledge iscumulativeandprogressive.Asolderhypothe
ses are shown to be false, they are replaced bynewer onesthatmore
accurately portray objective reality. Science is the onlymethod ofin
quiry or intellectual discipline which can claim that newer is better.



==================================================
                     PAGE 164                     
==================================================

148 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Though knowledge, styles, and methods in other disciplines, such as
music, art, philosophy, or literary criticism, may change over time, it
cannotbe claimedthatnewisnecessarilybetter.
• Any set ofpast observations (data) can be explained by an infinite
number of hypotheses. For example, Elliott Wave Theory, Gann
Lines, classical chart patterns, and a roster of other interpretive
methods can all explain past market behavior according to their
own paradigm of analysis. Therefore, all these explanations are
said to be empiricallyequal. The onlyway to decide which ones are
better is by seeing how well they can predict observations whose
outcomes are not yet known. Those methods that cannot generate
testable (falsifiable) predictions about new observations can be
eliminated immediately on the grounds that they are scientifically
meaningless. Those methods that can generate predictions that are
found to conflict with future observations can be eliminated on
grounds that they have been falsified. Thus, only methods that can
make testable predictionsthatdisplaygenuinepredictivepowerde
serve to be retained in the bodyofTA knowledge.
IF 11\ WERE TO ADOPT THE SCIENTIFIC METHOD
This section examines consequences of TA adopting the scientific
method.
The Elimination ofSubjective TA
The most important consequence of TA adopting the scientific method
would be the elimination ofsubjective approaches. Because they are not
testable, subjective methods are shielded from empirical challenge. This
makes themworsethanwrong. Theyaremeaninglesspropositionsdevoid
of information. Their elimination would make TA an entirely objective
practice.
SubjectiveTAwouldbeeliminatedinoneoftwoways: bytransforma
tionintoobjectivemethodsorabandonment. Perhaps GannLines, subjec
tive divergences, trend channels, and a host of subjective patterns and
concepts embody valid aspects ofmarket behavior. In their present sub
jectiveform, however, wearedeniedthis knowledge.
Transformingasubjectivemethodintoanobjectiveversionis nottriv
ial. To illustrate a case where this has been done, I discuss an algorithm
fortheautomateddictionofheadandshoulderpatternsandtestresults in
thesection "ObjectificationofSubjectiveTA: An Exan1ple."



==================================================
                     PAGE 165                     
==================================================

The Scientific Method and TechnicalAnalysis 149
Elirnimttion of'Meaningless Forecasts
Itis not practical to assume that all subjective practitioners will follow
my call to objectify or close up shop. For those who continue to use
subjective approaches, there is an important step that can be taken to
make theiroutput, ifnot theirmethodology, objective. Henceforth, they
would issue only falsifiable forecasts. This would at least make the in
formation they provide meaningful and informative. In this context, in
formative does not necessarily mean correct but rather that the
forecasts have cognitive content that passes the discernable difference
test discussed in the Introduction. In other words, the forecast would
conveysomething ofsubstance, the truth ofwhich can be clearly deter
mined by subsequent market action. In other words, the forecast will
make explicit or clearly imply which outcomes would show it to be
wrong. As stated previously, a forecast that does not make clear what
future events constitute prediction errors in essence says that any out
come can happen.
At the present time, most subjective forecasts, often referred to as
market calls, are meaningless. In all likelihood, this is not obvious to ei
ther consumers or the analysts issuing the forecasts. First, consider a
forecast thatisclearlymeaningless: "Myindicatorsare nowpredictingthe
market will either go up an infinite percentage, down 100 percent, or
something in between." On itsface, the statementis unfalsifiable because
there is no outcome that could possibly conflict with the prediction. The
only good thing about the forecast is that its lack ofmeaning and lack of
falsifiability are transparent. A more typical market call goes something
likethis: "Onthe basisofmy indicators [fillin one ormoreTAmethods], I
ambullish."Thisunfalsifiablestatementisjustasmeaningless, butitslack
ofsubstance isnotobvious. Thoughthere isapredictionofa rise, thepre
diction leaves unclear when it might occur or under what circumstances
thepredictionwould bewrong.
This bullish stance could have been made meaningful by clearly ex
cluding certain outcomes. Forexample, I expect the market to rise more
than 10 percent from current levels before it declines by more than 5
percentfrom current levels. Any instance of a decline of greater than 5
percentbeforea rise of10percentwould besufficientto classifythefore
castasanerror.
Ifyou ever suspect that you are being fed a meaningless prediction,
herearesomegood antidotes. Ask thefollowing question: "Howmuchad
verse movement (opposite to the direction predicted) would have to oc
cur for you to admit this forecast is mistaken?" Or "What outcomes are
precludedbyyourprediction?" Or"When, andunderwhatconditions, can
theforecastbeevaluated,suchasthepassageoftime,changeinprice (ad-



==================================================
                     PAGE 166                     
==================================================

.50 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
verse or favorable), or a specific indicator development?" Reader, I warn
younotto dothisifyouaresqueamishaboutwatchingpeoplesquirm.
Makingsubjectiveforecasts meaningfulwithan up-frontstatementof
when and how the forecast will be evaluated would eliminate the market
guru's after-the-fact wiggle room. Some ways ofadding meaning to a sub
jective forecast include (1) defining a future point in time when the fore
cast will be evaluated, (2) defining the maximum degree of adverse
movementthatwould be allowedwithoutdeclaringthe predictionwrong,
and (3) predictingaspecifiedmagnitudeoffavorable movementpriorto a
specified magnitude of unfavorable movement (X percent favorable be
fore Y percent unfavorable). Steps like these would allow a subjective
practitioner to develop a track record ofmeaningful market calls. Mean
ingful track records can also result from specific transaction recommen
dationsmadein real time.
One limitation ofthis recommendation is that it would still leave un
clearwhat the profitable track record represents. This is because subjec
tiveforecasts arederivedinanundefinedway, soevenifaprofitabletrack
record ofmeaningfulpredictionsisbuiltup overtime, itcannotbe known
thattheywerethe resultofa consistentanalysisprocedurethatcan bere
peated in the future. In fact, itis likelythat the method ofanalysis is not
stable over time. Studies of expert subjectivejudgment indicate that ex
perts do not combine infornlation in a consistent mannerfrom onejudg
ment to the next. "Intuitive judgments suffer from serious random
inconsistencies due to fatigue, boredom, and all the factors that make us
hunlan."40In otherwords, given the exactsame pattern ofmarketdataat
differenttimes, itisquitepossiblethatasubjective analystwould notgive
thesameforecast.41
Paradigm Shift
Refashioning TA into an objective practice would be whatThomas Kuhn
callsa paradigm shift. In his highlyinfluential book, The Structure ofSci
entific Revolutions, Kuhn rejected Popper's notion that science evolves
strictlybyfalsification and conjecture. Instead, Kuhn sawtheevolution of
a scienceasa sequence ofparadigms, orworld views. While agivenpara
digm is in place, practitioners indoctrinated in that point ofview confine
their activities to posing questions and hypotheses that are consistent
withandanswerablewithinthatview.
Alarge number ofTAanalysts have been indoctrinated with the non
scientific, intuitive analysisparadigm developed byTApioneerslike Dow,
Gann, Shabacker, Elliott, Edwards and Magee, and so forth. They estab
lished the subjective research tradition and postulated the background
wisdom that is accepted as true and taught to aspiring practitioners. The



==================================================
                     PAGE 167                     
==================================================

The Scientific Method and TechnicalAnalysis 151
certification exam given by the Market Technicians Association to aspir
ingCharteredMarketTechnicians(CMT) exemplifiesthistradition.
The shift to an evidence-based objective approach would challenge
much ofthis material as eithermeaningless or not sufficiently supported
by statistical evidence. Many of the teachings will go the way of early
Greek physics and astronomy. Some methods will survive objectification
and statistical testing and will warrant a position in a legitimate body of
TA knowledge.
Lestthe reader think my position too harsh, I am notadvocating that
the falsifiability criterion be used to cut off all research in budding TA
methodsjustas they are being formulated. Many ofthe brilliant theories
ofscience began as half-backed prescientific ideas on the wrong side of
the falsifiability criterion. These ideas needed time to develop, and one
day some turned into meaningful science. One example in TA is the new
field ofsocionomics, an outgrowth ofElliott Wave Theory. At the current
time, I regard this newly developing discipline as prescientific, though it
mayhavethepotentialto becomeascience.AccordingtoaconversationI
had with ProfessorJohn Nofsinger, who is working within the field ofso
cionomics, at this time the discipline is notyetable to make testable pre
dictions. This will require the quantification of social mood, the key
determinantofmarketmovementaccordingtosocionomics.
Nascentareasofresearchsuchasthisandothersshouldnotbeshort
circuitedsimplybecausetheyare notatthistimeable togeneratetestable
predictions. Onedaytheymaybeable to doso.
OBJECTIFICATION OF SUBJECTIVE 11\: AN EXAMPLE
One ofthe challenging aspects ofmoving TA to a science will be trans
forming subjective chart patterns into objectively defined testable pat
terns. This section presents an example of how two academic
technicians, Keving Chang and Carol Osler (C&O) objectified the head
and-shoulders pattern).42 Not all elements oftheir pattern are included
here. Rather, I have included enough of their rules and the problems
theyfaced and solved to illustrate the challenges oftransforminga sub
jective method into an objective one. Forfurther details please refer to
theiroriginal articles.
Descriptions ofthe head-and-shoulderspattern can be found in many
43
TA texts and are typically accompanied by a diagram similarto the one
in Figure 3.8. It represents the pattern as a sequence ofnoise-free price
swings. When the pattern manifests in perfect textbook form, even a be
ginningstudentofTAcanspotit.



==================================================
                     PAGE 168                     
==================================================

152 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The problem occurs when an actual chart pattern departs from this
ideal. Even seasoned chartists can debate whether a given pattern quali
fies as a legitimate head and shoulders, an unavoidable consequence of
the lack of an objective pattern definition. Subjective pattern definitions
generally describe how the head-and-shoulders pattern should look, but
they do notprovide clearrules for discriminating patterns thatdo qualify
asheadandshouldersfrompatternsthatpossesssomehead-and-shoulders
like features but do not qualify. In other words, the definitions lack clear
rules for which patterns to exclude. This problem is conceptualized in
Figure3.6.
Without objective rules for deciding what does and what does not
qualifyas a legitimate head-and-shoulders pattern itis impossible to eval
uate the pattern's profitability or predictive power. The solution to the
problem is to define objective rules that discriminate valid head-and
shoulders patterns from those that are not.44 This notion is illustrated in
Figure 3.7. The challenge ofturning a subjectivepattern into an objective
one can be thought of as the problem of defining the pattern as a clear
subsetinthesuper-setofallpossibleTApricepatterns.
C&O defined the head-and-shoulders top pattern as composed offive
pivotorpricereversal pointsthat, intum, define the pattern'sthreepeaks
and two troughs. These pivots are denoted by lettersAthrough E on Fig
ure 3.8. All eight texts consulted by C&O were clear that the head, de
noted by letter C, must be higher than the two surrounding peaks
(shoulders) denoted by letters A and E. There has been some debate
among chartists aboutseveral auxiliaryfeatures includingthe Adam'sap
ple, the double chin, and the cowlick. However, the TAmanualsconsulted
byC&Owereinconsistentontheseaspectsand they were notincluded.
Set ofAll Possible Chart Patterns
FIGURE 3.6 Subjective patterns-no definitive exclusion rules.



==================================================
                     PAGE 169                     
==================================================

The Scientific Methodand TechnicalAnalysis 153
NotH&S
~ NotDB
V
FIGURE 3.7 Objective patterns-definitive exclusion rules.
One challenge thatfaces the real-world chartistis the fact that actual
price oscillations do not trace out clearly identifiable peaks and troughs.
Rather, peaks and troughs occurat numerous oscillation scales from tiny
waves lasting minutes to very large ones lasting years or decades. This
property, called fractal scaling, imposes a burden on the subjective ana
lysttryingtoidentifyahead-and-shoulderspattern. Theanalystmustvisu
allyfllter the price behaviorto isolate peaks and troughs atone particular
scaleofinterest.Thisisrelativelyeasyinretrospect, butitisquitedifficult
asthepatternisactuallyunfolding in real time.
C&O addressed thisproblemby usingapercentagefllter, also known
asanAlexanderfilter45orazigzagindicator, whichisdiscussedbyMerrill
Price Head
C
Neck Line
Cross
Sell Signal
....................Neck Line····""
o
Right
Trough
Time
FIGURE 3.8 Head and shoulders.



==================================================
                     PAGE 170                     
==================================================

154 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
in Filtered Waves.46 It is an objective method for identifying peaks and
troughs as prices evolve. Troughs are identified after prices have moved
up from a recentprice low byan amountgreaterthan a specifiedthresh
old percentage and peaks are identified after prices have moved down
from a recentpricehigh bythethresholdpercentage. Theproblem is that
the identification occurs with a time lag, the time it takes for prices to
move the thresholdamount. For example, ifthe threshold is set at5per
cent, thenapeakisnotdetected untilpriceshavefallen atleast5percent
from the most recentprice maximum, and troughs are notdetected until
prices have risen at least 5 percent from the recent price minimum. The
minimum required price movement causes a lag between the time the
peak or trough actually occurred and the time at which it is detected by
the zigzagfilter.
Next, C&O addressed how to determine the correct threshold per
centage to define the zigzagfilter. Differentfilter thresholds would reveal
head-and-shoulders patterns of different size (scale). For example, a 3
percentfilter might reveal a head-and-shoulders pattern thata 10percent
filter would completely ignore. This makes it possible for multiple head
and-shoulderspatterns ofdifferingscaleto existsimultaneously. C&O ad
dressed this problem by subjecting each financial instrument (stock or
currency) to 10different zigzagfilters employinga range ofthreshold val
ues. This allowed them to identify head-and-shoulders patterns on a vari
etyofscales.
This raisedyetanotherproblem. Whatshould the 10filter thresholds
be? Clearly, a set of thresholds that would be good for one instrument
may not be good for another because theyare characterized by different
levels of volatility. Realizing this, C&O take each instrument's recent
volatility into account to arrive at the set of 10filter thresholds used for
that instrument. This insightallowed their head-and-shoulders algorithm
to generalize across markets with different volatilities. C&O defined a
market's volatility, V, as the standard deviation ofdaily percentage price
changes over the most recent 100 trading days. The 10 thresholds were
arrived at by multiplying Vby 10 different coefficients; 1.5, 2.0, 2.5, 3.0,
3.5, 4.0, 4.5, 5.0, 5.5, 6.0. This resulted in 10zigzagfilters withvaryingsen
sitivity. The validity of the set of coefficients chosen by C&O were con
firmed by visual inspection of price charts by TA practitioners who
agreed that the 10 zigzag filters did a reasonable job ofidentifying head
and-shoulderspatterns.
Next C&O addressed the problem ofdefining rules thatqualify a can
didate pattern as a valid head-and-shoulders. These rules were applied to
the instrument once its price had been zigzag filtered and the peaks and
troughs atagivenscalehad beenidentified.



==================================================
                     PAGE 171                     
==================================================

The Scientific Method and TechnicalAnalysis 155
First,the headandshouldersidentifiedbyC&O'salgorithmhadtosat
isfythefollowingbasicrules:
t. The head of the pattern must be higher than both the left and right
shoulders.
2. The instrument must be in an uptrend prior to the formation ofthe
head-and-shoulderspattern.Thus, thepattern'sleftshoulderhastobe
abovethepriorpeak(PP)and thepattern'slefttroughhastobeabove
the priortrough (PT).
NextC&O grappled withmore subtle and complexissuesto qualifya
candidate pattern as a valid head-and-shoulders. They accomplished this
with a set ofinnovative measurements thatqualified the pattern in terms
ofitsverticaland horizontalsymmetryandthe timeittookforthepattern
to complete. These rules allowed them to definitively label a candidate
patternaseitherhead-and-shoulders ora non-head-and-shoulders.
Vertical Symmetry Rules
The vertical symmetry rules exclude patterns with necklines that are too
steeplysloped. ThepatterninFigure3.9hasacceptableverticalsymmetry.
The rules compare the price levels ofthe right and left shoulders (A
and E) and the price levels ofthe right and lefttroughs (B and D) witha
price level defined by the midpoint ofsegment AB, designated as point
X, and the midpointofsegmentDE designatedas pointY. Toqualifyas a
Price
c
E>X&D<X
r---~+
A>Y&B<Y Time
FIGURE 3.9 Good vertical symmetry.



==================================================
                     PAGE 172                     
==================================================

156 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Price
Time
E'IGURE 3.10 Poorvertical symmetry-neckline slope too steep.
vertically symmetrical head-and-shoulders, the pattern must satisfy the
following rules.
t. The price level of the left shoulder peak, point A, must exceed the
pricelevelofpointY.
2. The price level ofthe right shoulder peak, point E, must exceed the
pricelevelofpointX.
3. The price level ofthe lefttrough, pointB, must be less than the price
level ofpointY.
4. Thepriceleveloftherighttrough, pointD, mustbelessthantheprice
levelofpointX.
Figures 3.10 and 3.11 show two head-and-shoulders patterns that
would be excluded because the vertical symmetry criteria have not been
satisfied.
Horizontal Symmetry Rule
Another feature used by C&O to distinguish head-and-shoulders patterns
from non-head-and-shoulders patterns was horizontal symmetry. A pat
tern with good horizontalsymmetryis one forwhich the head, atpointC,
isroughlyequidistantfrom thepeaksrepresentingthepattern'stwoshoul
ders (points Aand E). C&O's rule was that the distance from the head to



==================================================
                     PAGE 173                     
==================================================

The Scientific Method and TechnicalAnalysis 157
c
Price
Time
FIGURE 3.11 Poorvertical symmetry-neckline slope too steep.
oneshouldershouldnotbegreaterthan2.5timesthe distanceofthehead
to the othershoulder. There is nothing magical about the value 2.5 other
thanitseemedreasonable. See Figure3.12.
Figure3.13isan example ofa pattern thatfails the testfor horizontal
symmetry. Note the right shoulder is stretched too far to the right. Apat
ternwithexcessiveleftwardstretchwouldalso bedisqualified.
Price
Horizontal
Symmetry
Rule Time
FIG RE 3.12 Good horizontal symmetry.



==================================================
                     PAGE 174                     
==================================================

158 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Price
FIGURE 3.J3 Poor horizontal symmetry.
Pattern Completion Rule: Maximum Time
to Neckline Penetration
C&O alsospecifieda rule thatexcludespatternsthattaketoo longtopen
etrate the neckline once the rightshoulder, point E, has been formed. As
withotherfeatures, this criterionisdefinedinterms ofthepattern'sinter
nal proportions rather than some fixed numberoftime units. This allows
the rule to be applied to all patterns irrespective of their time frame or
scale.
The maximum time allowed for the movement from the right shoul
der, pointE, until the penetration ofthe necklineis the temporal distance
separating the two shoulders, points A and E. The pattern in Figure 3.14
meets the time to completion criterion because the temporal distance
from the right shoulder until neckline penetration (D4) is less than the
temporalseparationoftheshoulders(D3). ThepatterninFigure3.15does
notqualifybecauseD4exceedsD3.
Future Information Leakage: Look-Ahead Bias
Intheirsimulationofhead-and-shoulderspatterns, C&Otookprecautions
against the future information leakage or look-ahead bias problem. This
problem afflicts backtests thatassume the possession ofknowledge that
was not truly available whena trading decision was made. In the context
ofbacktesting, this can make results appear more profitable than would
be possible inactual trading. An extreme example would be assumingac
cessto the Wall StreetJournalthe daybeforeitspublication.



==================================================
                     PAGE 175                     
==================================================

The Scientific Methodand TechnicalAnalysis :159
Price Completion
c
Rule
:
:
Time
FIGUR"'~ 3.14 Pattern completion rule satisfied.
In the context of back testing the head-and-shoulders pattern, future
information leakage can occurifthe zigzagthreshold percentage is larger
than the percentage distance between the right shoulder peak and the
neckline. Itwould not be legitimate to assume a short-salesignal due to a
necklinepenetration untilthe rightshoulder(point E) has been detected.
However, it is possible for prices to cross the neckline before the right
shoulder is identified by the zigzag filter. To clarify, suppose the right
shoulder, pointE,liesonly4percentabovetheneckline, butthethreshold
Price
,4 •.4
Time
FIGURE 3.15 Pattern completion rule not satisfied.



==================================================
                     PAGE 176                     
==================================================

.60 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
forthe zigzagfilteris 10percent.Itwouldnotbelegitimate to assume that
a trader lrnew about the neckline penetration because a 10 percent de
cline from the right shoulder would have been required to identify the
rightshoulderofthe head-and-shoulders pattern. In this example, assum
ingan entry price only4percentbelow the rightshoulderwould be more
favorable than waitingfora price 10percentbelowthe rightshoulder, the
price movement required to have a fully formed head-and-shoulders pat
tern. To avoid this problem, C&O assumed an entry after the right shoul
der had been objectively identified by the zigzag filter. Though this was a
less favorable entry price in some instances, the back test was unencum
beredbylook-aheadbias. Imention thisbecauseitshowsC&O'sattention
to detail.
C&O's pattern definition also deals with numerous other issues re
garding entry and exit ofpositions, stop-loss levels, and so forth, but the
pointhas been made that it is possible to transform subjective chartpat
terns intoobjectivetestablepatterns. Readersmay takeexceptionto arbi
traryaspectsofC&O'spattern.Allwelland goodifthe readerhasa better
objectivedefinitionto offer.
As afinal sanitycheck, C&Oshowed patternsthathad beenidentified
by their automated head-and-shoulders algorithm to a number of
chartists. C&O claim that the chartists agreed that the patterns identified
bythe objectivehead-and-shouldersalgorithmdid indeedconformtosub
jectivehead-and-shoulderscriteria.
Head-and-Shoulders Back Test Results
Doesthe head-and-shoulderspatterncarrypredictiveinformationwith re
spect to stocks or currencies? In a word, the pattern hailed as a corner
stone of charting is a bust. Tests by C&O show that it is worthless on
stocks and only modestly profitable on currencies. The pattern was prof
itable in two out ofsix currencies tested, but the relatively complicated
head-and-shoulders algorithm was far outperformed by a much simpler
objective signal based on zigzag filters. Moreover, when C&O tested the
occurrence ofa head-and-shouldersin conjunctionwiththe zigzagrule ei
ther as a confirming or disconfirming signal, the head-and-shoulders pat
ternadded novalue. Inotherwords, zigzagsignals did no betterwhenthe
head-and-shoulders signal was in the same direction as the zigzag signal
and the zigzag did no worse when the head-and-shoulders signal was in
the opposite direction.The bottom line for currency traders: the value of
head-and-shouldersisdoubtful.47
The head-and-shoulders perforn1ed worse on stocks. C&O evaluated
the head-and-shoulders on 100 randomly selected equities48 over the pe-



==================================================
                     PAGE 177                     
==================================================

The Scientific Method and TechnicalAnalysis 161
riodfromJuly 1962untilDecember1993. Onaverage, eachstockgaveone
head-and-shoulders signalperyear, counting both long and shortsignals,
giving a sample ofover3,100 signals. To test the pattern'sprofitability on
actual stock prices, C&O established a benchmarkbased on the pattern's
performance on pseudo-price histories. These simulated price histories
were generated from actual historical price changes strung together in a
random fashion. Byusingactualprice changes, the pseudo-pricehistories
hadthe samestatisticalcharacteristics as realstocks, butanypredictabil
ity due to authentic temporalstructure-the structure TApatterns are in
tended to exploit-was eliminated Despite the fact that the pseudo-price
histories were randomly generated, head-and-shoulders patterns that fit
C&O's definition still emerged. This confirmsthe results ofHarryRoberts
referred to earlier.
If the head-and-shoulders patterns appearing in real stock data are
useful, theyshould generate profits superiorto those achieved bytrading
the patterns appearing in the fake stock price histories. C&O found that
head-and-shoulders patterns in actual stock prices lost slightly more
money than the signals on pseudo-price histories. According to the study
"the results uniformly suggest that head-and-shoulders trading are not
profitable." The signals lose on average about 0.25 percent over a lO-day
holding period. This compares with an average loss of 0.03 percent for
head-and-shoulders patterns in the pseudo-stock data. C&O referred to
traders using the pattern as "noise traders," speculators who mistake a
randomsignalforaninformativeone.
Confirming C&O's findings is the work ofLo et a1.49 Lo used an alter
nativemethodofobjectifyingthe H&Spatternbased onkernel regression,
asophisticated10ca150smoothingtechnique. Theirstudywasunable toun
seatthe nullhypothesis thatthe head-and-shoulderspatternis useless.51
Bulkowski52 found the head-and-shoulders was profitable, but his re
searchfalls short. He does notprovidean objectivepattern definitionthat
back-testable pattern or entry and exit rules. In otherwords, his study is
subjectiveTA. Inaddition, hisresultsfailtoadjustforthetrendofthegen
eralstockmarketoverthe timeperiodinwhichhe testedthepatterns.
SUBSETS OF 11\
Given theprecedingdiscussion, TAcan beseenas comprisedoffour sub
sets: (1) subjective TA, (2) objective TA with unknown statistical signifi
cance, (3) objective TA that is not statistically significant, and (4)
objectiveTAthatisstatisticallysignificant.



==================================================
                     PAGE 178                     
==================================================

162 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The first subset, subjective TA, has already been defined as methods
that cannot be reduced to a back-testable algorithm. Subsets two, three,
andfourreferto objectivemethods.
The second subset is comprised of objective methods of unknown
value. Though these methods are objective and may have been back
tested, their results have not been evaluated for statistical significance.
Here, itis simplya matter ofapplying the methods discussed in Chapters
4, 5, and 6. This is not meant to suggest that applying statistical methods
tobacktestresultsissimple, butratherthatthe decision to dosois.
The third subset, which I refer to as useless TA, consists ofobjective
TA rules for which results have been comprehensively back tested and
evaluatedwithstatistical methods, buthave been reveled to add no value
either on a stand-alone basis or when used in combination with other
methods. In all likelihood, the majority ofobjective TA methods will fall
into this subset. This is to be expected because financial markets are ex
tremely difficult to predict due to their inherent complexity and random
ness. In fact, in all fields of science, most proposed ideas do not work.
Importantdiscoveriesare rare. Thisis notobvious becausethe numerous
failures are typically not reported in the lay press or even in scientific
journals. Whatismostimportantishavingaprocedureinplaceforgetting
rid ofmethods thatdo notwork.
The fourth subset, useful TA, consists ofobjective methods that pro
duce statistically significant or, better yet, economically significant re
sults. Though some rules will be useful on a stand-alone basis, the
complexity and randomness offinancial markets make it likelythat most
~
NO---+ Subjective
YES
Objective
RIGOROUS
NO---+ Value
EVALUATION
Unknown
.................................EBTA EBTA
YES
NO---+
FIGURE 3.16 Subsets ofTechnical Analysis.



==================================================
                     PAGE 179                     
==================================================

The Scientific Method and TechnicalAnalysis 163
rules will add value when used in combination with other rules to form
complexrules.
Evidence-based technical analysis (EBTA) refers to subsets (3) and
(4)-objective TA that has been back tested and subjected to statistical
analysis. Giventhe precedingdiscussion, the categorization ofTAisillus
tratedinFigure3.16.
The next three chapters discuss the application ofstatistical analysis
toback-testresults.



==================================================
                     PAGE 180                     
==================================================





==================================================
                     PAGE 181                     
==================================================

Statistical
Analysis
Statistics is the science of data.1 In the late nineteenth century,
renowned British scientist and author H.G. Wells (1866-1946) said
that an intelligent citizen in a twentieth-century free society would
need to understand statistical methods. It can be said that an intelligent
twenty-fIrst-century practitioner or consumer of TA has the same need.
Thischapterand thenexttwo addressaspectsofstatisticsthatarepartic
ularly relevanttoTA.
Statistical methods are not needed when a body of data conveys a
message loudlyand clearly. Ifall people drinkingfrom a certainwell die
of cholera but all those drinking from a different well remain healthy,
there is no uncertainty aboutwhich well is infected and no need for sta
tistical analysis. However, when the implications ofdata are uncertain,
statistical analysis is the best, perhaps the only, way to draw reasonable
conclusions.
Identifying which TA methods have genuine predictive power is
highly uncertain. Even the most potent rules display highly variable per
formance from one data set to the next. Therefore, statistical analysis is
the only practical way to distinguish methods that are useful from those
thatarenot.
Whether ornot its practitioners acknowledge it, the essence ofTAis
statistical inference. It attempts to discover generalizations from histori
cal data in the form ofpatterns, rules, and so forth and then extrapolate
them to the future. Extrapolation is inherently uncertain. Uncertainty is
uncomfortable.
The discomfort can be dealt with in two ways. One wayis to pretend
165



==================================================
                     PAGE 182                     
==================================================

166 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
it does not exist. The other is the way of statistics, which meets uncer
tainty head on by acknowledging it, quantifying it, and then making the
best decision possible in the face of it. Bertrand Russell, the renowned
Britishmathematicianandphilosophersaid, "Uncertainty, in thepresence
ofvivid hopesandfears, ispainful, butmustbe enduredifwe wish to live
withoutthesupportofcomfortingfairy tales."2
Many people are distrustful or disdainful of statistical analysis and
statisticians are often portrayed as nerdy number-crunching geeks di
vorcedfrom reality. Thisshowsup injokes. Wederidewhatwedo notun
derstand. There is the story about the six-foot-tall man who drowns in a
pond with an average depth ofonly two feet. There's the tale about three
statisticians who go duck hunting. They spot a bird flying overhead. The
first shoots a foot too far to the left. The second shoots a foot too far to
the right. The thirdjumps up and exclaims, "We gotit!!" Even though the
average errorwaszero, there was no duckfordinner.
Powerful tools can be put to bad purpose. Critics often charge that
statistics are used to distort and deceive. Ofcourse, similar ends can be
achievedwithwords, althoughlanguageis notheldliable.Amore rational
stance is needed. Rather than viewing all claims based on statistics with
suspicionortakingthemallatface value, "amore matureresponsewould
be to learn enough about statistics to distinguish honest, useful conclu
sionsfrom skullduggeryorfoolishness."3 "Hewhoacceptsstatisticsindis
criminately will often be duped unnecessarily. But, he who distrusts
statisticsindiscriminatelywilloftenbeignorantunnecessarily.Themiddle
ground we seek between blind distrust and blind gullibility is an open
mindedskepticism. Thattakesanability to interpretdataskillfully."4
A PREVIEW OF S'IJ\TISTICAL REf\SONING
Statistical reasoning is new terrain for many practitionersand consumers
ofTA. Trips to strange places are easier when you know what to expect.
Thefollowing isa previewofthe nextthree chapters
For reasons discussed in Chapter 3, it is wise to start with the as
sumption that all TA rules are without predictive power and that a prof
itable back test was due to luck. This assumption is called the null
hypothesis. Luck, in this case, means a favorable but accidental corre
spondencebetweenthe rule'ssignalsandsubsequentmarkettrendsinthe
historical datasample in which the rule was tested. Although this hypoth
esis is a reasonable starting point, it is open to refutation with empirical
evidence. In other words, ifobservations contradictpredictions made by
the null hypothesis, it is abandoned and the alternative hypothesis, that



==================================================
                     PAGE 183                     
==================================================

StatisticalAnalysis 167
the rule has predictive power, would be adopted. In the context of rule
testing, evidence that would refute the null hypothesis is a back-tested
rateofreturnthatis toohightobereasonablyattributedto mereluck.
IfaTArulehasnopredictivepower, itsexpectedrateofreturnwillbe
zeroondetrended5data. However,overanysmallsampleofdata, theprof
itability ofa rule with no predictivepowercan deviate considerablyfrom
zero. These deviations are manifestations of chance-good or bad luck.
This phenomenon can be seen in a coin-toss experiment. Over a small
numberoftosses, the proportion ofheads can deviate considerablyfrom
0.50, which is the expectedproportion ofheads in averylarge numberof
tosses.
Generally, the chance deviations ofa useless rule from a zero return
are small. Sometimes, however, a useless rule will generate significant
profitsbysheerluck. Theserare instancescanfool usintobelievingause
less rule haspredictivepower.
The best protection against being fooled is to understand the degree
to which profits can result from luck. This is best accomplished with a
mathematical function thatspecifies the deviations from zero profits that
canoccurbychance. Thatiswhatstatisticscandoforus.
Thisfunction, calledaprobabilitydensityfunction, givestheprobabil
ity of every possible positive or negative deviation from zero. In other
words, it shows the degree to which chance can cause a useless rule to
generate profits. Figure4.1 showsaprobabilitydensityfunction.6Thefact
that the density curve is centered at a value of zero reflects the null hy
pothesisassertionthatthe rulehasanexpectedreturn ofzero.
Probability
Density
o
Negative Returns Positive Returns
Back-Tested Rate of Return
FIGURE 4.1 Probability density ofchance performance-range of possible per
formance forauselessTA rule.



==================================================
                     PAGE 184                     
==================================================

168 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
InFigure4.2, the arrow indicates the positive rate ofreturn earned by
a rule when it was back tested. This raises the question: Is the observed
rate ofreturnsufficientlypositivetowarranta rejectionofthe nullhypoth
esisthatthe rule's true rate ofreturn iszero?Ifthe observed performance
falls well within the range ofthe deviations thatare probably attributable
to chance, the evidence is considered to be insufficient to reject the null
hypothesis. Insuchacase, thenullhypothesishaswithstood the empirical
challenge of the back-test evidence, and a conservative interpretation of
theevidencewouldsuggestthatthe rulehasnopredictivepower.
The strength ofthe back-test evidence is quantified by the fractional
area7 of the probability density function that lies at values equal to or
greaterthan the rule's observed performance. This portion ofthe density
function isdepictedbythe darkened areato the rightoftheverticalarrow
in Figure 4.2. The size of this area can be interpreted as the probability
thata rateofreturnthishighorhighercouldhaveoccurred bychance un
derthe conditionthatthe rulehasnopredictivepower(expectedreturn =
0, orthenullhypothesisistrue). Whenthisareaoccupiesarelativelylarge
fraction ofthe density curve, it means that there is an equivalently large
probabilitythat the positiveperformance was due to chance. When this is
the case, there is nojustificationfor concludingthatthe null hypothesisis
false. In otherwords, there is nojustification for concluding that the rule
does havepredictivepower.
However, ifthe observed performance is far above zero, the portion
ofthe probability density function lying at even more extreme values is
Rate ofreturn
this low falls well
Probability
within the range of
Density
chance performance
o
Negative Returns Positive Returns
Back-Tested Rate ofReturn
FIGURE 4.2 Probabilityofchance performance forauseless rule.



==================================================
                     PAGE 185                     
==================================================

StatisticalAnalysis 169
small. Performancethispositivewould be inconsistentwith the assertion
thatthe rule has no predictivepower. Inotherwords, the evidence would
be sufficientto refute the null hypothesis. Anotherway to think ofthis is
as follows: If the null hypothesis were true, a level of performance this
positive would have a low probability of occurrence. This probability is
quantified by the proportion of the density function that lies at values
equal to or greater than the observed performance. This is illustrated in
Figure 4.3. Note that the observed performance lies in the extreme right
tail ofthe density curve thatwouldpertainifthe rule were devoid ofpre
dictivepower.
It is important to understand what this evidence does not tell us. It
tellsusnothingabouttheprobabilitythateitherthe nullhypothesisorthe
alternativehypothesisis true. Itonlyspeakstothe probabilitythatthe ev
idencecouldhaveoccurredundertheassumptionthatthenullhypothesis
is, in fact, true. Thus, the probability speaks to the likelihood of the evi
dence, not the likelihood of the truth of the hypothesis. Observed evi
dence that would be highly improbable, under the condition that the null
hypothesisis true, permitsaninferencethatthe nullhypothesisisfalse.
Recall that, in Chapter 3, it was shown that evidence that a creature
has four legs cannot conclusively establish the truth of the hypothesis:
The creatureisa dog. Althoughevidenceoffourlegswouldbeconsistent
with the hypothesis thatthe creature is a dog, itis notsufficientto prove,
deductively, that the creature is a dog. Similarly, while the observation of
positive performance would be consistentwith the hypothesis that a rule
haspredictivepower, itisnotsufficienttoprovethatitdoes. Anargument
that attempts to prove the truth of a hypothesis with observed evidence
Probability
Density Rate ofreturn
this high
unlikelyto
be due to chance
o
Negative Returns Positive Returns
Back-Tested Rate of Return
FIGURE 4.3 Probabilityofchance performance for agood rule.



==================================================
                     PAGE 186                     
==================================================

.70 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
thatisconsistentwiththehypothesiscommitsthelogicalfallacyofaffirm
ingthe consequent.
Ifthe creatureis a dog, then ithasfour legs.
The creaturehasfour legs.
Invalid Conclusion: Therefore, the creatureis adog.
Ifarulehaspredictivepower, then itwillhaveaprofitableback test.
Back test wasprofitable.
Invalid Conclusion: Therefore, rule haspredictivepower.
However, the absence offour legs is sufficient to prove that the hy
pothesis, thecreatureisa dog, isfalse.sInotherwords, observedevidence
can be used to conclusivelyprove thata hypothesis is false. Suchan argu
mentuses the valid deductive form, denial ofthe consequent. Thegeneral
form ofanargument, inwhichtheconsequentisdenied, isasshown:
IfPis true, then Q is true.
Qis not true.
Valid Conclusion: Therefore, Pis not true (i.e., Pisfalse).
Ifthe creatureisa dog, then ithasfourlegs.
Creaturedoes nothavefour legs.
Valid Conclusion: Therefore, itisfalse that the creatureisa dog.
The argumentjustgiven uses the evidence, the absence offour legs,
to conclusivelyfalsify the notion that the creature is a dog. However, this
level ofcertitude is notpossible in matters ofscience and statistics. One
can never conclusively falsify a hypothesis. Nevertheless, a similar logic
can be used to showthatcertain evidence is highlyunlikelyifthe hypoth
esiswere true. Inotherwords, the evidence gives us groundsto challenge
the hypothesis. Thus, a highly profitable back test can be used to chal
lenge the hypothesis thatthe rule has nopredictivepower(i.e., thatithas
anexpectedreturn ofzero).
Ifa rule's expected return is equal to zero or less, then a back test
shouldgenerateprofits that arereasonably close to zero.
The back-tested performance was not reasonably close to zero; in
fact, itwas significantlyabovezero.
Valid Conclusion: Therefore, the contention that the rule's expected
return is equal to zero orless islikely to befalse.



==================================================
                     PAGE 187                     
==================================================

StatisticalAnalysis 171
How unlikely or rare must the positive performance be to reject the
notion that the rule is devoid ofpredictive power? There is no hard and
fast rule. By convention most scientists would not be willing to reject a
hypothesis so unless the observed performance has a 0.05 probability or
less of occurrence under the assumption the null is true. This value is
calledthestatisticalsignificance oftheobservation.
The discussionsofarpertainsto the case where onlyonerule is back
tested. Inpractice, however, TA rule research is typicallynotrestricted to
testing a single rule. Economical computingpower, versatile back-testing
software, and plentifulhistorical datamakeiteasy, almostinviting, to test
many rules with the aim if selecting the one with the best performance.
Thispracticeis known as datamining.
Although data mining is an effective research method, testing many
rules increasesthe chance ofa luckyperformance. Therefore, the thresh
old of performance needed to reject the null hypothesis must be set
higher, perhaps much higher. This higher threshold compensates for the
greater likelihood of stumbling upon a useless rule that got lucky in a
backtest. Thistopic, the dataminingbias, isdiscussed inChapter6.
Figure 4.4 compares two probability density functions. The top one
would be appropriateforevaluatingthe significancefor asingle rule back
test. The lowerdensity curve would be appropriatefor evaluatingthe sig-
Appropriate Distribution
for Single-Rule Test
Probability
Observed Performance
Density
Looks Significant
o
Appropriate Distribution for Bestof 1000 Rules
Observed Performance
Probability Does Not Look
Density Significant
o
Back-Tested Rate ofReturn
FIGURE 4.4 Great performance in asingle rule back test is only mediocre when
1,000rules are tested.



==================================================
                     PAGE 188                     
==================================================

172 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
nificance ofthe best-performing rule out of 1,000 back-tested rules. This
density curve takes into account the increased likelihood ofluck that re
sults from data mining. Notice that if this best rule's observed perfor
mance were to be evaluated with the density curve appropriate for a
singlerule, itwould appearsignificantbecausethe performance isfar out
in the right tail ofthe distribution. However, when the best rule's perfor
mance is evaluated with the appropriate probability density function it
does not appear statistically significant. That is to say, the rule's rather
high performance would notwarrant the conclusion that ithas predictive
poweroranexpectedreturn thatisgreaterthan zero.
THE NEED FOR RIGOROUS sml'ISTICAL ANALYSIS
The tools and methods ofa discipline limitwhatit can discover. Improve
mentsinthempavethe wayto greaterknowledge. Astronomytookagreat
leap forward with the invention ofthe telescope. Though crude by today's
standards,theearliestinstrumentshad 10timesthe resolvingpowerofthe
unaided eye. Technical analysis has a similar opportunity, but it must re
placeinformaldataanalysiswithrigorousstatisticalmethods.
Informal dataanalysis is simply not up to the task ofextractingvalid
knowledge from financial markets. The data blossoms with illusory pat
terns whereasvalidpatternsare veiled bynoiseand complexity. Rigorous
statisticalanalysisisfarbettersuitedto this difficulttask.
Statistical analysis is a set ofwell-defined procedures for the collec
tion, analysis, and interpretation of data. This chapter and the next two
willintroducethe waystatisticaltools and reasoning canbe used to iden
tify TA rules that work. This overview is necessarily condensed, and in
manyinstancesIhavesacrificedmathematicalrigorforthesakeofclarity.
However, these departures do notdilute the essential message: IfTAis to
deliver on its claims, it must be grounded in a scientific approach that
usesformal statisticalanalysis.
AN EXAMPLE OF SAMPLINGAND
STATISTICAL INFERENCE
Statistical reasoning is abstract and often runs against the grain of com
mon sense. This is good and bad. Logic that runs counter to informal in
ference is good because it can help us where ordinary thinking lets us
down. However, this is exactly what makes it difficult to understand. So
weshouldstartwitha concrete example.



==================================================
                     PAGE 189                     
==================================================

StatisticalAnalysis .73
Thecentral conceptofstatisticalinferenceisextrapolatingfrom sam
ples. Asample ofobservations is studied, a pattern is discerned, and this
pattern is expected to hold for (extrapolated to) cases outside the ob
served sample. Forexample, a rule observed to be profitable in a sample
ofhistoryisprojectedto beprofitableinthefuture.
Let'sbeginto thinkabouttheconceptinthecontextofaproblemthat
has nothing to do with technical analysis. It comes from an excellent
book: Statistics, A New Approach by Wallis and Roberts.9 The problem
concerns a box filled with a mixture ofwhite and grey beads. The total
numberofbeads and the numbers ofgreyand white beads are unknown.
The task is to determine the fraction of beads that are grey in the entire
box. For purposes of brevity, this value will be designated as F-G (frac
tion-greyinbox).
To make thissituationsimilartostatisticalproblemsfaced in the real
world, there is a wrinkle. We are not allowed to view the entire contents
ofthe box at one time, thus preventing a direct observation ofF-G. This
constraint makes the problem realistic because, in actual problems, ob
serving all the items ofinterest, such as all beads in the box, is eitherim
possible or impractical. In fact, it is this constraint that creates the need
forstatistical inference.
Although we are not allowed to examine the box's contents in its en
tirety, wearepermittedtotakesamplesof20beadsatatimefrom the box
and observethem. Soourstrategyfor acquiring knowledge aboutF-Gwill
be to observe the fraction ofgrey beads in a multitude ofsamples. In this
example 50 samples will be taken. The lowercase f-g stands for the frac
tionofgreybeadsinasample.
A sample is obtained as follows: The bottom of the box contains a
slidingpanelwith 20smalldepressions, sizedsothatasinglebeadiscap
tured ineach depression. Thepanel can beslid outofthe boxbypushing
it with a similar panel that takes its place. This keeps the remaining
beads from dropping out of the bottom. Consequently, each time the
panel is removed, we obtain a sample of20 beads and have the opportu
nityto observethe fraction grey (f-g) in thatsample. This isillustrated in
Figure4.5.
Afterthefraction ofgreybeads(f-g)inagivensamplehasbeendeter
mined, the sample beads are placed back into the box and it is given a
thorough shaking before taking another sample of 20. This gives each
bead an equal chance ofwinding up in the next sample of20. In the par
lance ofstatistics, we are making sure that each sample is random. The
entire process oftakinga sample, noting the value f-g, puttingthe sample
back in the box, and shaking the box is repeated 50 times. At the end of
the whole procedurewe end up with 50 differentvaluesfor f-g, onevalue
foreachsample examined.



==================================================
                     PAGE 190                     
==================================================

174 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
What Fraction
Are
Gray?
F-G Sliding Panel
FIGURE 4.5 Determining f-g for each sample.
By placing each sample back in the box before taking another sam
ple, we are maintaining a stable concentration for the fraction of grey
beads in the box. That is to say, the value for F-G is kept constant over
the course ofthe 50 samplings. Problems in which the statistical charac
teristics remain stable over time are said to be stationary. Ifthe beads
werenotreplacedaftereachsamplethevalue F-Gwouldchangeoverthe
course of the 50 samplings, as groups of beads were permanently re
moved from the box. A problem in which the statistical characteristics
change over time is said to be nonstationary. Financial markets may in
deed be nonstationary, but for pedagogical purposes, the box-of-beads
problemis designedto bestationary.
Itis importantto keep in mind the distinction between F-G and f-g.
F-Grefers to the fraction ofgreybeads inthe entire box. Inthe language
ofstatistics, all the observations in which we are interested are called a
population. In thisexample, the termpopulationrefers to the colorofall
the beads in the box. The term sample refers to a subset ofthe popula
tion. Thus, F-G refers to the population while f-g refers to the sample.
Our assigned task is to gain as much knowledge as possible about the
value ofF-G byobservingthe valuef-g over50separate samples.
It is also important to keep clear the distinction between two num
bers: the number of observations comprising a sample-in this case 20
beads-andthe numberofsamplestaken-inthis case50.
PROBABILI1Y EXPERIMENTS AND RANDOM VARIABLES
Probabilityis the mathematics ofchance. LOAprobability experimentis
an observation on ora manipulation ofourenvironmentthathas an un
certain outcomeY This would include actions such as noting the pre-



==================================================
                     PAGE 191                     
==================================================

StatisticalAnalysis 175
cise time ofarrival after a long trip, observing the number ofinches of
snow that falls in a storm or the face of a coin that appears after the
tossing ofa coin.
The quantity orquality observed in a probability experimentis called
a random variable such as the face of a coin after a flip, the number of
inches ofsnowthatfell, or a value thatsummarizes a sample ofobserva
tions(e.g., asampleaverage).Thisquantityorqualityissaidtoberandom
because it is affected by chance. Whereas, an individual observation ofa
random variable is unpredictable, by definition, a large number ofobser
vations made on a randomvariable may have highlypredictable features.
Forexample, on a givencointoss, itisimpossible topredicthead ortails.
However, given one thousand tosses, that the number of heads will lie
withinaspecified rangeof500is highlypredictable.
Depending on how it is defined, a random variable assumes at least
two differentvalues, thoughitcanassumemore-perhapsevenaninfinite
number. The randomvariableina cointoss, theface visibleafterthetoss,
canassume twopossiblevalues (headsortails). The randomvariable, de
fined as the temperatureatnoontakenatthebaseoftheStatueofLiberty,
can assume a very large number ofvalues, with the number limited only
bytheprecisionofthe thermometer.
Sampling: The Most Important
Probability Experiment
The most important probability experiment in statistical analysis is sam
pling. Itinvolvestheextraction ofasubsetofobservationsfrom a popula
tion. Here the randomvariableatissueiscalledasamplestatistic.Itisany
computable characteristic ofthe sample. The value f-g is an example ofa
samplestatistic.
Sampling can be done in a number ofdifferent ways, but it is crucial
that the observations thatare selected for the sample be chosen in a ran
dom manner and that they be selected independently ofeach other. This
means thatallobservationsthatcouldpossiblyend up inthesamplehave
an equalchance ofdoingso. Because no particularobservationhas a bet
ter chance of appearing in the sample than any other, the observations
thatwind up inthesample dosobychance. Samplesmustbeconstructed
in this manner because the principles ofprobability, upon which statisti
calreasoning rests, assume thatthe observationsthatwind up inthesam
plegotthere randomly.
Consider the test of a new medicine on a sample ofvolunteers. The
samplemustbeconstructedsothatanyonewhomighteventuallytake the
medicine has an equal chance ofbeing selected for the test group. Ifthe
subjectsintheexperimentare notselectedthisway, theresultsofthetest



==================================================
                     PAGE 192                     
==================================================

176 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
willnotleadtoasoundconclusion.Amedicalexperimentthatselectstest
subjects who are expected to respond favorably to the medication will
produce a biased conclusions. In other words, the estimates ofthe med
ication'sefficacyinthegeneralpopulationwill betoo optimistic.
Imagine taking a single bead at random from the box with your eyes
closed and then observing its color. This is a sampling experimentwith a
samplesizeofone. The bead'scolor, greyorwhite, istherandomvariable.
Nowimagine randomlyselecting 20 beads from the box. Here the sample
sizeis20and thefraction greyinthatsample (f-g) isasamplestatisticthat
isalsoa randomvariable.
As discussed earlier, we will call this random variable f-g. It is a ran
domvariable becauseitsvalueisaffected bychance.Itcanassume 21 dif
ferent values {O, 0.05, 0.10, 0.15, ... , 1.01. Now let's get back to the goal:
increasingourknowledge aboutthe valueofF-G.
The Knowledge Gleaned from One Sample
Supposethefirstsanlplecontains 13greybeadsoutof20. Thevalueofthe
sample statistic f-g is 0.65 (13/20). Whatdoes this tell us about F-G? With
only this information, some might be tempted to conclude that we have
solved the problem, and that 0.65 is the fraction grey for the entire box
(i.e., F-G = 0.65). This view tacitly and naively assumes thata single sam
ple can provide perfect knowledge. At the opposite extreme are those
whojust as wrongly conclude that the sample is too small to supply any
information.
Both conclusionsare wrong. First, there is no basisfor thinking that
we could acquire perfect knowledge from one sample. Though it is pos
sible that a single sample of 20 beads might be a perfect replica of the
entire contents ofthe box, it is not likely. For example, ifthe true value
ofF-G were 0.568, a 20-bead sample could never produce such a value.
An f-g value of0.55 (11 grey of20) is as close as one could get. Ten grey
beads in the sample would give anf-g of0.50, 11 would give 0.55, and 12
wouldgive 0.60.
Those who assert that nothing has been learned are also mistaken.
On the basis ofthis sample alone, two possibilities can be eliminated.
We canrejectwith absolute certaintythatF-G = 1.0 (i.e., all beadsgrey)
because there were 7 white beads in the sample. With equal certainty,
we can reject the proposition that F-G is 0, because there were 13 grey
beads in the sample.
However, moreprecise estimates ofF-G onthe basisofthesingle ob
servedvalue off-g are subjectto uncertainty. Evenifwe were to take nu
merous samples and have numerous values off-g, the value ofF-Gwould
remain uncertain. This is because a sanlple is, by definition, a partial rep-



==================================================
                     PAGE 193                     
==================================================

StatisticalAnalysis 177
resentation ofthe full contents ofthe box. Only by observing every bead
in the population could all uncertainty about F-G be eliminated. And this
isprohibited.
However, with what has been learned from the single sample, some
intelligentguessescanbemadethatgobeyondthe certainknowledgethat
F-G is neither0nor 1.0. For example, although a claim that F-G is as low
as 0.10 cannot be conclusively ruled out; no one would take it seriously.
The observed f-g value of 0.65 obtained from the first sample is too far
above0.10forittobe a credible estimateofF-G. IfF-Greallywereaslow
0.10, theprobabilityofgettingasamplewithanf-g of0.65wouldseemun
likelyjustbasedoncommonsense. Wecouldapplythesame kind oflogic
to dispensewithaclaimthatF-G = 0.95.Anf-greadingof0.65wouldseem
too low if the entire box were composed of 95 percent grey beads. The
bottomline: Itisfairtosaythatasinglesampledoesprovidesomeknowl
edgeaboutF-G.
What Can Be Learned from 50 Samples?
GreaterknowledgeaboutF-Gcanbeobtainedbyanalyzingmoresamples.
Assumeanother49samplesare takenand the valuef-g for eachsampleis
measured. The first thing we would notice is that f-g varies in an unpre
dictable fashion from sample to sample. This particular form ofrandom
behavior is one ofthe most importantphenomenain all ofstatistics. Itis
calledsamplingvariabilityorsamplingvariation.
Sampling variation is important because it is responsible for the un
certainty instatistical conclusions. Greatersamplingvariabilitytranslates
to greateruncertainty. Thegreaterthefluctuations inf-g from onesanlple
to the next, thegreaterwillbethe uncertaintyaboutthevalueofF-G.
. Regrettably, this importantphenomenon, samplingvariability, is unfa
miliar to many people who analyze data. This is understandable because
real world problems do not offer the luxury oflooking at more than one
independentsample. The box-of-beadsprovidesthis opportunity.
As will be seen, by conducting 50 sampling experiments, the random
variationinf-g becomesobvious. Thisis quite differentfrom thesituation
faced byruleresearchers. Theytypicallyhaveasinglehistoricalsampleof
the market, and if all that data is used to test a rule, only one observed
value ofthe performance statistic is obtained. This provides no opportu
nity to see how the rule's performance would vary ifitwere tested in nu
merous independentsamples. The bottom line is this: Samplingvariation
is an important fact that may not be obvious to data analysts unfan1iliar
withstatisticalanalysis. Thisisahugeblindspot!
Sampling variation can be seen in a table of the 50 f-g values. See
Table4.1.



==================================================
                     PAGE 194                     
==================================================

178 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
TABLE 4.1 Fraction ofGrey Beads in 50 DifferentSamples
Sample Sample Sample
Number fog Number f·g Number fog
1 0.65 18 0.60 35 0.60
2 0.60 19 0.40 36 0.55
3 0.45 20 0.60 37 0.50
4 0.60 21 0.45 38 0.50
5 0.45 22 0.55 39 0.45
6 0.45 23 0.45 40 0.50
7 0.55 24 0.50 41 0.55
8 0.40 25 0.70 42 0.45
9 0.55 26 0.55 43 0.55
10 0.40 27 0.60 44 0.60
11 0.55 28 0.70 45 0.60
12 0.50 29 0.50 46 0.60
13 0.35 30 0.70 47 0.65
14 0.40 31 0.50 48 0.50
15 0.65 32 0.90 49 0.50
16 0.65 33 0.40 50 0.65
17 0.60 34 0.75
Notsurprisingly, the value off-g, a randomvariable, fluctuates from
sample to sample. This is an example ofchance in operation. However,
f-g's value is not entirely determined by chance. Its value is also
strongly influenced bythe value ofF-G, the proportion ofgrey beads in
the entire box.
Thus, itcanbesaidthateachobservedvalueoff-gisthe resultoftwo
influences; the underlying phenomenon, F-G, and randomness caused by
sampling. F-Gactslikea centerofgravity, alwayskeepingf-g withinacer
tain range. Insome samples, randomness nudgedthe valuef-g above F-G.
Inothersamples,randomnessnudgedf-gbelowF-G. Fromsampletosam
ple the value off-g oscillates randomlyaround this centerofgravity. This
issamplingvariability.
Anotherimportantaspectofthissituationisthatthenumberofobser
vations impacts our level of uncertainty about the value of F-G. The
greaterthe numberofobservations comprisingthe sample, the more pre
ciselythevaluef-g will reflect the value F-G. Supposethat, instead oftak
inga20-beadsample,we tooka200-beadsample. Therandomvariationin
f-g around F-G would be smaller. In fact, it would be about one third of
what it is for a 20-bead sample. This is a very important point: the larger
the sample size, the smaller the impact of randomness. Two hundred
beads dampen theabilityofanysinglebeadto pushf-g awayfrom F-G. In



==================================================
                     PAGE 195                     
==================================================

StatisticalAnalysis 179
a one-bead sample, thesingleselectedbeadwill produce anf-g ofeither0
or 1.0. Ina two-bead sample, f-g canbe 0, 0.50 or 1.0. Ina three-beadsam
ple, f-g can be 0, 0.33, 0.66, or 1.0. Thus, the larger the sample size, the
smaller will be the magnitude of random variation in f-g. Large samples
give F-G, thetruththatwe wishto know, the abilityto revealitself. Thisis
an effectoftheLaw ofLargeNumbers: Large samples reduce the role of
chance. In other words, it can be stated that the larger the size ofa sam
ple, the more tightlythevaluesoff-gwillclusteraboutthevalue F-G. This
isone ofthe mostimportantprinciplesinstatistics.
We have learned an important concept of statistics that can be
stated as follows: even though the value of F-G does not change over
the course ofthe 50 sampling experiments, the value f-g can vary con
siderably from sample to sample. The phenomenon is called sampling
variability. It is present whenever a random sample ofobservations is
used to form a conclusion about a larger universe (i.e., population).
Sampling variability is the source of the uncertainty that is addressed
bystatistical inference.
12
Frequency Distribution ofthe Sample Statistic f-g
In the box-of-beads experiment, the random variable f-g can assume
21 possible values ranging from zero, when there are no grey beads in
a sample, to 1.0, when the sample is composed entirely of grey beads.
The 50 observedvalues off-g are shown inTable 4.1. Acasual examina
tionshowsthatsomevalues off-g occurredwithgreaterfrequency than
others. Valuesin the range of0.40 to 0.65 occurred quite often, whereas
values less than 0.40 and greater than 0.65 almost never occurred. In
the 50 samples taken, f-g assumed the value 0.50 nine times, 0.55 eight
times, and 0.60 ten times. ote that the value 0.65, which characterized
the first sample, appeared only five times. Therefore, that value was
notamong the mostcommon values, butit was notparticularly unusual
either.
Aplotcalled afrequency distribution orfrequency histogranlcommu
nicates this information moreforcefully than words ora table. Itdisplays
howfrequently eachoff-g'spossiblevaluesoccurred overthe50sampling
experiments. Thetermdistributionisapt, becauseitdepicts howa setof
observationson arandomvariable are distributed orsprinkledacross the
variable'srange ofpossiblevalues.
Arrayed alongthe horizontalaxisofthefrequency distributionisase
quence ofintervals, orbins, onefor eachpossiblevalue off-g. The height
oftheverticalbarovereachintervalrepresentsthenumberoftimesthata
specificvalue off-g occurred. Figure4.6showsthefrequency distribution
ofthe 50f-gvalues.



==================================================
                     PAGE 196                     
==================================================

180 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Sample Size = 20 Number ofSamples = SO
10-fl--------
9 -11--------
F 8-tt-------
R
7 -tt------
E
Q 6-11-------
u
5
-11------
E
4
N -11-----------,
C
3 -tt-----
y
2-H------
1-11------
o
-JL---,--,----,--,---,-,r-
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.6 Frequency Distribution: (f-g).
The Equivalence ofFrequency and Area
Frequency distributions depict an important relationship between the
frequency ofa particularvalue orset ofvalues and the areacovered by
the bars representing that value or set of values. This concept can be
understood ifwe first consider the entire distribution. In Figure 4.6, all
the grey bars comprising the frequency distribution represent all 50
of the observed values of f-g. Thus, the area covered by all the verti
cal bars corresponds to 50 observations. You can verify this by sum
ming the counts associated with all ofthe vertical bars. They will add
up to 50.
Thesameprincipleappliesto anyfraction ofthe distribution'sarea. If
you were to determine the fraction of the frequency distribution's total
areacovered bytheareaofasinglebar,youwouldfind thatthe bar'sfrac
tionalareaisequaltothefraction ofthetotalobservationsrepresentedby
that bar. For example, the vertical bar associated with the f-g value 0.60
shows a count (frequency) of 10. Thus, the bar represents 20 percent
(10/50) ofthe observedvalues. Ifyou were to then measure the areacov
ered by this vertical bar, you would find that its area represents 0.20 of
the totalareacoveredbythe entiredistribution. Thisideaisillustrated in
Figure4.7.



==================================================
                     PAGE 197                     
==================================================

StatisticalAnalysis 181
10
This Bar Has 0.20 r--~
9 of Distribution's
Total Observations
F 8
This Bar Has 0.20
R
7---!l--------- ofDistribution's
E
Total Area
Q 6-#------
u
5---!l--------
E
N 4---!l--------
C 3
--\1------
y
2---!l--------
1-#------
a
---jL---,---r--r-----,----.----.--
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
I~I(;URI~4.7 Proportion ofobservations equals proportion ofdistribution's to
tal area.
Although this idea may seem obvious, even trivial, it is an essential
aspect ofstatistical reasoning. Ultimately, we will use the fractional area
ofa distribution to measure the probability that the back-testprofits ofa
rule could have occurred by chance under the assumption that the rule
has no predictivepower. When thatprobabilityis small, we are led to the
conclusion thattherule does have predictivepower.
Relative FrequencyDistribution off-g
The r-elative frequency distribution is similar to the ordinary frequency
distributiondiscussed earlier. The heightofa barinanordinaryfrequency
distribution represents an absolute number or count of the number of
times a particular value of a random variable was observed to occur. In
the relativefrequency distribution, the heightofabarrepresentsthenum
ber ofobserved occurrences relative to (divided by) the total number of
observations comprising the distribution. For example, f-g assumed the
value 0.60 on 10outof50 observations. Thus the value 0.60 had a relative
frequency of 10/50 or0.20. The distribution's barfor the value 0.60 would
be drawn to a heightof0.20 along the vertical scale. This is illustrated in
Figure4.8.



==================================================
                     PAGE 198                     
==================================================

•82 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Sample Size = 20 Number ofSamples = 50
R
E 0.2 -H-------------.----.--------
L
0.18
A
T 0.1 6 -tI----------
I
0.14
V
E 0.12
F 0.1
R
0.08 -tI-------
E
Q 0.06 -tI-------
u
0.04 -tI--------
E
N 0.02
C o
y -tL---,---,---,--.---.----.-
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.8 Relative frequency distribution: (f-g).
The equivalence between frequency and areathatapplies to ordinary
frequency distributions also applies to relative frequency distributions.
The relative frequency of all bars is equal (adds up) to 1.0. This simply
says that a random variable always (1.0 of the time) assumes a value
somewhere in the range encompassed by all bars. Ifyou were to add up
the relativefrequencies associated withall ofthe individual barscompris
ing the distribution, they would sum to 1.0 (100 percent of the observa
tions). This has to be so because each bar represents the fraction of
observations falling into that interval, and, by definition, all the bars to
getherrepresentall(1.0) the observations.
Therelativefrequency ofanysinglebarorgroup ofcontiguousbarsis
equal to theirproportion ofthe distribution's total area. Thus the relative
frequency ofanf-g value of0.65 and largeris equalto 0.10 + 0.06 + 0.02 +
0.02 =0.20. Thisis equivalenttosayingthat the combined areaofthe con
tiguous bars associated with f-g values of0.65 and largeris 20 percentof
the distribution's total area. We will be making statements similarto this
when testing claims about TA rules. Based on this, it can be said that the
relativefrequency off-gvaluesequal to orgreaterthan0.65is0.20. Thisis
illustratedinFigure4.9.



==================================================
                     PAGE 199                     
==================================================

StatisticalAnalysis 183
Sample Size = 20 Number ofSamples = 50
R
E 0.2 -4----------
L
0.18
A
T 0.16 Relative
Frequency
I
V 0.14 f-g = or> 0.65
E 0.12 Is 0.20
F 0.1
R
0.08
E
Q 0.06
u
0.04
E
N 0.02
C o
y --1L~~~~~
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.9 Relative frequency distribution: (f-g).
What Knowledge Has Been Gained about F-G
from Sampling?
So far, sampling has increased our level of knowledge about F-G. In the
first 20-bead sample, f-g had a value of 0.65. Based on this observation,
two possibilities were conclusively ruled out: that F-G is equal to 0, and
thatF-Gisequalto 1.0.
Not being contentwith these meager morsels ofwisdom, another 49
sampleswere taken, andthevaluef-g wasobserved ineach. These 50val
uesshowedthatf-gvariesrandomlyfrom sampletosample. However, de
spite the unpredictability of any single f-g value, an organized pattern
emerged from the randomness that was informative. The f-g values coa
lesced about a central value forming a well-organized hump. We suspect
that this central tendency is related to the value F-G, yet F-G's precise
value remains uncertain because ofthe randomvariationinf-g. However,
in lightofthe hump's relativelynarrowwidth, itseemsreasonable to con
jecturethatF-G liessomewhereinthe range 0.40to 0.65.
Given that we started with no knowledge of F-G and given that we
were precluded from examining the contents ofthe box in its entirety, a
lotwaslearnedfrom these 50samples.



==================================================
                     PAGE 200                     
==================================================

.84 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Sample Size = 20 Number ofSamples = SO
R
E 0.3
L
A 0.25
T
I
V 0.2
E
F 0.15
R
E
0.1
Q
u
E 0.05
N
c
0
y
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-9
FIGURE 4.10 Relative frequency distribution box 2: (f-g).
ASecond Box ofBeads
Now we will use sampling to learnthe colorproportions ofa second box
ofbeads.Ittooisamixtureofgreyandwhite. Theobjectiveis thesame
to learnthe relativeproportionofgreybeadstothe totalnumberofbeads
ina secondbox. We call this quantity F-G2.
As before, we are not allowed to examine the full contents ofbox 2,
butwe are permittedtotake 50samples, eachcomposedof20beads. The
value fog 2, which refers to the proportion ofgrey beads in a sample from
box 2, is measured in each sample. Again, note the distinction between
the terms F-G 2 and fog 2. F-G 2refers to the proportion ofgrey beads in
the entire box 2whereasfog 2refersto the proportionofgrey beadsinan
individualsample. F-G2isnotobservablewhilefog 2is.
Figure4.10is the relative frequency distributionfor 50values offog 2.
There areseveralthingsto notice:
t. Thegeneralshapeofthe distributionforbox2issimilartothatofbox
1, a hump clusteringaboutacentralvalue.
2. The centralvalue ofthe distribution for box2isdifferentthan thatof
box 1. The distribution for box 1was centered near a value of0.55.
The central value for box 2's distribution is near 0.15. This is clearly



==================================================
                     PAGE 201                     
==================================================

StatisticalAnalvsis 185
seen in Figure 4.11, whichshows both distributions on the same hori
zontal scale. An arrow has been placed above each distribution at
their approximate central values. Thus the arrows represent average
values for f-g and f-g 2. From this we can conclude that box 1has a
higherconcentrationofgreybeadsthanbox2.
3. Even though thesamplesfrom box 1and box2were bothaffected by
random variation, the degree ofrandom variation differs. Box 2's re
sultsarelessvariablethanthoseofbox1.Thisisevidencedbybox2's
narrowerclusteringaboutitscentralvalue. Given the lowerdegree of
sampling variation in box 2, it would be fair to say that we know the
value ofF-G2withgreatercertaintythan we know F-G.
What the Box Experiments Taught Us
about Statistics
Yogi Berra, former manager of the New York Yankees, said you can ob
servealotjustbylooking. Ifhewereastatistician, hemighthavesaidthat
you can learn a lotjustby sampling. Even though a sample is only a por
tion ofa largeruniverse (population), itcanteach us a lotaboutthatpop-
R
E 0.3
L
A 0.25
T
I
V 0.2
E
F 0.15
R
E
0.1
Q
u
E 0.05
N
c
y 0
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.11 Relative frequency distributions compared.



==================================================
                     PAGE 202                     
==================================================

186 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
ulation. However, there are limits to what a sample can tell us, limits im
posed byrandomness.
A fundamental task of statistics is to quantify the uncertainty due to
sampling variability. This makes it possible to quantify the reliability of
statements ofknowledge based on samples. Without such quantification,
such statements are of limited value. Statistics cannot eliminate uncer
tainty, butitcaninformusaboutit,and insodoing, ittempersassessments
of how much we know or how accurately we know it. Thus, statistical
analysisisapowerfulantidotetothetendencytobeoverconfident.
S11\TISTICAL THEORY
The box-of-beadsexperimentstouched onmanyofthe keyideasofstatis
tical inference. We will now mix in some theory to extend those ideas to
the evaluationofTArules.
The Six Elements ofa Statistical
Inference Problem
Evaluatinga claimthataTArule haspredictivepower, thatanewvaccine
can prevent illness, or any other claim of knowledge are examples of a
statisticalinference problem. Generally, suchproblems can bereduced to
six key elements: (1) a population, (2) a sample consisting ofa setofob
servations randomly selected from the population, (3) a population para
meter, (4) a sample statistic, (5) an inference, and (6) a statement about
thereliabilityoftheinference. Eachoftheseelementswillbediscussed as
they relate to the box-of-beads experiments and then to the evaluation of
TArules.
The Population. Apopulation consists ofallpossible observations of
a randomvariable. Itisalarge-perhapsinfinitelylarge-butawell-defined
universe ofobservations. In the typical statistical inference problem, we
want to learn some fact about the population, but it is either impractical
orimpossibleto observethepopulationinitsentirety. Inthebox-of-beads
experiments, the population consisted of the set of beads filling a box.
The colorwas the random variable. With respect to testing a TA rule, the
populationconsistsofallconceivabledailyreturns13thatwouldbeearned
bythe rule'ssignalovertheimmediatepracticalfuture.
To what does the termimmediatepracticalfuture refer? Itwould be
unreasonable to assume that the dynamics of financial markets are sta
tionary, andsoit would be unreasonableto expectthattheprofitabilityof



==================================================
                     PAGE 203                     
==================================================

StatisticalAnalysis 187
a rule will endure in perpetuity. For this reason, the population with re
spectto TArules cannotreferto returns occurringoveran infinite future.
Amore reasonable notionis whatIreferto as the immediatepracti
calJuture. Theimmediatepracticalfuture refers to afinite future timepe
riod, over which it would be reasonable to expect a useful rule's
profitability to persist, even though markets are nonstationary. Any en
deavor tofind predictivepatterns must make some assumption about the
continuityofpredictivepower. In otherwords, unless one is willingto as
sume some persistence inpredictivepower, allforms ofTAare pointless.
The assumptionbeingmade hereis thata rule will continue to work long
enough to compensate the researcherfor the effortofdiscoveringit. This
is consistentwith the position taken by Grossman and Stiglitz in "On the
impossibility of informationally efficient markets."14 It is also consistent
with the ideathat profitable rules signal opportunities to earn a risk pre
mium,15atopicdiscussed in Chapter7.
Therefore, the immediate practical future refers to all possible ran
dom realizations ofmarket behavior over a finite future. It is as if there
were an infinite number ofparallel universes, where all universes are an
exactduplicate except for the random componentofthe market's behav
ior.Ineachrealization, oruniverse, thepatternthataccountsforthe rule's
profitability is the same, but the random component ofthe market is dif
ferent. Thisideaisillustrated inFigure4.12.
Infinite Number
of
Parallel Universes
$
~ Immediate Time
Historical Practical Future
Sample
for Back-Testing Now
FIGURE 4.12 Different realizations of randomness in an infinitude of parallel
universes.



==================================================
                     PAGE 204                     
==================================================

188 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The Sample. Thesample isa subsetofthe population thatisavailable
for observation. In the box-of-beads case, we were able to observe 50 in
dependentsamples. In the case ofa TArule, we typicallyobservea single
sample ofthe rule'sperformancebybacktestingitoverasegmentofmar
kethistory. Thissampleiscomposed ofa sequenceofdailyreturns gener
ated by the rule's signals. The sequence ofreturns is reduced to a single
number, a sampleperformancestatistic.
The ropillation ral'amelel'. The population parameter is a fact or
characteristic about the populationthat we would like to know. Itis typi
callynumerical, butitneed notbe.
Unfortunately, the population parameter is unknown because the en
tire population cannotbe observed. The essence ofstatisticalinferenceis
the attempt to increase ourknowledge abouta populationparameter, de
spite the impossibility of observing the population in its entirety. In the
box-of-beads case, the population parameter ofinterest was the fraction
ofgreybeadsinthe box(Le., F-G, and F-G2). Inthe caseofaTArule, the
populationparameteristherule'sexpectedperformanceovertheimmedi
ate practical future. Performance can be defined in many ways. Some
commonmeasuresinclude average rate ofreturn, Sharpe ratio, profitfac
tor, and so forth. In this book, ourmeasure ofperformance is the annual
izedaverage dailyreturn ondetrended market(zero-centered) data.
In manystatisticalproblems, itis safeto assume the populationpara
meter never changes (Le., it is a constant). In the box-of-beads experi
ment, the proportion of grey beads remained constant. Statistical
problems in which the populationparameter remains fixed are saidto be
stationary.16 Now imagine a wrinkle on that situation. Suppose, unbe
knownst to the experimenter, an invisible demon secretly removes or
adds grey beads between samplings. Now the populationparameter, F-G,
is unstable, ornonstationary."17
Earlierin this chapter, Isaidthatitis besttostartwithanassumption
that any rule we test has no predictive power. That is, we assume that it
has an expected return equal to zero. Or, to put itin statistical terms, the
populationparameterisassumedequalto bezero.
The Sample Statistic. Asample statistic isa measurable attribute of
a sample.18Its value is known because it has been observed. In this book,
the term sample statistic is restricted to numerical facts, for example a
proportion, apercentage, astandarddeviation, anaveragerate ofreturn, a
trimmed average,19 a Sharpe ratio, and so forth. In a statistical inference
problem, the sample statistictypically refers to the same measurable at
tribute as the population parameter. In the case ofthe box-of-beads, the
sample statistic, f-g, referred to thefraction ofgreybeadsinan individual



==================================================
                     PAGE 205                     
==================================================

StatisticalAnalysis 189
sample. The population parameter, F-G, referred to the fraction of grey
beadsin the whole box.
The bottom line is this: A sample statistic is important because it
shedslightonthepopulationparameter. Beyondthis, itisafact ofnopar
ticularimportance. Some market historians seem to be unmindful ofthis
essentialtruth.
Ifa backtestshould resultinapositive rate ofreturn, itraises thefol
lowing question: Is the positive performance a random deviation above
zero due tosamplingvariabilityoris itattributable to the rule'spredictive
power(i.e., the rule hasanexpectedreturngreaterthanzero)?Answering
this question requires the tools ofstatistical inference. This is illustrated
inFigure4.13.
An Inference. Statistical inference is the inductive leap from the ob
served value of a sample statistic, which is known with certainty but
which is true only for a specific sample ofdata, to the value ofa popula
tionparanleter, which is uncertain butwhich is deemed to hold truefor a
wide, perhaps infinite, numberofunobservedcases.
When a rule's positivepastperformance canreasonablybe attributed
tosamplingvariability, the reasonableinferenceisthatitsexpectedreturn
overtheimmediatepracticalfuture iszeroorless. However, ifthepositive
performanceistoo highto bereasonablyattributedtosamplingvariability
(luck), the reasonable inferencewould bethatthe rulepossessesgenuine
predictive power and has a positive expected return over the immediate
practicalfuture.
. .
Expected "."
Return
????
Sample Statistic
Average Return = +10%
Immediate
Observable Past Practical Future
Time
FIGURE 4.13 Is the parameter's value greaterthan zero?



==================================================
                     PAGE 206                     
==================================================

190 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
1\ Statement about the Reliability of the Inference. Because a
sample can not represent the population perfectly, inferences based on a
sample statistic are subject to uncertainty. In other words, it is possible
that the inference is wrong. We have already seen how sample statistics
vary randomly around the true value ofthe population parameter value.
The science ofstatistics goes beyond simplyadmitting that its inferences
are uncertain. Itquantifies their reliability. This makes statistical conclu
sions considerably more useful than those reached by informal methods
thatdonotprovidesuchinformation.
The inference can bewrong in two ways. One erroris concluding the
rule has predictive power when it does not. This is a case of good luck
shining on a worthless rule. This type of mistake translates into taking
market risks thatwill not be compensated. The other error is concluding
the rule has no predictive abilitywhen itreally does. This mistake results
inmissed tradingopportunities.
DESCRIPTIVE S'OO'ISTICS
The field ofstatistics subdivides into two main areas: descriptive and in
ferential. The most important for TA is statistical inference, which was
discussed in the preceding sections. However, before making an infer
ence, wemustdescribe thesampledatainasuccinctandinformativeway.
Thetools ofdescriptivestatisticsservethispurpose.
The goal ofdescriptive statistics is datareduction, that is, reducing a
large set ofobservedvalues to a smaller, more intelligible setofnumbers
andplots. Descriptive statistics tell the story ofthe forest rather than the
individual trees. Three descriptive tools will be important for the work
that lies ahead: (1) frequency distributions, (2) measures of central ten
dency, and (3) measuresofvariation.
Frequency Distributions
Frequency distributions have already been discussed in connection with
thebox-of-beadsexperiments.Theywereusedtoreduceasetof50obser
vationsonrandomvariablef-g intoaninformativeplot.
With only 50 observations, one might be able to form an overall im
pressionofthe datasimplybylookingatatableofnumbers.Thenagain,if
the numberofobservationswere 500 or 5,000, a table would not be as in
formative asafrequency distribution.
Plottinga frequency distributionisusuallythefirst stepinanalyzinga



==================================================
                     PAGE 207                     
==================================================

StatisticalAnalysis 191
setofdata. Itprovides a quick visual impression oftwo key features ofa
sample ofobservations: the central tendency (e.g., averagevalue) and the
degree of dispersion or variation about the central value. For example,
Figure 4.11 showed thatthe two boxes ofbeads had differentcentralval
ues and differentdegreesofdispersion.
Avisual impression ofa sample is useful, butquantificationimproves
matters. For our purposes, we will need to quantify two features of the
frequency distribution: its central tendency and its variability or disper
sionaboutitscentraltendency.
Statistics That Measure Central Tendency
Therearemany measuresofcentraltendency. Threeofthe mostcommon
are the average, the median, and the mode. The average, also known as
the arithmetic mean, is used in manyTAapplications and is the summary
statisticused in this book. Itis the sum ofthe observedvalues divided by
the numberofobservations.
Itis important to distinguish the population mean from the sample
mean. The sample mean is a statistic, a known quantity that is com
puted from observed values and varies randomly from one sample to
the next. The population mean, in contrast, is unknown and does not
vary in stationary problems. The formula for the mean of a sample is
given in Figure 4.14 in two forms.
Sample Mean for Variable X
x=
Where Xiis an individualobservation on variable X
NG RE 4.14 Sample mean forvariable X.



==================================================
                     PAGE 208                     
==================================================

192 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Statistical Measures ofVariability (Dispersion)
Measuresofvariabilitydescribe the degree towhichthe observationsina
sample are dispersed about their central tendency. In other words, mea
suresofvariabilityquantifythe widthofafrequency distribution.
Among the widely used measures ofdispersion are the variance, the
mean absolute deviation, and the standard deviation. They are important
in the traditionalapproach to statistics (i.e., classical statistics). Thestan
dard deviationis the square root ofthe average squared deviation ofeach
observationfrom themean ofthe data. Theformulaforthestandarddevi
ationofasampleisshowninFigure4.15.
An intuitive notion ofdispersion is best conveyed with pictures. Fig
ure 4.16 shows a number ofidealized frequency distributions with differ
ing degrees of variation and different central tendencies. They are
idealized in the sense that the stair-step look that characterizes real fre
quency distributions has been smoothed away. The key point of Figure
4.16 is that central tendency and variation (dispersion) are independent
characteristics of a frequency distribution. In row 1 of Figure 4.16, the
four distributions have the same degree ofvariation but different central
values. Inrow2thedistributionshavethesamecentralvalue butdifferent
degrees ofvariation. In row 3 all distributions have different central val
uesand differentdegrees ofvariation.
Standard Deviation ofa Sample of Observations
s
n
Where:
Xi
Is an individual observation on variable X
X
Is the sample mean on variable X
n
Is numberofobservations in the sample
FIGURE 4.15 Standard deviation ofasample ofobservations.



==================================================
                     PAGE 209                     
==================================================

StatisticalAnalysis 193
I I
Same Degree of Dispersion but Different Means
o 5 10 15
I I
Equal Means but Different Degrees ofDispersion
2
3
1?IGURE 4.16 Central tendency and dispersion are distinct attributes of
adistribution.
PROBABILllY
The notion of probability is important in statistics because it is used to
quantify uncertainty. A conclusion reached via statistical inference is un
certain; that is, there is a chance of error. Thus, an inference about the
value of a population parameter has some chance of being wrong. This
chanceisgiven in termsofaprobability.
We routinelyuse informalnotions ofprobabilityto form expectations
and make choices. Is the personIfind so wonderful now, likelyto remain
so after marriage? What is the chance I will find gold ifI dig in a certain
spot? Is it probable I will feel fulfilled pursing a career in law? Should I
buythatstock, and ifIdo, how muchprofitam Ilikelytomake?
The informal usage of probability is connected to a cluster of inter
changeableterms illustratedinFigure4.17.
For our purposes, something more definitive is required. The defini
tion ofprobabilitythatmakes the mostsense for the work that lies ahead
is based on the notion ofrelative frequency. The relative frequency ofan
eventisthenumberoftimestheeventactuallyoccurreddividedbythe to
tal numberofopportunities onwhichthe eventcouldhave occurred. Rel
ative frequency is given as a fraction between 0and 1. Itcould rain every



==================================================
                     PAGE 210                     
==================================================

194 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Uncertainty
Risk Chance
Probability
Luck Likelihood
Random
FIGURE 4.17 Common notions ofprobability.
day, but in the month ofApril the relative frequency ofrain has actually
been0.366 (11 outof30days). Ifrain neveroccurred, then its relativefre
quencywouldbeO.Ifrain occurredeveryday, its relativefrequency is 1.0.
Thus, the relativefrequency ofan outcomeisequalto:
Number ofoccurrences ofan event
Number of possible opportunities for the event to occur
In the box-of-beads experiment the value of f-g was measured 50
times. Thiswasthe maximumnumberofopportunitiesforanyspecificf-g
value to occur. Infact, the valuef-g = 0.65 occurred on5outof50. There
fore, ithadarelativefrequency of5/50or0.10.
Probability is the relative frequency ofan event over the long run
the verylong run. Thatis to say, the probability ofan event is its relative
frequency ofoccurrence given an infinite number ofopportunities for its
occurrence. Probabilityis statedasa numberthatlies inthe range of0to
1 inclusive. A probability value of 0 means the event never occurs,
whereasavalueof1.0meansitalwaysoccurs.
Probability is a theoretical notion because it is never possible to ob
serve an infinite number of anything. For practical purposes, however,
when the number ofobservations becomesvery large, relative frequency
approaches theoreticalprobability.
The Law ofLarge Numbers
The tendency ofrelative frequencies to converge to theoreticalprobabili
ties, as the number of observations becomes large, is called the Law of
Large Numbers.20Theoperationofthe LawofLarge Numberscanbeillus
tratedwithcointossing. Thepossibleoutcomes(events)areheadsortails
andthe probabilityofheadsis knownto be 0.50. TheLawtells usthatthe



==================================================
                     PAGE 211                     
==================================================

StatisticalAnalysis 195
relative frequency ofheads will converge to its theoretical value, 0.5, as
the numberofcointosses becomeslarge. However, evenfor a large num
ber oftosses, a departure from 0.50 can still occur, though the likely size
ofthe departures decreases as the number oftosses increases. For small
samples, however, The Law ofLarge Numbers warns us that the relative
frequency ofheadscandiffersubstantiallyfrom 0.50. When the numberof
tosses is only three, a value of1.0 can easily occur-three heads in three
tosses.
Figure 4.18 shows the fraction ofheads in a coin-toss experiment as
the numberoftosses growsfrom one to one thousand. When the number
oftosses (observations) isless than 10, the fraction ofheads experienced
two large deviations from 0.50. At 3 tosses the fraction ofheads reached
0.66 (2 outof3). At8, thefraction was 0.375 (3 outof8). However, as the
number oftosses increased the random variable, fraction heads, experi
enced progressively smaller deviations from the expected value of 0.50.
Thisisexactlywhatthe LawofLarge Numberspredicts.
Now imagine a naive coin flipper. Upon observing five heads in five
tosses, he loudly proclaims, "I've found the holy grail ofcoins. It always
comes up heads." The Law ofLarge Numbers says that the poorfellow's
optimism is unwarranted. This is also likely to be true for the TA re
searcher who finds a rule with five historical signals, all of which were
correct. Optimism is mostlikely unwarranted. Chapter6will showthatif
this rule was selected because it was the best perfoIDling rule out of a
largeuniverseofback-testedrules, thatistosay, itwasdiscoveredbydata
mining, there would be even less reason to be optimistic about its future
performance.
F 1.0
R
A
C 0.80
T
I
a 0.60
N ~~----0.50
0.40
H 0.20
E
A
0
10 100 1,000
5
Number ofCoin Flips (log scale)
FIGURE 4.18 Law ofLarge Numbers.



==================================================
                     PAGE 212                     
==================================================

196 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Theoretical versus Empirical Probability
There are two kinds of probabilities: theoretical and empirical. "Theo
retical probabilities can be determined with a good degree of confi
dence purely on logical grounds. They are derived independent ofprior
experience, most often based on arguments of symmetry."21 In other
words, theycan be deduced withoutanyreference to the worldofexpe
rience. The probability ofa head in a coin toss is a theoretical probabil
ity. There is no need to toss it ten thousand times to predict that over
the long run the probability of heads is 0.50. Simply the fact that the
coin is fair, that it has two faces, and that it is unlikely to land on its
edge is sufficient grounds for suspecting heads and tails are equally
likely. The probabilities for being dealta royalflush in poker, the occur
rence of a six in a dice roll, and the chance of winning Lotto are also
theoretical because each can be determined by a logical analysis ofthe
situation.
Empirical probabilities are based on observed frequencies. Techni
cal analysis is concerned with this type of probability. The chance of
newsnowfall onMountHood during the month ofJuly orthe likelihood
ofa rise in the stockmarketfollowing a 2 percent drop in interestrates
are examples of empirical probabilities. They can only be detennined
by observing numerous past instances of the specified conditions
(Mount Hood in July), and determining the relative frequency of the
event (new snow).
When determining empirical probabilities, it is vitally important
thateachinstancebe characterizedbythe samesetofconditions. InTA
this is a practical impossibility. Each past instance ofa 2 percent drop
in interest rates is similar with respect to that specific condition, but
there are numerous other conditions that may not be the same. For ex
ample, the level ofinterest rates prior to the 2 percent drop may differ
from observation to observation. In one instance, interestrates were at
5percentpriorto the drop whereas in another they were 10percent. Of
course, the level of interest rates could be added to the set of condi
tions that define each instance, but that has the downside ofreducing
the number of comparable instances. Moreover, there will always be
otherconditions that are not part ofthe specified condition set. The in
ability to control all potentially relevant variables is an unfortunate fact
of life in nonexperimentaVobservational sciences. In contrast, the ex
perimental scientistenjoys thesupreme advantage ofbeingable to hold
constantall ornearlyall relevantvariables exceptthe one being investi
gated. Imagine being able to do that in TA! I am told this is how it is in
the afterlife.



==================================================
                     PAGE 213                     
==================================================

Statistical Analysis 197
PROBABILl1Y DISTRIBUTIONS OF RANDOM VARIABLES
A probability distribution shows how often we can expect the different
possible values ofa random variable to occur (Le., their relative frequen
cies). The probability distribution of a random variable is a relative fre
quencydistribution builtfrom an infinitenumberofobservations.
The concept of a probability distribution can be understood by
thinking of a sequence of relative frequency distributions, each built
from increasing numbers of observations and progressively narrower
binsorintervals. Gradually, the relativefrequency distributionbecomes
a probability distribution. With increasing numbers of observations,
more discrete intervals of decreasing width can be created.22 As the
number of observations approaches infinity, the number of intervals
approaches infinity and their widths shrink to zero. Thus, the rela
tive frequency distribution morphs into what is called a probability
distribution, more technically referred to as a probability density
function.23
Figures 4.19 though 4.23 show how a relative frequency distribution
evolves into a probability density function as the number ofintervals is
increased and their interval widths are decreased. The height of each
barrepresents the relativefrequency ofeventsfalling withinthe bar'sin
terval. The figures presume that the random variable is a measure of
price change.
Thissuccessionofdiagramsshowsthata probabilitydensityfunction
is, ineffect, a relativefrequency distributioncomposedofaninfinitenum
ber of observations whose intervals are infinitely narrow. Here is where
things get a bit strange. Ifthe intervals ofa probability distribution have
Relative
Frequency
Price Change Intervals
FIGURE 4.19 Relative frequency distribution based on five intervals.



==================================================
                     PAGE 214                     
==================================================

198 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Relative
Frequency
Price Change Intervals
FIGURE 4.20 Relative frequency distribution based on seven intervals.
Relative
Frequency
Price Change Intervals
FIGURE 4.21 Relative frequency distribution based on fifteen intervals.
Relative
Frequency
PriceChange Intervals
FIG RE 4.22 Relative frequency distribution based on twenty nine intervals.



==================================================
                     PAGE 215                     
==================================================

StatisticalAnalysis 199
Probability
Density
NoSpecificIntervals
FIGURE 4.23 Probabilitydensityfunction.
zero width, then there are zero observations per interval. This seems to
make no sense! However, it is quite common for mathematical concepts
to be at odds with common sense. In geometry, a point has location but
occupies no space (i.e., length, width, and breadth equal zero), a line has
length butzero width, andsoforth.
The fact thatan interval has zero observations has a strange implica
tion-the probability that any single value of a continuous random vari
able will everoccuris equal to zero. Forthis reason, it onlymakes sense
to speak of the probability that a random variable will assume a value
withinaspecifiedrange ofvalues. Inotherwords, itmakessensetospeak
of the probability that a random variable will assume a value between
specifiedminimum and maximumvalues. Italso makes sense to speak of
theprobabilitythata randomvariable will assume avalue that is equal to
or greater than some specified value or less than or equal to a specified
value. Forexample, we canspeak ofthe probabilitythata value of3.0 or
greaterwilloccur. However, itwould make nosensetospeakoftheprob
abilityofavalue ofexactly3.0.24
This somewhat counterintuitive idea fits nicely with rule testing.
When testing the statistical significance ofa rule's pastrate ofreturn, we
will beconcernedwiththe probabilitythatarate ofreturn of+10percent
orhighercouldhaveoccurredbychanceunderthe conditionthattherule
has no predictive power. Probability density functions can provide such
information.



==================================================
                     PAGE 216                     
==================================================

200 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
RElATIONSHIP BE1WEEN PROBABILITYAND
FRACTIONALARFA OF TIlE PROBABILITYDISTRIBUTION
Let's recap whathas beenestablishedup tothispoint:
1. The probability of an event is its relative frequency of occurrence
given an infinite number of instances in which the event could have
occurred.
2. Aprobabilitydensityfunctionisa relativefrequency distributionbuilt
from aninfinitenumberofobservationsand intervalsofzero width.
3. The relativefrequency ofa randomvariable takingonavaluewithina
given interval is equal to the fractional areaofthe frequency distribu
tionsittingontop ofthatinterval. See Figure4.24.
We are nowreadytotake thefinal stepinthissequence ofthinking. It
is analogous to point3 in the preceding list, except that the term relative
frequency distributionis replaced withprobability densityfunction. The
probability that a continuous random variable will assume a value within
a specified range is equal to the fraction ofthe probability density func-
Relative
Frequency
Relative Frequency
ofaValue inThis Range= 0.344
FIGURE 4.24 The correspondence ofrelative frequency and fractional area of
adistribution.



==================================================
                     PAGE 217                     
==================================================

StatisticalAnalysis 201
tionencompassed by(sittingabove) thatrange. Thisconceptisillustrated
in Figure 4.25. Itshows the probability that X, a continuous random vari
able, willassumeavalue withinthe rangeA-Bisequalto 0.70.
In many instances, the interval in question refers to the extreme or
tail of the distribution. For example, Figure 4.26 shows the probability
thatrandomvariable Xwillassume avalue ofBorgreater. The probabil
ity is 0.15.
The probabilitydistributionofa randomvariable isa veryuseful con
cept. Even though an individual future observation on random variable X
is not predictable, a large number of observations form a highly pre
dictable pattern. This pattern is the random variable's probability density
function.
Now, here is where these ideas startto come togetherfor us. Chapter
5will show that when a hypothesis is represented by the probability dis
tributionofa randomvariablewe willbeable to usean observedvalue of
therandom variabletotesttheveracityofthe hypothesis. Thiswillenable
us to conclude ifa rule's back-tested profitability was due to luck orgen
uine predictivepower.
The probabilitydistribution used for thispurpose is a specific type. It
iscalledasan1plingdistribution-perhapsthemostimportantoneinallof
statisticsand certainlythe mostimportantforTAanalysts.
P
R
o 0
B E
A N
B 5
I I
L T
I y
T
Y
A B
Range of Random Variable X
FIGURE 4.25 The probability random variable Xwill assume avalue between A
and Bequals the fraction ofthe distribution's total area above the interval A,B.



==================================================
                     PAGE 218                     
==================================================

202 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
P
R
o D
B E
A N
B S
I I Fraction of
L T Distribution
I Y Beyond "B"= 0.15
T ofArea
Y
B
Range of Random Variable X
FIGURE4.26 The probabilityrandom variable Xwill assumeavalue equal to or
greaterthan Bequals the fraction ofthe distribution's total areaequal to orgreater
than B.
THE SAMPLING DISTRIBUTIO : THE MOST IMPOR11\NT
CONCEPT IN S1l\TISTICAL INFERENCE
To recap:
1. Arandomvariableis the outcome ofa probabilityexperiment.
2. Takinga sample ofa given numberofobservationsfrom a population
and computing the value ofa statistic such as the sample's mean is a
probabilityexperiment.25
3. Asample statistic26is a randomvariable in the sense that itfluctuates
unpredictably from one sample ofobservations to the next. The sam
ple statistic fluctuates randomly because chance determines which
specificobservationswindupina particularsample, and itis thatpar
ticularsetofobservationsthatdetermines thevalueofthestatisticfor
that sample. In the box-of-beads experiment, the value f-g varied ran
domlyfrom one 20-beadsample to the nextbecausethe particularset
ofbeadsthatwound up inagivensamplewasdetermined bychance.
4. A relative frequency distribution describes how often the possible
values of a random variable occur over a very large number of
observations.
We are now ready to meetthe sampling distribution, the mostimpor
tantconceptinstatisticalinference.



==================================================
                     PAGE 219                     
==================================================

StatisticalAnalysis 203
The Sampling Distribution Defined
The sampling distribution is the probability distribution ofa random vari
able, and that random variable happens to be a sample statistic. In other
words, the sampling distribution shows the various possible values the
27
samplestatisticcanassume,and theirassociatedprobabilities. Forexam
ple, "thesamplingdistributionofthesamplemeanrefersto theprobability
distribution of all possible means for all possible random samples of a
givensizefrom somepopulation."28Here, the samplestatisticis the mean.
The sample statistic in the box-of-beads experiments was f-g. Its ran
dom variationacross50samples, each comprised of20 beads, wasshown
bythe relativefrequency distribution in Figure4.8. If, instead of50values
we were to take all possible 20-beadsanlples-avery large number-this
theoreticaldistributionwouldbef-g's samplingdistribution.
In rule back testing, the sample statistic is a measure ofperformance
observedina backtest. Inthisbookthatsamplestatisticwill bethe rule's
average rate of return. A back test typically produces a single observed
value for the performance statistic because we have a single sample of
markethistory. Nowimagine whatitwould be like ifwe were able to test
a rule in an infinite number of independent samples of market history.
This would provide an infinite number ofvalues for the perfornlance sta
tistic. Ifthis setofdatawere then converted into a relative frequency dis
tribution, it would be the statistic's exact sampling distribution. This is
obviouslynotpossiblebecausewe do nothaveaninfinitenumberofinde
pendentsamples ofhistorical data. However, statisticianshave developed
several methods to get something that approximates the exact sampling
distributioncloselyenoughtobeuseful, despitethefact thatwehaveonly
onesampleofhistoricaldataand onevalue ofthesamplestatistic. Twoof
these methodswill be discussed laterin this chapter.
The Sampling Distribution Quantifies Uncertainty
The sampling distribution ofa statistic is the foundation ofstatistical in
ference becauseitquantifiesthe uncertaintycausedbytherandomness of
sampling(samplingvariability).
As stated above, the sampling distribution displays the relative fre
quencies ofa statistic if it were to be measured in an infinite number of
random samples of the same size, drawn from the same parent popula
tion. The box-of-beadsexperimentsshowedthatthevalue ofastatistic, f-g
fluctuated randomlyfrom sampleto sample. Figure4.8showed thatthese
valuestendtofall intoa well-behavedpattern, notarandomchaoticmess.
The fact that the pattern of random variation is well behaved is what
makesstatisticalinferencepossible.



==================================================
                     PAGE 220                     
==================================================

204 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Itmay seem ironic that a sample statistic would show such a regu
larpatterngiven thatitis, in fact, a randomvariable. Thankfullyfor sta
tisticians, it does. Figure 4.8 revealed a central value (i.e., the mean of
the sampling distribution) ofapproximately 0.55. Italso showed a well
defined pattern of dispersion about this central value. This pattern al
lowedustoconclude,withafairdegreeofassurance, thatthevalueofthe
populationparameter F-G was contained within the range 0.40 to 0.65. It
also allowed us to conclude, although with somewhat diminished confi
dence, that F-G was more accurately pinned in the range between 0.50
and0.60. Itwas the degree ofdispersion inf-g's samplingdistribution that
permitted these statements to be made about the value ofthe population
parameterF-G.
The dispersion ofthe sampling distributionquantifies the uncertainty
ofour knowledge about the populationparameter F-G. The distribution's
central tendency conveys information about the most likelyvalue ofF-G,
approximately0.55. Knowingthisisnice, butitisnotenough.Itisalsoim
portant to know about the reliability of the value 0.55. In other words,
how accurately does the sampling distribution's central tendency of0.55
portraythetruevalueofF-G?
The reliability (degree ofcertainty) is conveyed by the dispersion of
thesamplingdistribution. Thegreaterthe distribution'sdispersionaround
its central value, 0.55, the less certain we can be that 0.55 accurately in
forms us aboutthe true value F-G, the proportion ofgrey beads inthe en
tirebox.
To illustrate this point consider the two following sampling distribu
tions. Both have centralvalues of0.55 but their dispersions are quite dif
ferent. The first shown in Figure 4.27 is a narrow dispersion thereby
deliveringastrongimpressionthatF-Gisinthevicinityof0.55. Thedistri
butioninFigure4.28iswiderthus conveyingalesscertainmessageabout
F-Gvalue. Itis sayingthat the true value F-G may be considerably differ
entfrom the centralvalue ofthesamplingdistribution.
In summary, certaintyis directly related to the dispersion ofthe sam
pling distribution. And the certainty of a conclusion about a population
parameter depends upon the width of the statistic's sampling distribu
tion-greaterwidthmeansgreateruncertainty.
Suppose it is hypothesized that a rule's expected return is equal to
zero. Also suppose the rule's back-tested return turns out to be greater
than zero. Is the positive returnsufficient evidence to conclude that the
hypothesized value ofzero is false? The answer will depend onhowfar
above zero the back-tested return is relative to the width of the sam
plingdistribution. Ifthe relative deviationislarge the hypothesiscan be
rejected. Quantifying the relative deviation and the reasoning behind a



==================================================
                     PAGE 221                     
==================================================

StatisticalAnalysis 205
R
E 0.2 -11---------
L 0.1 8
-11--------
A
T 0.1 6 -11-------
I
0.14
V
E 0.12
F 0.1
R
0.08
E
Q 0.06
u
0.04
E
N 0.02
C
o
y
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.27 Relative frequency distribution: fog. (sample size = 20, number of
samples=50).
R
E 0.2 -R--------------
L
0.18
A
T 0.16 ---ll---------------------
I 0.14 -11-------------------
V
E 0.12
F 0.1
R
0.08 ---ll---------
E
Q 0.06 -ft-----
u
0.04
E -11----
N 0.02
C o
y -+--.-----1'-
o 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
f-g
FIGURE 4.28 Relative frequency distribution: fog. (sample size = 20, numberof
samples=50).



==================================================
                     PAGE 222                     
==================================================

206 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
decision to rejectornotto rejectthe hypothesis will betreatedinChap
ter5. The important pointfor now is that the width ofthe sampling dis
tributionis criticallyimportantinanswering this question.
Sampling Distribution ofTrading Performance
A sampling distribution can be formed for any statistic: the average
(mean), the median, the standard deviation, and many other statistics
29
used instatisticalinference. Eachhasitsownsamplingdistribution.
This discussion here is confined to the sampling distribution of the
mean because the performance statistic used to evaluate TA rules in this
bookisa rule's mean rate ofreturn. However, there are manyotherperfor
mance measures that might be used: the Sharpe ratio,30 the profit factor,31
the mean return divided by the Ulcer Index,32 and so forth. It should be
pointedoutthatthesamplingdistributionsofthesealternativeperformance
statisticswouldbedifferentfrom thesamplingdistributionofthemean.
It should also be pointed out that the methods used in this book to
generate the samplingdistributionofthe mean may be oflimitedvalue in
generating sampling distributions for performance statistics with elon
gated right tails. This can occur with performance statistics that involve
ratios such as the Sharpe ratio, the mean-return-to-Ulcer-Index ratio, and
theprofitfactor. However, thisproblemcanbemitigated bytakingthelog
ofthe ratiotoshortenthe righttail.
The Three Distributions ofStatistical Inference
Statistical inference actually involves three different distributions, one of
whichis the samplingdistribution. Theyare easyto confuse. Thissection
is intended to clarify their differences. In the context of rule testing the
threedistributionsare:
1. Data distribution of the population: an infinite sized distribution
comprisedofallpossibledailyrulereturns, whichweassumeextends
intothe immediatepracticalfuture.
2. Data distribution ofthe sample: a distribution comprised ofa finite
number(N) ofdailyrule returnsfrom thepast.
3. Sampling distribution: an infinite-sized distribution of the sample
statistic-in this case it is the rule's mean return. It represents the
mean returns ofthe rule, ifit were to be tested in an infinite number
ofrandomsamplesofsize Nextractedfrom the population.
Thereare twopointsto emphasize. First,the observationscomprising
the data distribution of the population and the data distribution of the



==================================================
                     PAGE 223                     
==================================================

StatisticalAnalysis 207
sample (1 and 2in the preceding list) are comprised ofsingle-day rule re
turns. In contrast, the observations comprising the sampling distribution
are sample statistics in which each observation represents the mean re
turn ofthe rule computed overasample ofdays. Second, boththepopula
tion distributionand thesamplingdistributionare theoreticalinthesense
that they refer to an infinite numberofpossible observations. Incontrast,
the datadistribution ofthe sample is composed ofa finite number ofob
servationsfrom a historicalbacktest.
The relationship between these three distributions can be visualized
by imagining an experiment similar to the box-of-beads experiment. In
this case, however, you are to imagine a population distribution of daily
rule returnsthat is infiniteinsize. Imagine taking50independentsamples
from the population, where each sample is comprised of a substantial
number of daily rule returns. Next, determine the mean return for each
sample and then plotthe sampling distribution ofthe 50 means. Thisisil
lustrated inFigure4.29.33
The Real World: The Problem ofOne Sample
The preceding discussion was theoretical in the sense that we imagined
whatitwould be like ifwe could observe 50 independentsamples from a
Data
Distribution
in the
Population
(daily returns)
1
••••• Sample# SOofn
Data DailyReturns
Distribution
in the
Sample
ofSize n
(daily returns)
.l
Sampling
Distribution
50
Sample Means Distribution ofthe Means
ofSample Size n
FIG HE 4.29 The three distributions ofstatistical inference.



==================================================
                     PAGE 224                     
==================================================

208 METHODOLOGICAL.PSYCHOLOGICAL. PHILOSOPHICAL. STATISTICALFOUNDATIONS
Population
Mean
Distribution of Daily Returns (not known)
Comprising the Population
DailyReturns
One Value for Sample
Distribution ofDaily Returns
Mean
Comprising the Sample (known)
DailyReturns
FIGURE 4.30 The real world: one sample and one value ofthe test statistic.
parent population. In most real-world statistical problems, there is only
one sample ofobservations and thus one value for the sample mean. The
problemisthatwithonlyonesamplemeanavailablewehavenonotionof
thesamplestatistic'svariability. Theproblemofonesampleand onemean
isillustratedinFigure4.30.
Fortunately, we can learn a lot about a sample statistic's variability
and its sampling distribution by looking at the single available sample.
The discovery of how to do this, early in the twentieth century, is what
made the field ofstatistical inference possible. In fact, statisticians have
developed two distinct approaches for estimating the sampling distribu
tion ofa statisticfrom a single san1ple ofobservations: classicaland com
puter intensive. Although both will be discussed, the computer-intensive
approach will be usedforthe rule testsperformedinPartTwo.
DERIVING THE SAMPLING DISTRIBUTION:
THE CIASSICt\L APPROACH
This classical approach is the one most often taught in basic statistics
courses. It is attributed to the two fathers ofmathematical statistics, Sir
Ronald Fisher (1890-1962) and Jerzy Neyman (1894-1981). It utilizes
probability theory and integral calculus to derive the sampling distribu
tion on the basis ofa single observed sample. Itprovides estimates ofthe
sampling distribution's dispersion, its mean, and its basic shape (normal).



==================================================
                     PAGE 225                     
==================================================

Statistical Analysis 209
Inotherwords, itprovides everythingneeded to quantify the reliabilityof
an inference basedonasinglesample ofobservations.
The Sampling Distribution of the Sample Mean
Every statistic has its own sampling distribution. This discussion will fo
cus on the sampling distribution ofthe mean (average), because it is the
statisticused inPartTwo to evaluateTArules.
Classical statistical theory tells us several things about the sampling
distributionofthe mean.
1. Themeanofalargesampleisagoodestimateofthemeanofthepop
ulation distribution from which the sample was obtained. The larger
the sample, the more closelythe sample mean conforms to the popu
lationmean.
2. Thedispersionofthesamplingdistributiondepends onthe size ofthe
sample. For a given population, the larger the sample, the narrower
thesamplingdistribution.
3. The dispersion of the sanlpling distribution also depends on the
amount ofvariation within the parentpopulation data. The biggerthe
variationin theparentpopulation, thewiderthesanlplingdistribution.
4. Under the conditions that apply to the evaluation of TA rules, the
shape ofthe sampling distribution ofthe mean will tend towards the
so-called normal or bell shape, with the conformity to the normal
shapeincreasingasthesamplesizeincreases. Othersamplestatistics,
suchas ratios, can beprofoundlynonnormal.
Eachoftheseconceptsisnowexplained.
The Mean of a Large Sample Is a Good Estimate of the ropu
lation Mean. This is a manifestation of The Law of Large Numbers,
which we metpreviouslyin the context ofa coin-flipping experiment. To
recap, The Law of Large Numbers tells us that the larger the nunlber of
observations comprising the sanlple, the more closely the sample mean
will approximate the population mean. There are some qualifying condi
tions34 attached to thispromise,buttheyneednotconcernushere.
Thegraph ofthe coin-flippingexperimentillustratedthe actionofthe
Law of Large Numbers (Figure 4.18). As the number of coin tosses in
creased, the proportion ofheads gradually converged to its theoretically
correctvalue of0.50. Intheearlystages oftheexperiment, when thesam
ple size was small, there were large departures from 0.50. These depar
tures illustrate the large role that chance plays in small sanlples. In four



==================================================
                     PAGE 226                     
==================================================

210 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
tosses, though values of0.75 or 0.25 are not the most probable, they are
quite common. However, when the number ofcoin tosses reaches 60, the
probability of0.75 or0.25is lessthan 1in 1,000. The importantlesson: In
creasedsamplesizediminishes the roleofchance.
The Dispersion of the Sampling Distribution Depends on the
Size of the Sample: the Larger the Sample the Less the Dis
persion. Imagineaverylargepopulationofobservationsonsomevari
able whose standard deviation is 100. Usually, we do not lmow the
populationstandarddeviation, butinthiscasewe willassume thatwedo.
In Figure 4.31, I show the parent population distribution at the bot
tom. Above it are two sampling distributions for the variable's mean for
sample sizes of10and 100. Notice thatthe width ofthe samplingdistribu
tion is cut by approximately one-third each time the sample size is in
creased by a factor of 10. One of the lessons from classical statistics is
thatthe standard deviation ofthe sampling distribution ofthe mean isin
versely proportional to the square root of the sample size.35 Thus, if the
samplesizeisincreased byafactorof10,thewidthofthesamplingdistri
butionisreducedafactorof3.16, thesquarerootof10.Theparentdistrib-
Sampling Distribution
ofthe Mean
Sample Size = 100
Standard Error= 10
10
- 4 k -
Sampling Distribution
ofThe Mean
Sample Size = 10
St. Error= 31 .62
31.62
Distribution of Data
in the Population
St. Deviation =100
100
FIGURE 4.31 The narrowing ofthe sampling distribution with increased sam·
pie size.



==================================================
                     PAGE 227                     
==================================================

Statistical Analysis 211
ution can be thought ofas a sampling distribution with a sample size of
one (one mean-the population mean). The sampling distribution for a
samplesizeof100isone-tenth the widthofthe parentpopulation. Thees
sential message is the larger the sample size, the less the uncertainty
about the value ofa sample statistic. Incidentally, the standard deviation
ofthe sampling distribution ofthe mean is given a special name in statis
tics, thestandard errorofthemean (seelaterexplanation).
Withsomequalifications,36the Law ofLarge Numbers tells us thatthe
larger the sample size, the more accurate our lmowledge of the popula
tion'saverage. Withsome caveats,37therefore, we wouldliketo workwith
samples that are as large as possible so as to minimize the width of the
samplingdistributionasmuch aspossible.
l'he Dispersion ofthe Sampling Distribution of the Mean Also
Depends On theAmount ofVariation within the Data Compris
ing the Parent Population. Thereisanotherfactorthatimpactsthe
width of the sampling distribution-the amount of variation within the
parentpopulationfrom which thesample was taken. Thegreaterthevari
ation within the population data, the greater will be the variation (disper
sion) ofthesamplingdistribution.
You canvisualize this bythinkingabouta populationthathas novari
ation at all. For example, ifall people in the population weighed 150 lbs,
every sample is going to have an average weight of 150 lbs. Therefore,
zero variation among the members ofthe population would lead to zero
variationin the sample average, and consequentlya sampling distribution
that has no dispersion whatsoever. Conversely, ifthere is great variation
among the individuals comprising the population, there is going to be
greater variation in the san1ple averages, which translates to a fat sam
plingdistribution.
Shape of' the Sampling Distribution Tends Toward ormal.
TheCentralLimitTheorem, afoundationalprinciple ofclassicalstatistics,
states thatas the size ofa samplegetslarger, the sampling distribution of
themean, withsomequalifications,38convergestowardaspecificshapeir
respective ofthe shape ofthe population distribution, In otherwords, no
matterhowweirdlyshapedthedistributionofthe dataintheparentpopu
lation, the shape ofthe sampling distribution approachesa specificshape
that statisticians refer to as the normal distribution, also lmown as the
Gaussian distribution.
The normal distribution is the shape ofthe most common probabil
ity distribution. Often referred to as the bell curve because it has the sil
houette of the Liberty Bell, the normal distribution characterizes
continuousvariables thatdescribe many real-worldphenomena.



==================================================
                     PAGE 228                     
==================================================

212 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
There are a few points about the normal distribution that should be
understood:
• It is completely described by its mean and its standard deviation. If
thesetwofacts aremown, youmoweverythingthatthereistomow
aboutthe distribution.
• About 68 percent of the observations lie within one standard devia
tion ofthe mean, andabout95percentlie within two standard devia
tionsofthe mean.
• The tails ofthe distribution become quite thin beyond two standard
deviations. Thusvaluesbeyondthreestandarddeviationsare rare and
beyondfourareextremelyrare.
The normal distribution, whichisillustratedinFigure4.32, isso com
monthatithas beencalledafundamental feature ofthe natural world. Its
shapeiscausedbytheadditiveeffectsofmanyindependentfactorsacting
onthesame situation. Forexample, systolic bloodpressureisaffected by
genetic factors, diet, weight, lifestyle, aerobic conditioning, and a multi
tude of other factors. When they interact, the probability distribution of
bloodpressuresinalargegroupofrandomlyselectedpeoplewill havethe
bell-shapeddistribution.
Figure 4.3339 shows several very nonnormal population distributions
along with the shape ofthe sampling distribution ofthe mean. Note how
the sampling distribution convergesto a normal shape as a sample size is
increased irrespective ofthe shape of the distribution ofthe data in the
2 0" 10" +10" +20"
\"c"
I I
68%oftheArea
r
95% ofthe Area
FIGURE 4.32 The normal distribution.



==================================================
                     PAGE 229                     
==================================================

StatisticalAnalysis 213
Parent Sampling Sampling Sampling
Population Distribution: n=.2..Distribution: n=.5.. Distribution: n=30
...
f'IGURE 4.33 Given sufficient sample size the sampling distribution of the
mean approaches normal shape irrespective ofthe variable's distributional shape.
parentpopulation. Thereisnothingmagicalaboutthenumber30. Therate
atwhich the samplingdistributionconvergesto a normalshapeasa func
tion ofsample size depends on the shape ofthe parent distribution. Note
that for three ofthe cases in the figure, a sample size offive produces a
nearly normally shaped sampling distribution ofthe mean. However, for
the bottom case in the figure, the sampling distribution ofthe mean does
notbecome normal-like untilasamplesize of30.
The Standard Error ofthe Mean
We are now in a position to take the final step toward defining the sam
pling distribution ofthe mean as itis done in classicalstatistics. So far, it
hasbeenestablished that:
1. The sampling distribution of the mean converges to the shape of a
normaldistributionas thesamplesizeisincreased.
2. Anormal distribution is fully described by its mean and its standard
deviation.



==================================================
                     PAGE 230                     
==================================================

214 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
3. The standard deviation ofthe sampling distribution ofthe mean, also
known as the standard error of the mean, is directly related to the
standard deviation of the population from which the samples were
taken.
4. The standard errorofthe meanis inverselyrelated to the square root
ofthesamplesize.
The standard error ofthe mean is equal to the standard deviation of
thepopulationdivided bythesquare rootofthesamplesize. Thisisa true
statement but not ofpractical use because the standard deviation ofthe
populationis notknown. Thisissimplya consequenceofthefact thatthe
full population is not observable. However, the standard deviation ofthe
populationcanbeestimatedfrom thesample. Thisestimate, whichisdes
ignatedbysigma-hat, isshowninequationin Figure4.34. Thelittleroofor
hatoverthesigmaisstatisticalsymbolismfor aquantitythathas beenes
timated. You will note that the divisor within the radical sign is n - 1,
ratherthann. This modification compensatesforthe fact thatthe sample
standarddeviationunderstates thestandard deviationofthepopulation.
This expression is used to estimate the standard deviation ofthe par
ent population, which is then used in the equation in Figure 4.35 to esti
mate thestandarderrorofthe mean.
With the standard errorofthe mean in hand, and theassumption that
thesamplingdistribution is normal, itis nowpossible to form agood esti-
Estimate of Population Standard Deviation
Based on a Sample of Observations
J
a-
Where:
X· Isan individual observation on variable X in the population
I
X Isthe sample mean on variable X
n
Is numberofobservations in the sample
FIGURE 4.34 Estimate of population standard deviation based on a sample of
observations.



==================================================
                     PAGE 231                     
==================================================

StatisticalAnalysis 215
Standard
Error
of
the Mean
FIGURE 4.35 The standard errorofthe mean.
mate ofthe sampling distribution ofthe mean. And recall all this was on
the basisofasinglesample.
There is one problem with the traditional approach. It assumes that
the sampling distribution is normally shaped (bell curve). Ifthis proves
notto beso, the conclusionsreached will beinaccurate. Forthisreason, I
have chosen to use an alternative methodfor estimatingthe samplingdis
tributionofthe mean, basedoncomputersimulation.
DERIVING THE SAMPLING DISTRIBUTION WITH
THE COMPUTER·llVfENSIVE APPROACH
Untilabout30yearsago there was noalternativetotheclassicalapproach
forderivingthesamplingdistributionfrom asinglesample. Nowthereis
the computer-intensive approach. By systematically resampling the single
availablesamplemanytimes, itispossibletoapproximatetheshapeofthe
samplingdistribution. Thisprocedureisdescribedinthissection.
The next chapter will present two computer-based methods: boot
strap resampling and Monte Carlo permutation. Although each method
has its limitations, as is true for anystatistical method, they are useful al
ternatives to the classicapproach.
The bootstrapissonamed becauseitseeminglyliftsitselfup, asifby
its own bootstraps, to approximate the shape of the sampling distribu
tion. In this book, it will be used to approximate the shape ofthe sam
pling distribution ofthe average return for rules that have no predictive
power. The Monte Carlo permutation method is named after the famed
European gambling casino because it uses the computer as if it were a
roulette wheel.
By whatever means it is generated, the sampling distribution serves
the same purpose. It tells us how much the back-tested return ofa non
predictive rule canvaryabove orbelowzero due to the random effects of
sampling. Thus, it serves as the benchmark for evaluating rule perfor
mance.Ifthe rule'smean returnis too hightobedue tosamplingvariabil
ity,the ruleisjudgedtohave predictivepower.



==================================================
                     PAGE 232                     
==================================================

216 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
PREVIEW OF NEXT CIIAPTER
Wehavealreadymadeanumberofreferencestousingstatisticalinference
to test claims. The official title for this activity is hypothesis testing. The
nextchapterdiscussestherationaleandmechanicsofhypothesistesting.
It will also consider another use of statistical inference, estimating
confidence intervals. A confidence interval is a range that contains the
true value of the population parameter with a specified level of confi
dence. For example, ifthe observed mean return ofa rule is 15 percent
the 95 percent confidence interval might indicate that the rule's true re
turnliesbetween5percentreturnand25percentreturn. Thewidthofthe
confidence interval, in this case 20 percentagepoints, is derivedfrom the
samesamplingdistributionthatisusedforhypothesis testing.



==================================================
                     PAGE 233                     
==================================================

Hypothesis Tests
and Confidence
Intervals
TWO TYPES OF STATISTICt\L INFERENCE
Statisticalinferenceencompassestwoprocedures: hypothesis testingand
parameter estimation. Both are concerned with the unknown value of a
population parameter. Ahypothesis test determines ifa sample ofdatais
consistent with or contradicts a hypothesis about the value of a popula
tion parameter, for example, the hypothesis that its value is less than or
equal to zero. The otherinference procedure, parameter estimation, uses
the information in a sampleto detern1ine the approximate value ofa pop
ulationparameter. Thus,a hypothesistesttells usifaneffectispresentor
1
not, whereasan estimatetells us aboutthesize ofaneffect.
In some ways both forms of inference are similar. Both attempt to
draw a conclusion about an entire population based only on what has
been observed in a sample drawn from the population. In going beyond
whatis known, bothhypothesistestingandparameterestimationtake the
inductiveleapfrom the certainvalue ofasamplestatisticto the uncertain
valueofa populationparameter. Assuch, bothare subjectto error.
However, important differences distinguish parameter estimation
from hypothesis testing. Their goals are different. The hypothesis test
evaluates the veracity of a conjecture about a population parameter
leadingto an acceptance orrejection ofthat conjecture. In contrast, es
timation is aimed at providing a plausible value or range ofvalues for
the populationparameter. In this sense, estimationis a bolderendeavor
and offers potentially more useful information. Rather than merely
telling us whether we should accept orreject a specific claim such as a
217



==================================================
                     PAGE 234                     
==================================================

218 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
rule's average return is less than or equal to zero, estimation approxi
mates the average return and provides a range ofvalues within which
the rule's true rate ofreturnshould lie ata specified level ofprobability.
Forexample, itmay tell us thatthe rule's estimated return is 10percent
and there is a 95 percent probability that it falls within the range of 5
percentto 15percent. This statementcontainstwo kinds ofestimates; a
point estimate, thatthe rule's return is 10percent, and aninterval esti
mate, that the return lies in the range 5 percentto 15 percent. The rule
studies discussed in Part Two use estimation as an adjunct to the hy
pothesis tests.
IIYPOTHESIS TESTS VERSUS INFORMAL INFERENCE
Ifa rule has beenprofitable inasample ofhistorical data, thissample sta
tisticisanindisputablefact. However, from thisfact, whatcanbeinferred
about the rule'sfuture performance? Isitlikelyto be profitable because it
possesses genuine predictive power or are profits unlikely because its
pastprofitsweredue to chance?The hypothesistestisafonnal and rigor
ous inference procedure for deciding which ofthese alternatives is more
likelytobecorrect,andsocanhelp us decideifitwould berationalto use
the rule for actualtradinginthefuture.
Confirmatory Evidence: It's ice, It's Necessary,
but It Ain't Sufficient
Chapter2pointed outthat informal inference is biased in favor ofconfir
matory evidence. That is to say, when we use common sense to test the
validity ofan idea, we tend to lookfor confirma~oryevidence-facts con
sistent with the idea's truth. At the same time, we tend to ignore or give
too little weight to contradictory evidence. Common sense tells us, and
rightlyso, thatiftheideaistrue, instanceswhere the ideaworked (confir
matory evidence) should exist. However, informal inference makes the
mistake ofassuming that confirmatory evidence is sufficient to establish
its truth. This is a logical error. Confirmatory evidence does not compel
the conclusionthatthe ideaistrue. Becauseitisconsistentwiththeidea's
truth, itmerelyallowsforthepossibilitythatthe ideais true.
The crucial distinction between necessary evidence and sufficient evi
dence was illustrated in Chapter 4with in the following example. Suppose
wewishtotestthetruthoftheassertion: ThecreatureIobserveisadog. We
observethatthe creaturehasfour legs (the evidence). Thisevidenceiscon
sistentwith(i.e.,confirmatoryot)thecreaturebeingadog.Inotherwords, if



==================================================
                     PAGE 235                     
==================================================

Hypothesis Tests andConfidence Intervals 219
the creature is a dog, then it will necessarily have four legs. However, four
legsarenotsufficientevidenceto establishthatthecreatureisadog. Itmay
verywellbeanotherfour-legged creature(cat, rhino, andsoforth).
PopulararticlesonTAwilloftentrytoarguethatapatternhaspredic
tive power by presenting instances where the pattern made successful
predictions. Itis true that, ifthe pattern has predictive power, then there
will be historical cases where the pattern gave successful predictions.
However, suchconfirmatoryevidence,whilenecessary, isnotsufficientto
logicallyestablishthatthepatternhaspredictivepower.Itisno moreable
to compel the conclusion that the pattern has predictive power than the
presence offour legs is able to compel the conclusion that the creature
is adog.
To argue that confirmatory instances are sufficient commits the fal
lacyofaffirmingthe consequent.
Ifp is true, then qis true.
qis true.
Invalid Conclusion: Therefore, p is true.
If the pattern has predictive power, then past examples ofsuccess
shouldexist.
Past examples ofsuccess exist, and here they are.
Therefore, thepatternhaspredictivepower.
Chapter 3 showed that although confirmatory evidence is not suffi
cienttoprovethetruth ofanassertion, contradictoryevidence-evidence
that is incompatible with an assertion's truth-is sufficient to establish
that the assertion is false. The fact thata creature lacks four legs is suffi
cient to falsify the assertion that the creature is a dog. This is the valid
form ofargumentcalled denialoftheconsequent.
Ifp is true, then qis true.
qis not true.
Therefore, p isnot true (i.e., p isfalse).
Ifthe creatureis adog, then the creaturehasfour legs.
Creature does nothavefor legs.
Therefore, creatureis not adog.
The logical basis of the hypothesis test is falsification of the conse
quent. Assuch, itis apotentantidote to the confirmation bias ofinformal
inferenceandaneffectivepreventativeoferroneousbelief.



==================================================
                     PAGE 236                     
==================================================

220 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
What Is a Statistical Hypothesis?
Astatisticalhypothesisisa conjectureaboutthevalue ofapopulationpa
rameter. Often this is a numerical characteristic, such as the average re
turn ofa rule. The population parameter's value is unknown because itis
unobservable. For reasons previously discussed, it is assumed to have
value equal to orlessthan zero.
What an observer does know is the value ofa sample statistic for a
sample that has been drawn from the population. Thus, the observer is
faced witha question: Isthe observedvalueofthesamplestatistic consis
tent with the hypothesized value ofthe population parameter? If the ob
served value is close to the hypothesized value, the reasonable inference
would be thatthe hypothesis is correct. If, onthe otherhand, the value of
thesamplevalueisfarawayfrom the hypothesizedvalue, the truthofthe
hypothesisis calledintoquestion.
Close and far are ambiguous terms. The hypothesis test quantifies
these terms making it possible to a draw a conclusion about the veracity
ofthe hypothesis. The test's conclusion is typicallygiven as a numberbe
tween 0 and 1.0. This number indicates the probability that the observed
value of the sample statistic could have occurred by chance under the
condition that (given that or assuming that) the hypothesized value is
true. Forexample, supposeitis hypothesized thata rule'sexpectedreturn
is equal to zero, but the back test produced a return of +20 percent. The
conclusion ofthe hypothesis testmaysaysomethinglikethefollowing: If
therule's expectedrate ofreturnwere truly equal to zero, there is a 0.03
probability that the back-tested return could be equal to orgreater than
+20 percent due to chance. Because there is only a 3 percentprobability
that a 20 percent return could have occurred by chance if the rule were
truly devoid ofpredictive power, then we can be quite confident that the
rule was notsimplyluckyinthe backtest.
Falsifying a Hypothesis with Improbable Evidence
A hypothesis test begins by assuming that the hypothesis being tested is
true. Based onthisassumption, predictionsare deducedfrom the hypoth
esis about the likelihood ofvarious new observations. In other words, if
the hypothesis is true, then certainoutcomeswould be probableto occur
whereas otheroutcomeswould beimprobable. Armed with thissetofex
pectations an observer is in a position to compare the predictions with
subsequent observations. If predictions and observations agree, there is
no reason to question the hypothesis. However, if low probability out
comes are observed-outcomesthatwould be inconsistentwith the truth
ofthe hypothesis-the hypothesis is deemed falsified. Thus, it is the oc-



==================================================
                     PAGE 237                     
==================================================

Hypothesis Tests and Confidence Intervals 221
currence of unexpected evidence that is the basis for refuting a hypothe
sis. Though this line ofreasoning is counterintuitive, itis logicallycorrect
(denial ofthe consequent) and extremelypowerful. It is the logical basis
ofscientificdiscovery.
To give a concrete example, suppose Iview myselfas an excellentso
cial tennis player. My hypothesis is DavidAronson is an exceUent social
tennis player. Ijoin a tennis club with members whose age and years of
play are similar to mine. On the basis ofmy hypothesis, I confidentlypre
dict to other club members that I will win at least three-quarters of my
games (predicted win rate = 0.75). This prediction is merely a deductive
consequence of my hypothesis. I test the hypothesis by keeping track of
myfirst20games. After20gamesIamshockedanddisappointed. Notonly
have Inotscored a singlevictory (observed win rate = 0), butmostlosses
have been by wide margins. This outcome is clearly inconsistentwith the
prediction deduced from my hypothesis. Said differently, my hypothesis
implied that this evidence had a very low probability ofoccurrence. Such
surprising evidence forcefully calls for a revision (falsification) ofmy hy
pothesis. Unless I prefer feel-good delusions to observed evidence, it is
timeto abandon mydelusionsoftennis grandeur.2
In the preceding situation, the evidence was overwhelmingly clear. I
lost every one of20 games. However, what ifthe evidence had been am
biguous? Suppose I had won two-thirds of my games. An observed win
rate of0.66isbelowthepredictedwin rate of0.75butnotdramaticallyso.
Was this merelya random negative deviation from the predicted win rate
or was the deviation of sufficient magnitude to indicate the hypothesis
about my tennis ability was faulty? This is where statistical analysis be
comes necessary. It attempts to answer the question: Was the difference
betweentheobservedwinrate (0.66)andthewinratepredictedbymyhy
pothesis (0.75) large enough to raise doubts about the veracity ofthe hy
pothesis? Or, alternatively: Was the difference between 0.66 and 0.75
merely random variationin thatparticularsample oftennis matches?The
hypothesis test attempts to distinguish prediction errors that are small
enough to be the resultofrandomsanlplingfrom errorssolargethatthey
indicateafaulty hypothesis.
Dueling Hypotheses: The Null Hypothesis versus
the Alternative Hypothesis
Ahypothesis test relies on the method ofindirect proof. That is, it estab
lishesthetruthofsomethingbyshowingthatsomethingelseisfalse. There
fore, toprovethe hypothesisthatwewouldliketo demonstrateas correct,
we show that an opposing hypothesis is incorrect. To establish that hy
pothesisA istrue, weshowthatthe opposinghypothesisNot-A isfalse.



==================================================
                     PAGE 238                     
==================================================

222 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
A hypothesis test, therefore, involves two hypotheses. One is called
the null hypothesis and the other the alternative hypothesis. The names
are strange, but they are so well entrenched that they will be used here.
The alternative hypothesis, the one the scientist would like to prove, as
sertsthe discoveryofimportantnewknowledge. Theopposingornull hy
pothesis simply asserts that nothing new has been discovered. For
example, Jonas Salk, inventorofthe polio vaccine, putforward the alter
native hypothesis that his new vaccine would prevent polio more effec
tively than a placebo. The null hypothesis asserted that the Salk vaccine
would notpreventpoliomore effectivelythanaplacebo. Forthe TArules
tested in this book, the alternative hypothesis asserts the rule has an ex
pected return greaterthan zero. The null hypothesis asserts that the rule
doesnothaveanexpectedreturn greaterthanzero.
Forpurposes ofbrevity, Iwilladoptthe conventionalnotation: H for
A
the alternative hypothesis, and H for the null hypothesis. A way to re
o
member this is the null hypothesis asserts that zero new knowledge has
beendiscovered, thusthesymbolH '
o
ltis crucialto the logic ofa hypothesis testthatH and H be defined
A o
as mutually exclusive and exhaustive propositions. Whatdoes this mean?
Two propositions are said to be exhaustive if, when taken together, they
cover all possibilities. H and H cover all possibilities. Either the polio
A o
vaccinehasapreventiveeffectoritdoesnot. Thereisnootherpossibility.
EitheraTArule generatesreturns greaterthanzerooritdoes not.
The two hypotheses must also be defined as mutually exclusive. Mu
tuallyexclusivepropositionscannotbothbetrueatthesametime, soifH
o
is shown to be false, then H must be true andvice versa. By defining the
A
hypotheses as exhaustive and mutually exclusive statements, ifit can be
shown that one hypothesis is false, then we are left with the inescapable
Null Alternative
Hypothesis Hypothesis
Rule Return Rule Return
LessThan GreaterThan
OrEqualto Zero
Zero
I~IGURE 5.1 Mutuallyexclusive and exhaustive hypotheses.



==================================================
                     PAGE 239                     
==================================================

Hypothesis Tests and Confidence Intervals 223
conclusion that the other hypothesis must be true. Proving truth in this
fashion is called the method of indirect proof. These concepts are illus
trated inthe Figure5.1.
RATIONALE OF 'rUE HWOTUESIS TEST
Two aspects of the hypothesis test warrant explanation. First, why the
test is focused on the null hypothesis. Second, why the null hypothesis is
assumed tobetrueratherthanthealternative hypothesis. Thissectionex
plainsthe reasoning behind bothaspects.
Why Is the ull Hypothesis the Target
ofthe Test?
As discussed in Chapter3, evidence can be used to logically deduce that
a hypothesis is false, but it cannot be used to deduce that it is true.3
Therefore, hypothesis testing must be about trying to falsify something.
The question is: Which ofthe hypotheses, H or HAl should be the target
o
ofthis effort?
Ofthe two competing claims, H presents a bettertargetfor falsifica
o
tion because itcan be reduced to asingle claim aboutthe value ofthe pa
rameter. This means that only one test must be performed. Ifthat single
value can be successfully challengedwith evidence, H will have been fal
o
sified. In contrast, the alternative hypothesis represents an infinite num
ber ofclaims about the parameter's value. With no unique value to shoot
at, an infinitenumberoftestswouldhavetobeperformedtofalsify theal
ternative hypothesis.
In fact, both H and H represent an infinite number ofclaims about
o A
the rule's expected return, butH can be reduced to a single claim. First,
o
let's consider why H represents an infinite number of claims. In assert
A
ing that the rule's return is greater than zero, H effectively says that the
A
rule's expected return, overthe immediatepracticalfuture, mightbe any
one ofan infinitesetofvaluesgreaterthan zero: +0.1 percent, or+2 per
centor+6percentorany otherpositivevalue. ThisisillustratedinFigure
5.2. GiventhatH makes an infinite numberofclaims, an infinite number
A
oftests would have to be conducted to conclusivelyrefute it. Clearlythis
isimpractical.
H also makes an infinite number of claims about the value of
o
the population parameter. It asserts that the rule's average return is
equal to zero or some value less than zero. However, only one ofthese
claims really matters-that the rule's average return is equal to zero.



==================================================
                     PAGE 240                     
==================================================

224 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Etc.
••
•
Negative Returns
HI Asserts:
Rule's Expected Return> 0%.
FIGURE 5.2 The alternative hypothesis makes an infinite number of claims
aboutthe rule's expected rate ofreturn.
The attemptto falsify a single claim is a practicalgoal. Ifthe mostposi
tive ofthese claims (return = 0) can be called intoquestion bythe rule's
back-testprofitability, then alllesserclaims (e.g., the rule's return = -1.3
percent) are contradicted butto an even greater degree. This is how H
o
reduces to the single claim that the rule's return is zero. This is illus
tratedinFigure 5.3.
Etc.
•••
Positive Returns
HoAsserts:
Rule's Return <or= 0%.
FIGURE 5.3 The null hypothesis makes an infinite number ofclaims about the
population mean, but only one matters.



==================================================
                     PAGE 241                     
==================================================

Hypothesis Tests and Confidence Intervals 225
Why Is the ull Hypothesis Assumed to Be True?
The hypothesis test assumes H is true on two grounds: scientific skepti
o
cismand the principle ofsimplicity (parsimony).
Skepticism. TheassumptionthatH istrueisconsistentwithscience's
o
skeptical attitude toward all new claims of knowledge. As explained in
Chapter 3, this conservative stance is justified because making claims is
easy butmaking genuine discoveries is hard. The burden ofproofshould
beontheclaimant.
Science has a legitimate concern with protecting the storehouse of
knowledge from contan1ination with falsehoods and weird beliefs. Ause
ful analogy can be made to the criminal justice system. Free societies
have a legitimate concern with protecting the lone citizen from the vast
prosecutorialpowerofthe state. Forthis reason the defendantin a crimi
nal prosecution starts off with a presumption of innocence. This places
the burden ofproving guilt-falsifying the assumption ofinnocence-on
the state, and it is indeed a substantial burden. To gain a conviction, the
state must provide evidence of guilt beyond a reasonable doubt, a very
high threshold. The initial assumption ofinnocence and the high thresh
old ofproofcan be seen as the legal system's way ofpreventing thejails
andgallowsfrom beingcontaminatedwithinnocentcitizens.
Following the scientific tradition, the hypothesis test places the bur
den of proof on those asserting new knowledge. In fact, whenever the
back-tested performance of a rule is greater than zero, the sample evi
dence actually favors HA-that the rule's expected return is greater than
zero. However, the hypothesis testdemands more thanmerelyreasonable
evidence. It demands compelling evidence before abandoning the as
sumed truth of Hoo Those asserting that a TA rule has predictive power
(H ) mustmeetalarge burdenofproofbeforetheycanreasonablyexpect
A
theirclaimto be accepted byscientificpractitioners ofTA.
Simplicity. An additional reason to grantpriorityto H is the principle
o
ofsimplicity. This fundamental preceptofscience says thatsimplertheo
ries are more likelyto capture the true patterns ofnature than more elab
orate ones. This principle, known as Occam:S Razor, says that if a
phenomenoncanbeexplainedbymorethanonehypothesis, thenthesim
plest hypothesis is more likely to be correct. H ' which explains a rule's
o
past success as luck, is simpler than H , which asserts that profits stem
A
from arecurringmarketpatternwithpredictivepower.
Simpler explanations (theories, rules, hypotheses, models, and so
forth) are more likely to be correctbecause they are less likely to fit data
bychance.Themorecomplexanexplanation-thatis, themorenumerous



==================================================
                     PAGE 242                     
==================================================

226 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
itsassumptions, itsconditions, andconstraints-themorelikelyitis tofit
a set of observations by chance. This is illustrated when fitting a mathe
maticalfunction to a setofdata. Afunction can bethoughtofasamathe
maticalhypothesisthatexplainsagivensetofobservations. InFigure 5.4,
two mathematicalfunctions have beenfitted to the same set ofdata. One
function islinear. Itisrelativelysimplein thatitisdefinedbyonlytwo co
efficients or degrees offreedom: its slope and where it cuts the vertical
axis (Y-intercept). In other words, the line's fit to the data can be im
provedbymanipulatingonlythesetwofactors.
The other function is a complex polynomial with 10 coefficients.
Each coefficient allows the curve to make an additional bend. With this
degree of flexibility, the curve can weave and bend so that it touches
every data point. In fact, when a function is fit to data using a method
called least-squares regression, a perfectfit is guaranteed ifthe function
is allowed to contain as many degrees offreedom (coefficients) as there
are data points. Although the linear function does not manage to touch
every observed point, it does describe the general tendency ofthe data;
increasingvaluesofXareassociatedwithincreasingvaluesofY. Inother
words, the simple function captures the essential feature of the data,
making it more likely that it represents the real relationship between X
and Y. Incontrast, the complexfunction ismostlikelya detaileddescrip
tion ofthe data's random fluctuations in addition to the positive correla
tionbetweenX and Y.
...•...
y
'.
x
FIGURE 5.4 The superiority ofsimplicity-all else being equal.



==================================================
                     PAGE 243                     
==================================================

Hypothesis Tests and Confidence Intervals 227
This is notto suggestthatcomplex curves are neverjustified. Ifthere
is enough data and it was generated by a complex process, an elaborate
model may very well bejustified. However, all else being equal, the sim
plerexplanationismore likelyto becorrect.
Strong and Weak Decisions
Ahypothesis testleadsto one oftwo decisions; rejectH orretain H 'The
o o
decisionsarequalitativelydifferent: thefirst isa strongdecisionwhilethe
second is a weak one.4 Rejection is a strong decision because it is com
pelled byimprobable, infomlativeevidence thatforcefully contradicts H '
o
Incontrast, the decisionto retainH isa relativelyweakerchoicebecause
o
the evidence is nothing more than consistent with what we had already
expected and assumed to be so, that H is true. In others words, the ob
o
servedvalue ofthe teststatistic is unsurprising and hence uninformative.
Had testsofSalk'spoliovaccineshownthatrecipientsoftherealvac
cine had the same risk ofgettingthe disease as recipients ofthe placebo,
H would be leftintact, and no onewould have beensurprisedbythevac
o
cine'sfailure. Nothing had worked up to thatpoint, so itwould have been
justanotherfrustrating day atthe lab. Ofcourse this is not whatactually
happened, and the course ofmedical historywaschanged. Salk'sdecision
to rejectH was compelled by a rate ofinfection in the treated group that
o
wassurprisinglylessthantheplacebogroup.
Itisinthiswaythata decisionto rejectH isfundamentallyastronger
o
decision than one to retain H ' It is a decision forced by evidence that is
o
strong enough to rebut an initial and entirely reasonable doubt that new
knowledge has been discovered. In contrast, the decision to retain H is
o
dueto anabsence ofcompellingevidence. Theabsenceofcompellingevi
dence does notmeanthe nullis necessarilytrue orevenprobablytrue.5It
simplymeans that it could be true. Because science takes a conservative
stancetowardnew knowledge, intheabsenceofcompellingevidence, the
morereasonable conclusionisthatnothingnewhas been discovered.
IIWOTHESIS TESTING: THE MECIIANICS
"Three ingredients are usually necessary for a hypothesis test: (1) a hy
pothesis, (2) a test statistic, and (3) somemeans ofgeneratingthe proba
bility distribution (sampling distribution) of the test statistic under the
assumption that the hypothesis is true."6 The term test statistic refers to
thesamplestatisticthatis beingused totesta hypothesis. Thus, theterms
are interchangeable.



==================================================
                     PAGE 244                     
==================================================

228 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
To recapthese itemsastheypertainto ruletesting: (1) the hypothesis
R isthatthe rulehasanexpectedreturnofzeroorless, (2) theteststatis
o
tic isthe rule'smeanreturnobtainedbybacktestingitina historicalsam
ple of data, and (3) the sampling distribution represents the random
variationinthe rule'smean return ifitwere to betested in manyindepen
dent samples. The sampling distribution is centered at a mean return of
zero, reflectingtheassumptionassertedbyR '
o
Asdiscussed in Chapter4, thesamplingdistributioncanbe derived in
two ways: the analytical approach ofclassical statistics andviacomputer
simulation. There are two computer-based approaches: Monte-Carlo per
mutationandthe bootstrap. Bothwill be used inthe casestudypresented
inPartTwo.
The Question: Is the Test Statistic Improbable?
The basic idea of a hypothesis test is simple: an outcome (observation)
that would rarely happen under the condition that the hypothesis were
true is good evidence that the hypothesis is not true."7 Ifmy hypothesis
thatI am agood tennis playerwere true, then itwould be rare (improba
ble)formetolose20gamesinarow. TomysurpriseandembarrassmentI
did lose 20gamesina row. Thatevidenceimpliesmyhypothesisisfalse.
Suppose a TA rule called MA50 is back tested. The MA50 rule is de
fined asfollows: IfS&P500closeisgreaterthana 50-daymovingaverage,
hold a longpositionin the S&P 500, otherwise hold a short position. The
alternativehypothesis (R assertsthatthe rule haspredictivepowerand
)
A
is, therefore, expected to earn a rate of return greater than zero on de
trended data.s Zero is the rate ofreturn expected for a rule with no pre
dictive power. The null hypothesis (R ) asserts that the MA50's expected
o
return is equal to zero or less. Ro's assertion is depicted in Figure 5.5.
Value ofthe Population Parameter
Asserted by H
o
Negative Returns o Positive Returns
FIGURE 5.5 Value ofpopulation parameterasserted by Ho'



==================================================
                     PAGE 245                     
==================================================

Hvpothesis Tests and Confidence Intervals 229
The horizontal axis represents expected return overthe immediateprac
ticalfuture.
MA50'smeanreturnisobtainedby backtestingtheruleinasampleof
historical data. When thisvalue isplotted onthe same axis as the hypoth
esizedvalueforpopulationparameter, wegetFigure 5.6.
ote that in Figure 5.6 there is a positive deviationbetweenthe value
predicted byH and theperformanceofthe rule obtained in the backtest.
o
This raises the question: Is the positive deviation so surprising that H
o
should berejected asanimplausiblehypothesis?
There are two possible explanations for the positive deviation. It
couldsimplybeduetosamplingerror-therule gotluckyintheparticular
sampleofdatausedforback-testing-or, itcouldbebecausethehypothe
sizedvalue ofzero is wrong-the rule does have predictive powerand its
expected return isindeed greaterthan zero. The objective ofthe hypothe
sistestis to determine ifthe evidence, specificallythe size ofpositive de
viation, issufficientlyrare, surprising, orimprobablethatitwouldwarrant
a rejectionofH .
o
To assess the improbability of the evidence, the observed average
return, in particular its deviation from the hypothesized value, is evalu
ated in light ofthe sampling distribution. Recall, the sampling gives the
probability for various sized deviations between the observed value of
the sample statistic and its expected value due to sampling error. Ifthe
observed value's deviation is greater than what could reasonably be at
tributed to sampling error, then H is rejected and the alternative hy
o
pothesis, HAl is adopted. In other words, we conclude that the rule has
predictivepower.
Figure5.7givesusanintuitivesenseofthevaluesofthesamplestatistic
Value ofthe Population Parameter
Asserted by H
o
Sample Statistic
Back-Tested
Performance
ofthe rule
Deviation
~
Negative Returns o Positive Returns
FIGURE 5.6 Hypothesized value of population parameter compared to back
tested performance.



==================================================
                     PAGE 246                     
==================================================

230 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Value ofthe Population Parameter
Asserted by H
o
Sample Statistic
Sampling Distribution
for
Mean Return
Deviation
Negative Returns o Positive Returns
FIGURE 5.7 Unsurprising evidence lies well within the range ofsampling varia
tion; Honot rejected.
thatcould occurdue tosamplingvariability. Note thatthesamplingdistri
butionispositionedsothatthe mostlikelyvalueofthereturn iszero. This
merelyreflects thevalueofthepopulationparameterasserted byH' Also
o
notethattheobserved(backtested)valueoftherule'saveragereturnfalls
wellwithinthe range ofrandomvariationallowed bythesamplingdistrib
ution. This is an unsurprising result. In other words, the deviation be
tween the observed value of the test statistic and the hypothesized
(predicted) value could easily be due to sampling error. Hence the evi
dence is notstrongenoughtowarranta rejectionofH '
o
Figure 5.7makesitclearthatthe widthofthesamplingdistributionis
critical in deciding ifthe deviation between the observed return and the
hypothesized return is large enough to warrant a rejection ofHO' We saw
in Chapter 4 that the width ofa statistic's sampling distribution is deter
mined bytwofactors: (1) the amountofvariation within the parentpopu
lation which gave rise to the sample, and (2) the number ofobservations
comprising the sample. With respect to the first factor, the greater the
variabilityofthe datacomprisingthe population, inthis case dailyrule re
turns, thelargerthewidthofthesamplingdistribution. Withrespectto the
second factor, the largerthe numberofobservations comprising the sam
ple, thesmallerthe widthofthesamplingdistribution.
In Figure 5.8 the sampling distribution is relatively narrow. The ob
servedvalue ofthe sample statistic lies in the outer right tail ofthe sam
pling distribution. This would be considered an improbable orsurprising
observation and one that would be incompatible with the hypothesized
value. Suchevidencewouldwarranta rejectionofH '
o



==================================================
                     PAGE 247                     
==================================================

Hypothesis Tests and Confidence Intervals 231
Value ofthe Population Parameter
Asserted by H
o
Sample Statistic
Sampling Distribution
for
Mean Return
Deviation
Negative Returns o Positive Returns
FIGURE 5.8 Surprising (improbable) evidence at the outer edge ofthe range of
sampling variation; H rejected.
o
Thesediagramsconveyanintuitivesenseofhowthesizeofthe devia
tion between an observedvalue and a hypothesized value (the value pre
dicted by the hypothesis) is used to falsify a hypothesis. To be rigorous,
this intuitionmust be quantified. This is done bytranslating the observed
value'sdeviationintoaprobability-specificallytheprobabilityofobserv
ing a deviation that large underthe conditionthatthe hypothesizedvalue
is true. A probability that is contingent on the existence of a specified
condition, inthiscasethatthehypothesizedvalueistrue, iscalledacondi
tionalprobability. Said differently, a conditionalprobabilityis a probabil
itythatisconditionaluponsomeotherfactbeingtrue.
In a hypothesis test, this conditional probability is given the special
namep-value. Specifically, itis the probability that the observedvalue of
the test statistic could have occurred conditioned upon (given that) the
hypothesisbeingtested(H ) istrue. Thesmallerthep-value, the greateris
o
ourjustification for calling into question the truth ofH ' Ifthe p-value is
o
lessthanathreshold, whichmustbe definedbeforethetestiscarriedout,
H is rejectedand H accepted. Thep-value can also beinterpreted as the
o A
probabilityH will beerroneouslyrejectedwhenH isinfact true. P-value
o o
also has a graphical interpretation. It is equal to the fraction ofthe sam
pling distribution's total areathat lies at values equal to and greaterthan
the observedvalue ofthe teststatistic.
Let's considerhow all ofthis pertains to the test ofa rule. Forexam
ple, if a rule's return in a back test was +3.5 percent, we mark the value
+3.5 percent on the horizontal axis upon which the sampling distribution
sits. We then determine the fraction ofthe sampling distribution's area



==================================================
                     PAGE 248                     
==================================================

232 METHODOLOGICAL. PSYCHOLOGICAL, PHILOSOPHICAL. STATISTICALFOUNDATIONS
occupyingvalues equal to orgreaterthan +3.5 percent. Suppose that this
areaequals 0.10 ofthe sampling distribution's total area. Thevalue0.10 is
the sample statistic's p-value. This fact is equivalent to saying that ifthe
rule's true return were zero, there is a 0.10 probability that its return in a
back test would attain a value as high as +3.5 percent or higher due to
samplingvariability(chance). ThisisillustratedinFigure 5.9.
p-value, Statistical Significance, and
Rejecting the Null Hypothesis
Asecond name forthep-value oftheteststatisticisthestatisticalsignif
icance of the test. The smaller the p-value, the more statistically signifi
cant the test result. Astatistically significant result is one for which the
p-value islowenoughtowarrantarejectionofH
w
The smaller the p-value ofa test statistic, the more confidentwe can
bethatarejectionofthe null hypothesisisa correctdecision. Thep-value
can be looked upon as the degree to which the observedvalue ofthe test
statistic conforms to the null hypothesis (H )' Larger p-values mean
o
greaterconformity, and smallervalues mean less conformity. This is sim
plyanotherwayofsayingthatthe moresurprising (improbable) an obser
vation is in relation to a given view of the world (the hypothesis), the
morelikelyitis thatworldviewisfalse.
How small does the p-value need to be to justify a rejection ofthe
H ? This is problem specific and relates to the cost that would be in
o
curred by an erroneous rejection. We will deal with the matteroferrors
and their costs in a moment. However, there are some standards that
Null Hypothesis
&
Sampling Distribution
Mean Return
TestStatistic:+3.5%
p-value= 0.10
o
Area =0.10oftotal~sampllngdistribution
FIGURE 5.9 P-Value: fractional areaofsampling distribution greaterthan +3.5%.
conditional probabilityof+3.5%or more given that H is true.
o



==================================================
                     PAGE 249                     
==================================================

Hypothesis Tests and Confidence Intervals 233
are commonly used. A p-value of 0.10 is often called possibly signifi
cant. Ap-value of0.05 or less is typically termed statistically significant
and is usually considered to be the largestp-value that wouldgive a sci
entist license to reject H ' When the p-value is 0.01 or less it is called
o
called very significant and values of 0.001 or less are termed highly
significant.
Test Conclusions and Errors
Ahypothesis test leads to one oftwo possible conclusions: (1) reject H '
o
or (2) accept H ' Whatis actually true also has two possibilities: (l) H is
o o
true, thatis, the expected return ofthe rule iszero orless); or(2) H is in
o
fact false, thatis, the rule's expected return isgreaterthan zero because it
possesses some degree of predictive power. Given that the test has two
possible conclusions, and the truthhas twopossiblestates, there arefour
possible outcomes. They can be illustrated by a table with four cells
showninFigure 5.10.
The hypothesis test can errin two ways. A typeI erroris saidto oc
curwhena low p-value leads us to rejectH ' butin reality H is true. This
o o
would be the casewhen a rule is truly devoid ofpredictive power, butby
luck itgenerates a sufficiently profitable back test that its p-value is low
enough tojustify rejecting H ' This is a case ofthe rule researcher being
o
fooled by randomness. The second type of error, called a type II error,
occurs when a high p-value leads us to retain H ' when it is in fact false.
o
In other words, the back test fooled us into concluding the rule has no
predictive power, but it actually does and its expected return is greater
than zero.
Truth
(The Reality Only Known to God)
H oTrue H oFalse
RuleReturn<=0 RuleReturn>0
High CorrectDecision TypeIIError
P·value TARuleUseless TARuleGood
H o WeDiscard It WeDon'tUse It
Test Not OpportunityLoss
Result Rejected
&
Conclusion low TypeIError CorrectDecision
P-value TARuleUseless TARuleGood
H WeUseit WeUseRule&
o EarnZeroReturn MakeProfits
Rejected
&TakeRisk
FIGURE 5.10 Possible outcomes ofa hypothesis test.



==================================================
                     PAGE 250                     
==================================================

234 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Atthe time the hypothesis testis conducted, only God knows for cer
tain ifan error has occurred and ofwhich type. Mere mortals relying on
statisticalinference mustacceptthe realitythatthe test's conclusionmay
bein error.
From the standpoint ofan objective technician, the two types ofer
rors have different consequences. Atype I error, where H is mistakenly
o
rejected, leadsto the use ofa worthless rule. Thisexposes tradingcapital
to riskwithouttheprospectofcompensation.Atype IIerrorcausesa use
ful rule to be ignored, resulting in lost trading opportunities. Of the two
kinds of error, type I is more serious. Lost trading capital is worse than
lost opportunities. When capital is exhausted one is out of the game,
whereasthere willalways be othertradingopportunities.
The hypothesis testcan also be right in two ways: a correctrejection
ofH ' when the rule does have merit, and a correctacceptance ofthe H'
o o
when the TArule is useless.
COMPUTER-INTENSIVE METHODS FOR GENERATING
THE SAMPLING DISTRIBUTION
As previously mentioned, hypothesis testing requires a method for esti
mating the shape ofthe sampling distribution ofthe test statistic. There
are two ways to do this: the traditional approach ofmathematical statis
tics, and the more recently developed computer-intensive randomization
methods. Thissection discusses two computer-basedmethods: bootstrap
pingand theMonteCarlopermutation.
Both the traditional and computer-intensive approaches solve the
problem of estimating the degree of random variation in a test statistic
when there is only a single sample of data and, therefore, only a single
value ofthe teststatistic. As previouslystated, a single value ofastatistic
cannotconveyasenseofitsvariability.
Computer-intensive methods estimate the sampling distribution's
shape by randomly resampling (reusing) the original sample ofobserva
tion so as to producenewcomputer-generated samples. Ateststatistic is
then computed for each resample. This procedure can be repeated as
manytimesasdesired, perhapsthousandsoftimes, thusproducingalarge
set ofvalues for the sample statistic. The sampling distribution is devel
oped from this large set of computer-generated values. It might seem
strange that reusing the original sample of observations over and over
again would allowoneto approximatethevariabilityofasamplestatistic,
yetitdoes! Notonlydoes this workquite wellinpractice, the approach is
groundedinsound mathematicaltheory.



==================================================
                     PAGE 251                     
==================================================

Hypothesis Tests andConfidence Intervals 235
The two computer-intensive methods, the bootstrap and Monte
Carlo permutation, are similar in that they both rely on randomization.
That is, they randomly resample the original sample. However, the two
methods are different in several important respects. First, they test
slightly different versions of H ' Although, in both, H asserts that the
o o
rule being tested has no predictive power, they do so in slightly differ
entways. TheH tested bythe bootstrap asserts thatthe populationdis
o
tribution of rule returns has an expected value of zero or less. In
contrast, the H tested by the Monte Carlo permutation method asserts
o
that the rule's output values (+1 and -1) are randomly paired9 with fu
ture market price changes. In otherwords, itasserts that the rule's out
putis uninformative noise thatcould havejustas easily been generated
by a roulette wheel.
Because the bootstrap and the Monte Carlo methods test somewhat
different versions ofthe null hypothesis, they require different data. The
bootstrap utilizes a daily history ofrule returns. The Monte Carlo uses a
daily history of the rule's output values (Le., a sequence of +1 and -l's)
and adailyhistoryofprice changesfor the marketbeingtraded.
The two methods also use different random sampling methods. The
bootstrap uses a randomization method called resampling with replace
ment, whereas the Monte Carlo randomly pairs rule output values with
market returns without replacement. This distinction will be clarified in
the descriptionofeachmethod'salgorithm.
Because ofthese differences, the methods generate somewhat differ
entsamplingdistributions. Therefore, itispossible thatthe conclusionto
rejectornotto rejectH maynotalways bethe same. However, extensive
o
simulations conducted by Dr. Timothy Masters, developer of the Monte
Carlo permutation method, show that both methods generally do agree
when they are applied to detrended market data. Forthis reason, the hy
pothesistests conducted in this book use detrended marketdatafor both
the bootstrap and MonteCarlomethods.
Thefinal distinction ofthe Monte Carlopermutationmethodis thatit
is in thepublic domain, whereas the bootstrap method thatissuitablefor
rule testing isa patentedproductthatisavailable onlyfrom itsdeveloper,
Quantmetrics.1O
The Bootstrap
The bootstrap method was first described by Efronll in 1979 and then re
fined in several laterpublications citedin Eric Noreen's ComputerInten
sive Methodsfor Testing Hypotheses.12 The bootstrap derives a sampling
distribution ofthe test statistic by resampling with replacement from an
originalsample.



==================================================
                     PAGE 252                     
==================================================

236 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The bootstrap is based on a truly amazing mathematical fact, the
bootstrap theorem. Amathematicaltheorem deduces a previouslyunrec
ognized truth from the established theorems and foundational assump
tions (axioms) of a mathematical system. Assuming that certain
reasonable conditionsaresatisfied, the bootstraptheoremassures usthat
itwill converge to a correctsamplingdistributionas the sample size goes
toinfinity. Fromapracticalstandpoint,thismeansthatgivenasinglesam
ple of observations bootstrapping can produce the sampling distribution
neededto testthesignificance ofaTArule.
Initsbasicform, thebootstrapisnotsuitablefor evaluatingthestatis
ticalsignificanceofrules discovered bydatamining. However, amodifica
tion invented and patented by Dr. Halbert White, professor ofeconomics
atthe University ofCalifornia, San Diego, extended the application ofthe
bootstrap to rules discovered by data mining. This modification to the
bootstrap, which is incorporated in software called "Forecaster's Reality
Check,"isdiscussedinChapter6and utilizedin Chapter9to evaluatethe
statisticalsignificanceofover6,000rulesfortradingthe S&P500.
Bootstrap Procedure: White's Reality Check. The description
thatfollows pertains to the use ofbootstrapping in the context oftesting
the statisticalsignificance testofa single TArule. Thus, the following de
scriptiondoes notaddressthe issue ofdatamining. Figure 5.11 illustrates
Resample2
1,231 Observations
BSMean=-1.8
ResampleS,000
1,231 Obse=rvations
BSMean -2.6
5000
Resampled Means
1------ 0.8
2------ -1.8
3------ 1.9
•
•
•
SOOO--- -2.6 Bootstrapped Sampling Distribution
OfThe Mean
FIGURE 5.II How bootstrapping produces the sampling distribution for the
sample mean return.



==================================================
                     PAGE 253                     
==================================================

Hypothesis Tests and Confidence Intervals 237
the bootstrap procedure. The double arrows between each resample and
the original sample indicatethat the sampling is being done with replace
ment(explained below).
There areseveralthings worthyofnoteinFigure 5.11. First, the origi
nalsample, representedbythelargeoval,iscomprisedofthedailyreturns
earned by the rule on detrended data. As discussed in Chapter 1, the de
trendedmarketdatahasanaverage dailychange ofzero.
Second, before the resampling operation begins, the daily returns of
the rule are adjusted by a procedure called zero-centering, not to be con
fused with the detrending. Thezero-centeringadjustmentmakesthemean
dailyreturn ofthe ruleequaltozero. Inotherwords, iftherulewasableto
earn a nonzero return on detrended data, its returns must be zero cen
tered. This serves the purpose of bringing the daily returns into confor
mity with the H ' which asserts that their average value is equal to zero.
o
This step is accomplished byfirst computing the mean daily return ofthe
rule and then subtracting that mean value from each daily rule return.
Once the rule's daily returns have been zero centered in this fashion we
are in a position to generate a sampling distribution thatconforms to Ho's
assumption.
Third, the number of daily observations comprising each resample
mustbeexactlyequaltothenumberofobservationsinthe originalsample.
The bootstrap theorem only holds true if the number of observations in
eachresample is equal to the number ofobservations in the original sam
ple. In the figure, the original sample is composed of 1,231 observations.
Thus, eachbootstrappedresampleisalsocomprisedof1,231 observations.
Fourth, each resample is produced by sampling with replacement.
Thismeansthataftera dailyrulereturnhasbeenselectedatrandomfrom
the original sample and itsvalue has been noted, it is then replaced back
intothe originalsamplebeforeanotherdailyreturn isselectedatrandom.
This makes it possible for an individual daily return to be selected more
than onceorforitnevertobeselectedatallforagivenresample.Itisthis
elementofrandomnessthatenablesthebootstrapproceduretomodelthe
variabilityin thesamplestatistic.
Fifth, the diagramshows5,000resanlples beingtaken. Ameaniscom
puted for each resample. These 5,000 means are used to construct the
samplingdistributionofthemean.
The sequence ofstepsinvolved in bootstrappingthe sampling distrib
utionofthe meanis:
t. Calculate the mean daily returnfor the rule overthe observations in
the originalsample (1,231 inFigure 5.11).
2. Zero centering: Subtract the mean daily return from each day's re
turn inthe originalsample.



==================================================
                     PAGE 254                     
==================================================

238 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
3. Placethe zero-centered dataina bucket.
4. Selecta dailyreturn atrandomfrom the bucketand noteitsvalue.
5. Place that return back in the bucket, and then thoroughly stir the
bucket (some statisticians prefer their samples shaken rather than
stirred).
6. Perfornlsteps4and 5exactly N- 1more times (e.g., 1,230) creating
a total ofN (1,231) randomlyselected observations. This completes
thefirst bootstrappedsample.
7. Compute the mean return for the N (1,231) observations in the first
resample. Thisis onevalue ofthe bootstrappedmean.
8. Perform steps 6through 9a large numberoftimes (5,000) yieldinga
large numberofbootstrappedmeans.
9. Formthesamplingdistributionofthe means.
10. Comparethe observedmeanreturnoftheruleto thesamplingdistri
butionand determine thefraction ofthe 5,000meanscomprisingthe
sampling distribution that exceed the observed mean return of the
rule to determine thep-value. SeeFigure5.12.
Monte Carlo Permutation Method (MCP)
Monte Carlo simulation, invented by Stanislaw Ulam (1909-1984), is a
general method for solving mathematical problems by random sampling.
The Monte Carlo permutation method for rule testing was developed by
Dr. Timothy Masters. He is the first to propose this approach as a way to
Null Hypothesis
0%
Average
Return
of
Tested
Rule
p-value = 0.04
o
FIGURE 5.12 Comparing the rule's mean return to the bootstrapped sampling
distribution ofthe mean.



==================================================
                     PAGE 255                     
==================================================

Hypothesis Tests and Confidence Intervals 239
produce the sampling distribution to test the statistical significance ofa
rule's back-tested performance. Itispresentedasanalternative to White's
realitycheck.
Although the Monte Carlo method has been in existence for a long
time, it had not been previously applied to rule testing. This was made
possiblebyDr. Masters' insightthatthe Monte Carlomethodcouldgener
atethesamplingdistributionofa rulewithnopredictivepower.Thisisac
complishedbyrandomlypairingorpermutingthe detrended dailyreturns
ofthe market (e.g., S&P 500) with the ordered13times series representing
the sequence ofdaily rule outputvalues. Recall that the H tested by the
o
MonteCarlopermutationmethodassertsthatthereturns ofthe rule being
evaluated are a sample from a population ofreturns that were generated
bya rule with no predictivepower. The daily returns ofsucha rule can be
simulated by randomly pairing the rule's output values (+1 and -1) with
the market's price changes. The random pairing ofthe rule outputvalues
with market changes destroys any predictive power that the rule may
have had. Ireferto thisrandom pairingasanoiserule.
The process ofrandomly pairing market-price changes with rule out
putvalues is illustratedin Figure 5.13. The daily timeseries ofrule output
valuesaresimplythosethatwereproducedbytherulethatisbeingevalu
ated in their original order. After the sequence ofoutputvalues has been
randomly paired with what is effectively a scrambled version ofthe mar
ket's history, the mean daily return of the noise rule can be computed.
Thisvalue appears in the greyboxatthe end ofeachrow. To producethe
samplingdistribution weneed manysuchvalues.
TimeSeriesof
RuleOutputValues +1 +1 +1 +1 -1 -1 ·1 ·1 -1 -1
Randomized SPSOO Mean
Return, -0.8 +0.3 -0.9 -2.6 +3.1 +1.7 -0.8 -2.6 +1.2 -0.4 Return,
Randomized Rule -0.62
Return, -0.8 +0.3 -0.9 -2.6 -3.1 -1.7 +0.8 +2.6 -1.2 +0.4
Randomized SPSOO -0.4 +1.2 -2.6 -0.8 +1.7 +3.1 -2.6 -0.9 +0.3 -0.8 Mean
Returnz Returnz
Randomized Rule -0.34
Returnz -0.4 +1.2 -2.6 -0.8 -1.7 -3.1 +2.6 +0.9 -0.3 +0.8
Randomized SPSOO Mean
Return -2.6 +1.7 -0.4 -0.9 -0.8 -0.8 +0.3 +1.2 -2.6 +3.1 Return
3 3
Randomized Rule -0.26
Return -2.6 +1.7 -0.4 -0.9 +0.8 +0.8 -0.3 -1.2 +2.6 -3.1
3
FIGURE 5.13 Monte Carlo permutation method.



==================================================
                     PAGE 256                     
==================================================

240 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
To produce additional values of noise rule mean returns, the same
timeseries ofactualrule outputvaluesispairedwith (permutedwith) nu
merousscrambled(randomized)versionsofmarket-pricechanges. Theil
lustration shows only three Monte Carlo permutations but in practice it
would be done a large number oftimes, perhaps 5,000. The 5,000 values
for average return are then used to form the sampling distribution ofthe
mean returnearnedbya noise rule-arulewithnopredictivepower.
Pl'ocedure. The sequence of steps to generate the sampling distribu
tion bytheMonte Carlopermutationmethodisasfollows:
1. Obtain a sample of one-day market-price changes for the period of
time over which the TA rule was tested, detrended as described in
Chapter1.
2. Obtain the time series ofdaily rule outputvalues overthe back-test
period. Assume for this illustration that there were 1,231 such val
ues, onerule outputvalueforeachdayonwhichthe rulewastested.
3. Place the market's detrended one-day-forward price changes on a
pieceofpaper. Placethem ina binandstir.
4. Randomlyselecta market-price changefrom the binandpairitwith
the first (earliest) rule output value. Do not put the price change
back in the bin. In other words this sampling is being done without
replacement.
5. Repeatstep 4until all the returns in the bin have been paired with a
rule outputvalue. In this example there will be a total of 1,231 such
pairings.
6. Compute the return for each of the 1,231 random pairings. This is
done bymultiplyingtherule's outputvalue (+1 for long, -1 forshort)
bythemarket'sone-day-forwardpricechange.
7. Computethe average returnforthe 1,231 returns obtainedinstep6.
8. Repeatsteps4through 7a largenumberoftimes (e.g., 5,000).
9. Formthesamplingdistributionofthe5,000valuesobtainedinstep8.
10. Placethetestedrule'srateofreturn onthesamplingdistributionand
compute thep-value (thefraction ofrandom rule returns equalto or
greaterthanthe testedrule'sreturn).
Application ofComputer Intensive Methods
to Back-Test ofa Single Rule
This section demonstrates the application ofthe two computer-intensive
hypothesis testing methods to a single rule: 91 day channelbreakoutl4 us-



==================================================
                     PAGE 257                     
==================================================

Hypothesis Tests and Confidence Intervals 241
ingthe DowJonesTransportationIndex(inputseries4)astherule's input
series. This rule, which is designated as TT-4-91, and all others tested in
Part1\voofthis book are described indetailin Chapter8. Themainpoint
for current purposes is to demonstrate the hypothesis test. The rule was
used to generate long and short signals on the S&P 500 index from No
vember 1980throughJune 2005. Overthis period oftime, the rule earned
an annualized return of 4.84 percent using detrended S&P 500 data to
computetherule'sdailyreturns. Theexpectedannualreturnofarulewith
nopredictivepoweriszeroondetrendeddata.
Both the bootstrap and the Monte Carlo permutation methods were
used to testthe null hypothesisthatthe rule has nopredictivepower. The
question is this: Is TT-4-91's +4.84 percent return sufficient to reject the
nullhypothesis?
Testing Rule Performance Using Bootstrap: White's Reality
Check. To generatethesampling distributionofthe average return, the
specificstepstakenstartingwiththe detrended S&P500dataare:
1. Zero Centering the Daily Rule Returns: Because the rule generates a
positive return (+4.84 percent) on the detrended market data, the av
erage daily return ofthe rule (approximately 0.0192 percentper day)
is subtracted from the return earned by the rule each day. This trans
formation creates a set ofdaily returns whose average value is zero,
thereby making the data conform to Ho- Note-this is not to be con
fused with the detrendingofthe S&P500data.
2. Resampling the Daily Returns: The zero-centered daily returns as
computed in the preceding step are sampled with replacement. This
must be done exactly 6,800 times (the number ofobservations in the
originalsample)for the BootstrapTheoremto hold true.
3. ComputetheMean Return: Themeandailyreturniscomputedfor the
6,800resampled returns. Thisisthefirst bootstrappedmean.
4. Repeatsteps 2and 35,000 times. This obtains 5,000values for the re
sampledmean.
5. Createthe bootstrappeddistribution ofresampled means.
6. Compare rule's +4.84 percent return to the sampling distribution to
determine the fraction ofthe sampling distribution's area that lies at
values equalto orgreaterthan +4.84percentperannum. This is done
by counting the fraction of the 5,000 bootstrapped means that have
valuesequalto orgreaterthanthisreturn.
Results: 11 Reiected-Rule Possibly lias Predictive Power.
0
Figure 5.14 shows the bootstrapped samplingdistribution with the actual



==================================================
                     PAGE 258                     
==================================================

242 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
0.12 ,-------------------------,
TI-4-911
0.10 ---------------------------------------..-.....-.--.--.-- ... +4.84% 1-----------------------------
0.08 ---.------.--.-.- --.-------.------.--.
0.06 ------.---------------------------------------
p~~;~~e
0.04 -----------------------.------.--.-.-.-. II 1-------------------
I(
0.02
I•.
I I I I o I I I I
-0.15 -0.10 -0.05 0.05 0.10 0.15
FIGURE 5.14 Bootstrapped sampling distributionforruleTI-4-91 and its p-value.
performance of rule TT-4-91 superimposed. The p-value of 0.0692 indi
cates that .069 ofthe 5,000 bootstrapped means were equal to or greater
than +4.84 percent. This means that if the rule's expected return were
trulyequalto zero, about 7times out of100, the rule would earn a return
of +4.84 percent or greater simply due to chance (sampling variability).
Statisticianswould regard such a resultaspossiblysignificant. As will be
seeninChapter6, theamountofsearchingthatledtothediscoveryofthe
rule, specifically whether the rule was found amongst a large group of
back-testedrules, canimpactthe p-value and the rule's significance. Con
sequently, the results quoted here assume that this was the only rule pro
posedforbacktesting.
l'esting Rule Performance with Monte ~arlo Permutation
Method. The following steps describe the process of applying the
Monte CarloPermutationMethodto arule'sback-testedperformance:
1. The time series ofdaily rule outputvalues is laid outin its properse
quence over the time period of the rule back test. As stated earlier
therewillbe6,800ofthese daily +1and-1values.
2. Each of the 6,800 detrended one day forward price changes is in
scribed ona ball. These6,800ballsareplacedina bin.



==================================================
                     PAGE 259                     
==================================================

Hypothesis Tests andConfidence Intervals 243
3. The bin is shaken, and then, one ata time, the balls are drawn at ran
domandpairedwithanindividualruleoutputvalue.Thus, eachofthe
rule's daily output values is matched with a single one day forward
S&P 500 return. This is done without replacement until all balls have
been paired with a rule outputvalue. Because there are exactly 6,800
daily market returns and 6,800 rule output values, the bin will be
emptywhenthisstep iscomplete.
4. Multiply each rule value (+1 or-1) by the associated S&P 500 return.
This gives the return that would be earned by the noise rule overthe
nextday. Thisstepwill produce6,800dailyrule returns.
5. Average the 6,800 values obtained in step 4. This is the first Monte
Carlopermutedmean returnfora noise rule.
6. Steps3through 5mustbe repeated 5,000times.
7. Producethesamplingdistributionfrom the 5,000Monte Carlomeans.
8. Compare the return ofthe TT-4-91 rule withthe sampling distribution
and determinethefraction ofMonteCarlomeansthatequalorexceed
the return earned bythe rule. Thisisthep-value.
Monte Carlo Results: Rule Possibly lias Predictive Power.
Confirmingthebootstrappedresult, theMonteCarlomethodgivesap-value
thatisalmostidentical.
ESTIMA1'ION
Estimationis the otherform ofstatisticalinference. Incontrastto hypoth
esis testing, which is oriented to the acceptance or rejection of a claim
made about the value ofa population parameter, estimation's purpose is
to approximate the value ofthe populationparameter. In ourcase, it will
be used to estimatetheexpectedreturn ofa rule.
Point Estimates
There are two kinds ofestimates: pointand interval. Apointestimateis a
singlevalue thatapproximates the populationparameter, for example the
rule has an expectedreturn oj10percent. An intervalestimate is a range
ofvalues withinwhich the populationparameterlies with a givenlevelof
probability. The following statement would exemplify this: The rule's ex
pectedreturn lies within therange5percent to 15percentwithaproba
bility ojO.95.
Actually, we already have beenmaking pointestimates, butthey have



==================================================
                     PAGE 260                     
==================================================

244 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
not been described as such. Every time we compute a sample mean and
use it to approximate the population mean, we are making a point esti
mate. This fact is easily overlooked. Some commonly used point estima
tors are: the mean, median, standard deviation, and the variance. The
estimate iscomputedfrom asampleofobservationstakenfrom thepopu
lation. In otherwords, a point estimate is a sample statistic. The formula
to computeasamplemeanisshowninFigure5.15.
The use ofmeans (averages) isso ubiquitous inTAthatitis takenfor
granted, yetthesamplemeanturnsouttobeanelegantandpowerful esti
mator. It provides a single value that is, in an important sense, the best
(most informative) estimate of the population's mean. This is an impor
tantfact.
Just how informative the sample mean is becomes clear when we
considerthe criteriaused tojudge the quality ofan estimator. Good esti
matorsshould be: unbiased, consistent, efficient, andsuffldent. In terms
ofthesefour criteria, itcanbeshownthatthesamplemeanis the bestes
timatorofthepopulationmean.
15
An estimator is unbiased ifits expected value is equal to the popula
tion value. Said differently, ifan estimatoris unbiased its deviationsfrom
the true population value have an average value of zero. The sample
mean's deviations from the population mean are unbiased. This allows us
to say thata rule's mean return in a historical sample is an unbiased esti
mate ofitsmean return inthe inunediatepracticalfuture.
Another criterion of a point estimator's goodness is its consistency.
An estimatoris said to be consistentifitsvalue converges to the value of
Sample Mean of Variable X
x
n
x-
n
Where Xiis an individual observation on variable X
FIGURE 5.15 Sample mean ofvariable X.



==================================================
                     PAGE 261                     
==================================================

Hypothesis Tests andConfidence Intervals 245
the population parameter as sample size is increased. The Law ofLarge
Numberstellsus thatthis issoforthesamplemean.
Estimatorsshould also beefficient. This criterionrelates to the width
ofitssamplingdistribution. As mentionedearlier, anestimatorisasample
statistic, and thus has a sampling distribution. The most efficient estima
tor is the onethatproduces the narrowestsampling distribution. In other
words, the mostefficientestimatorhasthesmalleststandard error. Both
16
thesample meanand thesamplemedianareunbiasedand consistentesti
mators ofthe population mean for populations that are distributed sym
metrically. However, the sample mean is more efficient than the sample
median. Forlargesan1ples, thestandarderrorofthemeanisabout80per
centsmallerthan thesamplemedian'sstandarderror.
17
The final trait ofa good point estimator is called its sufficiency. "An
estimatorissufficientifitmakessuch use ofall the available sample data
that no other estimator would add any information about the paran1eter
beingestimated."L8Thesample mean issufficientin thissense.
Interval Estimates-The Confidence Interval
More informative than the point estimate is the interval estin1ate, also
knownasaconfidence interval. Itisdescribed in thissection.
What 1)0 Confidence Intervals 'I'ell Us'! Apoint estimate has lim
ited value because it conveys no sense ofthe uncertainty in the estimate
due to sampling error. The confidence interval solves this problem by
combining the infom1ation of the point estimate with the infonnation
abouttheestimator'ssamplingdistribution.
Aconfidenceintervalisa range ofvalues thatsurroundthepointesti
mate. The intervalis defined by upperand lower values called bounds. In
addition, the intervalisaccompaniedbyaprobabilitynumberthattellsus
howlikelyitis thatthe true value ofthepopulationparameterfalls within
the bounds of the confidence interval. By convention the probability is
stated as a percentage rather than a fraction. Thus, a 90 percent confi
dence interval for the mean has a 0.90 probability ofenclosing within its
boundsthepopulation'strue meanvalue.
When thinking about what a confidence interval tells us, it is best to
thinkofwhatwould happenifonewere to constructa large numberof90
percent confidence intervals, each based on an independent sample of
data taken from the population. Ifthis were to be done, about90 percent
of the intervals would actually encompass the value of the population's
parameter. By extension, about 10 percent of the confidence intervals
would fail to include the population parameter. This is illustrated in Fig
ure5.16for 10confidenceintervals. Tenisasmallnumber, whichIusedto



==================================================
                     PAGE 262                     
==================================================

246 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
True Population Mean
•
! 1 •
Means :.
& •
I • I
90%Confidence
I Error
Intervals 1 I • I 90%CI
For • FailsTo
•
10Independent I 1 I Enclose
•
Samples • Population
1 Mean
FIGURE 5.16 90 percent confidence intervals (.90 probabilitycorrect).
keep the figure simple. The true, but unknown, population mean is indi
catedbytheGreekletterJl (mu). Thesamplemeanisidentifiedbythe dot
withineach confidence interval. Note thatone ofthe confidenceintervals
fails to enclosethepopulationmean.
The researcher can choose whatever confidence level is desired. For
example a 99 percentconfidence level contains the true population mean
with a probability of 0.99. Of course, there is a price to be paid for the
higherlevel ofconfidence-theintervaliswider. Inotherwords, theprice
forhigherconfidenceisreducedprecision. NoteinFigure5.17,byincreas
ing the confidence level to 99 percent, the error made by the 90 percent
True Population Mean
:.
•
•
Means • 99%CI
& Correct
99% 1 1-1----411.. 1: -:-------jI l Where
Confidence • 90%CI
Intervals • Was
For 10 • Wrong
•
Independent
Samples
FIGURE 5.17 99 percent confidence intervals (.99 probabilitycorrect).



==================================================
                     PAGE 263                     
==================================================

Hypothesis Tests and Confidence Intervals 247
confidence interval in Figure 5.16 has been eliminated. The reduced error
ratewasaccomplishedbyusingawider(lessprecise)confidenceinterval.
Ifa rule were to be backtested on one hundredindependentsamples
ofdata and a 90 percent confidence interval were constructed about the
mean return observed in each sample, approximately 90percent ofthese
confidence intervals would contain the rule's true or expected return in
the population. Figure 5.18 shows a 90 percent confidence interval for a
rule thatearneda7percentreturn ina backtest. Wecanbecertain,witha
probabilityof0.90, thatthe rulehasanexpectedreturnbetween2percent
and 12percent.
l'he Conlidence Interval and Its Connection to the Sampling
Distribution. Confidence intervals are derived from the same sam
pling distribution that is used to compute the p-values for a hypothesis
test. Given whatwe have alreadyleamed aboutsampling error(sampling
variability), it can be said that the value ofa sample mean is equal to the
unknown value ofthe population mean, plus orminus its sampling error.
This relationship is shown by the upper formula in Figure 5.19. By rear
rangingthe terms ofthisformulawegetthe lowerformula in Figure 5.19.
Itsaysthat the value ofthe population mean is equal to the known value
ofthesamplemeanplusorminussamplingerror.
The bottom formula tells us that, although we do not know the pre
cise value ofthe population mean, we can take the value ofthe sample
mean, which we do know, and the sampling distribution of the mean,
whichwealso know, and obtaina range ofvalues thatcontainsthe popu
lation mean with a specified level of probability. In operational terms,
this tells us that if we were to repeat the following procedure 1,000
times-compute a samplemean and a 90percent confidence interval-
Rule Return
Was 7% In
Historical Sample
7%
Rate of Return
FIGURE 5.18 90 percent confidence interval for rule backtest.



==================================================
                     PAGE 264                     
==================================================

248 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The known sample mean is
the unknown population mean with error
x
= ~ +/- Sampling Error
The unknown population mean is
the known sample mean with error
~ = X +/- Sampling Error
FIGURE 5.19 The known sample mean is the unknown population mean with
error.
the populationmeanwouldlie withinapproximately900ofthe 1,000con
fidence intervals. This concept is illustrated when the procedure is re
peated only 10 times in Figure 5.20. Note that one of the 10 confidence
intervals fails to include the population mean. The pointofthis section is
that the confidence interval's width is derived from the width ofthe sam
plingdistribution.
Aswas said earlier, the confidence intervalis based onthe same sam
pling distribution that is used in the hypothesis test. However, in the case
ofthe confidenceinterval, thesamplingdistributionissimplyshiftedfrom
the position it occupies in a hypothesis test. In the hypothesis test, the
samplingdistribution is centered atthe hypothesizedvalue ofthe popula
tionmean,forexample, zero. Inthe caseofaconfidenceinterval, thesam
pling distribution is centered over the sample mean, for example, 7
percent. ThisconceptisillustratedinFigure5.21.
Generating Confidence Intervals with the Bootstrap. Boot
strapping can be used to derive confidence intervals. The procedure is al
mostidenticalto the one used to generate the sampling distribution for a
hypothesis test.
There are numerous methods for computingbootstrap confidence in
tervals. Theonepresentedhere, the bootstrappercentilemethod, ispopu
lar, easy to use, and generally gives good results. More sophisticated
methodsarebeyondthescope ofthistext.
It should be pointed out that the Monte Carlo permutation method
cannot be used to generate confidence intervals. This is because the



==================================================
                     PAGE 265                     
==================================================

Hypothesis Tests andConfidence Intervals 249
Sampling
Distribution
ofthe Mean
confidenceIntervals
Derived from
Sampling Distribution
)
•
•
I
Means
& I .: I
90%Confidence 1-1 --II
---4••:
Intervals •
I
for •
•
10 Independent
•
Samples •
u
FIGURE 5.20 The connection between the confidence interval and the sampling
distribution.
Hypothesized
Mean Return
Value
in
Population
Historical Sample
Parameter
Sampling Distribution Sampling Distribution
Positioned for Positioned for
HypothesisTest Confidence Interval
0% 7%
FIGURE 5.21 The sampling distribution positioned for a hypothesis test and
positioned for aconfidence interval.



==================================================
                     PAGE 266                     
==================================================

250 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
method has nothing to do with estimating the value ofa populationpara
meter or testing a claim made about its value. As previously mentioned,
the Monte Carlo permutationmethod tests a claim about the information
content ofthe rule's signals. Specifically, the H asserted underthe Monte
o
Carlopermutationmethodis thatthe longand shortpositions dictated by
therulearedevoidofusefulinformationaboutfuturemarketchanges. Be
cause there is no reference to a populationparameter (e.g;, mean rule re
turn) there is nothingto createa confidenceintervalfor!
The bootstrap percentile procedure for constructing the confidence
intervals works as follows: Suppose the rule's returns have been resam
pled 5,000 times and a mean is computed for each resample. This would
result in 5,000 different values for the resampled mean return. We know
thatbecauseofsamplingvariability, thesemeanswilldiffer. Nextsuppose
thatthesetof5,000valuesisarranged inrankorderfrom highestmeanto
lowest. Then, depending on the confidence interval desired, the highestx
percent and lowest >x percent ofvalues are removed from the ordered
list, where
x = 100- Confidence Interval Desired
2
So if a 90 percent confidence interval is desired, one would remove
thehighest5percentandthelowest5percentofthevaluesinthe5,000re
sampledmeans. This would require removing the highest 250 and the 250
lowestvalues ofthe resampled mean. After these extreme values are re
moved, the highestremainingresampled meanwould bethe upperbound
ofthe90percentconfidenceinterval and the lowestremaining resampled
meanis the lowerbound.The 99 percentconfidence intervalwould result
by removing only the highest 25 (top .5 percent) and the lowest 25 (bot
tom .5 percent) from the set of 5,000 resampled means comprising the
samplingdistribution.
Hypothesis Tests versus Confidence Intervals: "otential Con
Diet. Someastutereadersmayhaveenvisionedaproblem.Itispossible
for a hypothesis testand a confidence interval to lead to differentconclu
sions about a rule's expected return. This prospect stems from the fact
thata hypothesis testfocuses onthe righttailofthesamplingdistribution
whereas the confidence interval focuses on the left tail of the sampling
distribution. This meansitispossible forthe lower bound ofa 90 percent
confidence interval to imply that there is a 5 percent probability that a
rule's expectedreturnisless than zero, while a hypothesistestconducted
atthe .05significancelevelrejectsH ' Inotherwords, itispossibleforthe
o



==================================================
                     PAGE 267                     
==================================================

Hvpothesis Tests and Confidence Intervals 251
confidence intervalto tell us thatthe rule does nothave predictivepower
while the hypothesis tells us that it does. In theory, the hypothesis test
and the confidence interval should come to the same conclusion on this
issue. Thatis to say, ifthe lower bound ofa 90 percent confidence inter
val tells us that the rule's expected return is less than 0 percent, then a
hypothesis test at the .05 significance level would presumably not reject
the null hypothesis.
Conflicting conclusions can arise when the sampling distribution is
not symmetrical (i.e., is skewed to the right or left). This is illustrated in
Figure5.22. Thesamplingdistribution, whichis clearlyskewedtothe left,
is shown in two positions. In the lower portion of Figure 5.22, the sam
pling distributionis positioned as it would be for conducting a test ofH '
o
Because less than 5 percent of the sampling distribution's right tail lies
abovethemeanreturnoftheback-testedrule, thetestindicatesthe rule is
significant at the .05 level. In other words, H can be rejected in favor of
o
thealternative hypothesis, H thatclaimsthe rule hasanexpected return
,
A
thatisgreaterthan zero.
The upperportion ofFigure 5.22 shows the sampling distribution as
it would be positioned to construct a 90 percent confidence interval us
ing the bootstrap percentile method. Note that the lower bound of the
confidence interval is below a zero rate of return. This tells us there is
greater than a .05 probability that the rule's true rate of return is less
than zero. In other words, the 90 percent confidence interval leads to a
conclusionthat is opposite to the conclusion ofthe hypothesis test. The
Mean Back-Test Return
Historical Sample
LowerBound
90%Confidence SamplingDistribution
IntervalBelow0 Positionedfor
ConfidenceInterval
Sampling
Distribution Lessthan.05
Positionedfor ofSamplingDistribution
Testof AboveMeanReturn
NullHypothesis
o 10
FIGURE 5.22 Potentially conflicting conclusions: hypothesis test versus confi
dence interval.



==================================================
                     PAGE 268                     
==================================================

252 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
figure shows that the ambiguity is due to the asymmetrical shape ofthe
sampling distribution.
Fortunatelythis problem does not afflict the research conducted in
this book, which uses the sample mean as a performance statistic. The
all-importantCentral LimitTheorem assures us thatthe sampling distri
bution ofthe mean will not be seriouslyskewed (asymmetrical) as long
as the sample size is large. To recap, the Central LimitTheorem tells us
that, as sample size increases, the sampling distribution of the mean
tends toward a symmetrical bell shape. Other performance statistics
may not behave this way. In situations where the sampling distribution
is notsymmetrical, there are bootstraptechniques, whichinvolvepivot
ing the sampling distribution to alleviate this problem. Unfortunately,
these other methods can have problems oftheir own. In any case, this
all makes the mean return an attractive performance statistic to use in
rule testing.
Confidence Intel'Vals fol' the '1"1~4·91 Itllie. This section gives an
example ofthe confidenceintervalfor the rule TT-4-91. Figure 5.23 shows
the 80 percent confidence interval superimposed on the bootstrapped
0.1 2 r-------------,----,----------,
.-- 80%--+
0.10
0.08 --------------------------------------------------------------------
0.06 ------------------------------------------------------------------
0.04 ------------------------------------------------------------
0.02 ------------------.--..----..--.--..- ---- --..
o
-0.15 -0.10 -0.05 0.05 0.10 0.15
FIGURE 5.23 Sampling distribution and 80 percent confidence interval for
rule TT-4·91.



==================================================
                     PAGE 269                     
==================================================

Hypothesis Tests and Confidence Intervals 253
sampling distributionpositioned atthe rule's observed back-testedreturn
of+4.84percent. The lowerbound ofthe80percentconfidence interval is
+0.62 percent. The upper bound is +9.06 percent. This tells us that ifrule
TT-4-91 were tobebacktestedon 100independentsamplesofdataand an
80 percentconfidence interval were to be placed around the mean return
in each sample, in approximately 80 ofthe samples, the true expected re
turn ofthe rulewould beenclosedbythe confidenceinterval.



==================================================
                     PAGE 270                     
==================================================





==================================================
                     PAGE 271                     
==================================================

Data-Mining
Bias: The Fool's
Gold of
Objective TA
In rule datamining, manyrules are backtested andthe rule with the best
observed performance is selected. Thatis to say, data mining involves a
perfonnance competition that leads to a winning rule being picked. The
problem is that the winning rule's observed performance that allowed itto
be picked overall otherrulessystematicallyoverstateshow well the rule is
likelytoperforminthefuture. Thissystematicerroristhe data-miningbias.
Despite this problem, data mining is a useful research approach. It
can be proven mathematically that, out of all the rules tested, the rule
with the highest observed performance is the rule most likely to do the
best in the future, provided a sufficient number ofobservations are used
to compute performance statistics.l In other words, it pays to data mine
even though the best rule's observed performance is positively biased.
This chapter explains why the bias occurs, why it must be taken into ac
count when making inferences about the future performance of the best
rule, and howsuchinferencescanbemade.
Ibegin byintroducing this somewhatabstract topic with several anec
dotes, onlyone ofwhichis related to rule datamining. Theyare appetizers
intended tomakelatermaterial more digestible. Readers whowanttostart
on the main course immediately may choose to skip to the section titled
"DataMining."
Thefollowing definitionswillbe used throughoutthischapterandare
placedhereforthe convenienceofthe reader.
• Expected performance: theexpectedreturnofa rule inthe immedi
ate practical future. This can also be called the true performance of
the rule, whichisattributable to itslegitimatepredictivepower.
255



==================================================
                     PAGE 272                     
==================================================

256 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
• Observedperformance: the rateofreturnearnedbya rule inaback
test.
• Data-mining bias: the expected difference between the observed
performance ofthe bestrule andits expectedperformance. Expected
difference refers to a long-run average difference that would be ob
tained by numerous experiments that measure the difference be
tweenthe observed return ofthebestrule and the expected return of
the bestrule.
• Data mining: the process oflooking for patterns, models, predictive
rules, andsoforthinlargestatisticaldatabases.
• Best rule: the rule with the best observed performance when many
rules are backtested andtheirperformancesare compared.
• In-sample data: the datausedfor datamining(Le., rule backtesting).
• Out-of-sample data: datanotusedinthe dataminingorback-testing
process.
• Rule universe: the full set of rules back tested in a data mining
venture.
• Universe size: the numberofrules comprisingthe rule universe.
FALLING INTO THE PIT: TALES OF THE
DAm-MINING BIAS
This following account is apocryphal. Some years ago, before I took up
thestudyofstatistics, Iwasapproached byanimpresarioseekingbackers
for a show business venture that he claimed would be enormously prof
itable. The showwas to feature a monkeythatcould write Shakespearian
prose bydancingonthe keyboard ofa wordprocessor.
Ateachshow,theliteratemonkey,whomthepromoterhad namedthe
Bard, would be placed ata keyboard and a large screen would display to
an audience what the monkey wrote, as it was being written. Surely, peo
plewould rush tosee the Bard, andmyshare ofticketsaleswould yield a
handsome return onthe required investmentof$50,000. Atleast, thatwas
thepromoter'sclaim. "Itcan'tmiss!" washisrefrain.
I was intrigued, but wanted some proofthat the monkey could actu
ally produce Shakespearianprose. Anyinvestorwould. Iwas given proof
... ofa sort. Itwas whataccountantscall a cold-comfortletter. The letter
said, "We have examined the Bard's previous works and he has in fact
written the words 'To be or not to be, that is the question.' We are, how
ever, unfamiliar with the circumstances under which these words were
written."
What I really wanted was a live demonstration. Regrettably, my



==================================================
                     PAGE 273                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 257
requestcould notbe accommodated. The impatientpromoterexplained
the monkey was temperamental and besides, there were many other
anxious investors clamoring to buy the limited number ofshares being
offered. So I seized the opportunity and plunked down $50,000. I was
confident it was just a matter of time before the profits would start
flowing in.
The night of the first show arrived. Carnegie Hall was packed to ca
pacity with a crowd that anxiously awaited the first words. With every
one'seyesglued to thebigscreen, the Bard'sfirst line oftextappeared.
lkasldlk5jfwo44iuldjs skOek 123pwkdzsdidip'adipjasdopiksd
Things went downhill quickly from there. The audience began
screaming for refunds, I threw up, and the Bard defecated on the key
board before scampering offthe stage. Myinvestmentwent up in a cloud
ofsmoke.
What happened? Thinking it unimportant, the promoter failed to
disclose a key fact. The Bard had been chosen from 1,125,385 other
monkeys, all of whom had been given the opportunity to dance on a
keyboard every day for the past 11 years, 4 months, and 5 days. Acom
puter monitored all their gibberish to flag any sequence ofletters that
matched anything ever written by Shakespeare. The Bard was the first
monkey to everdo so.
Evenin mystateofstatisticalilliteracy, IdoubtIwould haveinvested
had I known this. Mere common sense would have told me that chance
alone favored the occurrence of some Shakespearian quote in such a
large mass ofnonsense. The freedom to data mine the trillions ofletters
generated by an army of monkeys raised the probability of a lucky se
quence ofletters to a virtual certainty. The Bard was not literate, he was
justlucky.
The fault, Dear Brutus, lay with the sincere but statistically naIve
promoter. He was deluded by the data-mining bias and attributed too
much significance to a result obtained by data mining. Despite my loss,
I've tried not to judge the promoter too harshly. He sincerely believed
that he had found a truly remarkable monkey. He was simplymisled by
intuition, a faculty inadequate for evaluating matters statistical and
probabilistic.
By the way, the promoter has kept the Bard as a pet and still allows
him to dance on thatonce-magical keyboard in hopes ofnew evidence of
literacy. In the meanwhile, to keep bodyandsoul together, he is nowsell
ing technical trading systems developed along similar lines. He has a
throngofdancingmonkeysdevelopingrules,someofwhichseemtowork
quitewell, inthe historicaldata.



==================================================
                     PAGE 274                     
==================================================

258 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Proving the Existence ofGod with Baseball Stats
Collectors ofsports statistics have also been seduced bythe data-mining
bias. For example, there is Norman Bloom, who concluded that interest
ing and unusual patterns found in baseball statistics prove the existence
ofGod. After thousands ofsearches through his database, the dedicated
dataminerfoundpatternshebelievedtobesoamazingtheycouldonlybe
explainedbya universe madeorderlybyGod.
One ofBloom'spatternswas asfollows: George Brett, the third base
manfor Kansas City, hit his third home run in the third game oftheplay
offs, to tie the score3-3. Bloomreasonedthat, forthe numberthree to be
connected in so many ways, compelled the conclusion it was the handi
work ofGod. Anotherinterestingpattern discovered by Bloom had to do
with the stock market: The Dow Jones Industrial Average crossed the
1,000 level 13 times in 1976, miraculously similar to the fact that there
were 13originalcoloniesthatunitedin 1776toform theUnitedStates.
As pointed out by Ronald Kahn,2 Bloom committed several errors on
the way to his unjustified conclusions. First, he did not understand the
role ofrandomness and thatseeminglyrare coincidences are infact quite
probable if one searches enough. Bloom found his mystical patterns by
evaluating thousands ofpossible attribute combinations. Second, Bloom
didnotspecifywhatconstituted animportantpatternbeforehe beganhis
searches. Instead, he tookthe libertyofusing an arbitrary criterion ofim
portance defined after the fact. Whatever struck his fancy as interesting
and unusual was deemed to be important. Kahn points out that one is
guaranteed to discover "interesting" patterns when they are searched for
insuchanundisciplined manner.
Discovering Hidden Predictions in
the Old Testament
Even Bible scholars have fallen into the data mining pit. In this instance,
the well intentioned but statistically unsophisticated researchers found
predictions ofmajor world events encoded in the text of the Old Testa
ment. Knowledge ofthe future would certainly imply that the words had
been inspired by an omniscient Creator. However, there was one small
problemwiththesepredictions, knownas Bible Codes. Theywerealways
discovered after the predicted eventhad taken place. In otherwords, the
codespredictwith20/20hindsight.3
The Bible Codes are clusters ofwords imbedded in the text that are
discovered by linking together letters separated by a specific number of
interveningspaces orotherletters. These constructed words are referred
to as equal letter sequences or ELS. Code researchers grant themselves



==================================================
                     PAGE 275                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 259
the freedom to try any spacing interval they wish and allow the words
comprising the cluster to be arranged in virtually any configuration so
longas the clusteroccurs in whatthe researcher deems4to be a compact
region of the original text. What constitutes a compact region and what
words constitute a prediction are always defined after a code has been
discovered. Note the use ofan evaluationcriterion defined after thefact.
Thisisnotscientificallykosher.
The Bible Code scholars contend that the occurrence ofa code is so
statistically unlikely that it can only be explained by its having been put
there by God. Their fundamental error-the error made by all naive data
miners-is the failure to understand that given enough searching (data
mining), theoccurrenceofsuchpatternsisactuallyhighlyprobable.Thus,
it is likely that researchers will find codes that correspond to names,
places, and events ofhistorical importance. For example, the word 1990
in the same region of text as Saddam Hussein and war are not rare
events requiring a metaphysical explanation. However, when found in
1992, after the first Iraqwar had taken place, the word pattern seemingly
predictedthe 1990Iraqwar. Bearinmind thatthewordsBush, Iraq, inva
sion, and desert storm would servejust as nicely as a code that also ap
pears to predict the 1990 Iraq war. Indeed, there are a huge number of
word combinations thatwould correspond to the 1990war, after the par
ticularsofthathistoricaleventare known.
Inhis 1997book, TheBibleCode, authorMichaelDrosnin, ajournalist
with no formal training in statistics, describes the research ofDr. Eliyahu
Rips. Dr. Rips is an expert in the mathematics ofgroup theory, a branch
of mathematics that is not particularly relevant to the problem of data
mining bias. Though Drosnin claims that the Bible Codes have been en
dorsed by a roster of famous mathematicians, 45 statisticians who
reviewed Rips's work found it to be totally unconvincing.5 Data-mining
biasis, atits heart, aproblemoffaulty statisticalinference.
Statisticians take a dim view of the unconstrained searching prac
ticedbyBibleCoderesearchers. Itcommitsa mathematicalsincalledthe
excessive burning ofdegrees offreedom. To the statistical sophisticate,
the stench produced by this incineration is most foul. As pointed out by
Dr. Barry Simon, "A Skeptical Look at the Torah Codes,"6 in Chumash,
just one of the 14 books comprising the Torah, approximately 3 billion
possible words can be produced from the existing text when the ELS
spacing interval is allowed to vary from 1to 5,000. Searching this set of
manufactured words for interesting configurations is no different than
searchingthrough tons ofgibberishwrittenbyanarmyofmonkeysdanc
ingonkeyboards.
The foolishness of the Bible Code scholars' search algorithms be
comes apparent when they are applied to non-Biblical texts of similar



==================================================
                     PAGE 276                     
==================================================

260 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
length, such as the Sears catalogue, Moby Dick, Tolstoy's War andPeace,
orthe Chicagotelephone directory. Whenthese textsaresearched, coded
afterthefactpredictionsofhistoricaleventsarealsofound. Thissuggests
the codes are a by-productofthesearchmethod and notofthe textbeing
searched.
In a more recent book by Drosnin, The Bible Code II: The Counl
down, he shows how the codes "predicted" the terrible events of9/11/01.
Why, you ask, didn't he warn us before the events happened? He did not
because he could not. He discovered the predictions afterthe tragedyoc
curred. Drosnin is another example of a well-intentioned but naive re
searcherfooled bythe data-miningbias.
Data-Mining Newspaper Reporters
Newspaperreportershavealso beenduped bythe data-miningbias. Inthe
mid 1980s, they reported the story of Evelyn Adams, who had won the
New Jersey state lottery twice in four months.7 Newspaper accounts put
the probabilityofsuch an occurrence at 1in 17trillion. Infact, the proba
bility of finding a double winner was far higher and the story far less
newsworthythanthe reportershadthought.
The before-the-fact(apriori) probability that Ms. Adams orany other
individual will win the lottery winner twice is indeed 17 trillion to one.
However, the after-the-fact probability of finding someone who has al
ready won twice by searching the entire universe ofall lottery players is
far higher. Harvard statisticians Percy Diaconis and Frederick Mosteller
estimatedtheprobabilityto be about 1in30.
Thequalifierafter-the-factisthekey. Itreferstoaperusalofdataafter
outcomes are known. Just as the probability that any individual monkey
will in the future produce a Shakespearian quote is extremely small, the
probability that there exists some monkey, among millions of monkeys,
that has already produced some literate prose, is substantially higher.
Givenenoughopportunity, randomnessproducessomeextraordinaryout
comes. The seeminglyrare isactuallyquite likely.
Mining the UN Database for Gold
and Finding Butter
David J. Leinweber, on the faculty of California Institute of Teclmology
and formerly a managIng partner at First Quandrant, a quantitative pen
sion management company, has warned financial market researchers
about the data-mining bias. To illustrate the pitfalls of excessive search
ing, he tested several hundred economic tin1e series in a UN database to



==================================================
                     PAGE 277                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 261
find the one with the highest predictive correlation to the S&P 500. It
turned outto be the level ofbutterproduction in Bangladesh, with a cor
relation ofabout0.70, an unusually high correlationinthe domain ofeco
nomicforecasting.
Intuition alone would tell us a high correlationbetweenBangladesh
butter and the S&P 500 is specious, but now imagine ifthe time series
with the highest correlation had a plausible connection to the S&P 500.
Intuition would not warn us. As Leinweber points out, when the total
number of time series examined is taken into account, the correlation
between Bangladesh butter and the S&P 500 Index is not statistically
significant.
The bottom line: whether one searches sport statistics, the Bible, the
random writings ofmonkeys, the universe oflottery players, or financial
market history, data mining can lead to false conclusions ifthe data min
ing biasisnottaken intoaccount.
THE PROBLEM OF ERRONEOUS KNOWLEDGE
IN OBJECTIVE TECHNICALANAIXSIS
TA is comprised oftwo mutually exclusive domains-subjective and ob
jective. To recap, objectiveTAis confined to methods thatare sufficiently
well defined that they can be reduced to a computerized algorithm and
backtested. Everythingelseis, bydefault, subjectiveTA.
Bothdomains are contaminatedwith erroneous knowledge, butthe
falsehoods are ofvery different types. The loosely defined propositions
ofsubjective TA carry no cognitive freight, generate no testable predic
tions,S and are, therefore, shielded from empirical challenge. Without
this crucialprocedure for excisingworthlessideas, falsehoods accumu
late. Asa result, subjectiveTAis notalegitimate bodyofknowledge but
a collection offolklore resting on a flimsy foundation of anecdote and
intuition.
Objective TA methods have the potential to bevalid knowledge but
only if back-test results are considered in light of randomness (sam
pling variability) and data-mining bias. Because many objective practi
tioners are unaware of these effects, falsehoods accumulate in this
domain as well.
Erroneous objective TA manifests as out-of-sample9 performance de
terioration-arule thatperformswellin the sample usedfor backtesting
butthenperformsworseinout-of-sampledata. Thisproblemisillustrated
inFigure6.1.



==================================================
                     PAGE 278                     
==================================================

262 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Cumulative
Gains
Observed
$
Performance
50%. ROI
In-Sample Out-of-sampie
............................~
Time
1?IGURE 6.1 Out-of-sample performance deterioration.
Explanations for Out-of-Sample Deterioration:
Old and New
Out-of-sample performance deterioration is a well-\mown problem. Ob
10
jective technicians have proposed a number ofexplanations. Ipropose a
relativelynewone, basedondata-miningbias.
Theleastplausibleexplanationattributestheproblemtorandomvari
ation. Althoughitistruethata rule'sperformance willvaryfrom onesam
pleofhistoryto anotherdue tosamplingvariability, thisexplanation does
noteven fit the evidence. Ifrandom variation were responsible, then out
of-sample performance would be higher than in-sample performance
about as frequently as it is lower. Anyone experienced with back testing
\mowsthatout-of-sampleperformance isinferiorfarmoreoften.
Asecond rationale is that the market's dynamics changed when the
out-of-sample period began. It is reasonable to assume that financial
markets are nonstationary systems.ll However, it is not reasonable to
assume thateachtime a rulefails outofsample itis because marketdy
namics have changed. It would be odd, almost fiendish, for the market
to always change its waysjustwhen a rule moves from the technician's
laboratory to the real world oftrading. It is simply implausible to sug
gest that a market's dynamics change as frequently as rules fail out of
sample.
An even more elaborate explanation, also based on changing market
dynamics, invokestheadditionalassumptionthatthe changeoccurredbe
cause the rule has been adopted by too many traders. This story asserts
thatthe buying and selling ofthe traders using the rule has destroyed the



==================================================
                     PAGE 279                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 263
marketpatternthataccountedfortherule'sback-testprofitability. Thisra
tionale also lacksplausibility. Given thatthere are analmostinfinitenum
ber of rules that could be formulated, it would be unlikely that a large
enough number of participants would adopt any particular rule. Even
when numerous traders adopt similar rules, as is the case with futures
trading funds that employ objective trend-following methods, reduced
rule performance seems to be due to changesin marketvolatilitythatare
notrelatedtotheusageoftechnicalsystems.
12
A more plausible explanation13 of out-of-sample performance break
downis basedonrandomness. Itasserts thatmarketdynamicsare a com
bination of systematic or patterned behavior and randomness or noise.
Valid rules exploitthe market's systematic features. Because the system
atic component should continue to manifest in out-of-sample data, it is a
dependablesource ofruleprofits. Incontrast, the random componentisa
nonrecurringphenomenonthatwill manifestdifferentlyineachsampleof
data. It is reasoned that a portion ofa rule's in-sample performance was
luck-a coincidental correspondence between the rule's signals and the
market's nonrecurring noise. These lucky profits will not manifest out of
sample, and sofuture performance willfall below pastperformance. This
explanationisclosertothemark, butitisincomplete. Randomnessisonly
onevillain.
Amorecompleteaccountofout-of-sampleperformancedeterioration
is based on the data-mining bias. It names two villains: (1) randomness,
which is a relatively large component of observed performance and (2)
the logic ofdatamining, in which a best-performing rule is selected after
the back-testedperformances ofall tested rules are available for the data
miner's examination. When these two effects combine, they cause the ob
servedperformance ofthebestrule to overstateitsfuture (expected) per
formance. Thus, itis likelythatthe bestrule's future performance will be
worsethanthe levelofperformancethatcausedittobeselectedfromthe
universe ofrulestested.
The data-mining bias explanation for out-of-sample performance
deteriorationissuperiorto the one basedonchangedmarketdynamics.
Both explanations fit the datain the sensethat they are both consistent
with the fact that out-of-sample performance tends to be worse. How
ever, the latterexplanationinvokesthe assumption that market dynam
ics have changed. The data-mining bias explanation does not. It simply
says that the process of data mining favors the selection of rules that
benefited from good luck during the back test. When choosing among a
set ofexplanations that are equally adept at accounting for a phenome
non, itis wisest to choose the one making the fewest assumptions. This
is the principle of simplicity known as Occam's Razor, which was dis
cussed in Chapter3.



==================================================
                     PAGE 280                     
==================================================

264 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
As will be shown, the observed performance ofa rule can be decom
posed into two independent components: randomness and the rule's in
herent predictive power. Of these two, randomness is by far the more
dominant component. Thus, the rule with the best-observedperformance
was most likely the beneficiary ofgood luck. That is to say, randomness
boostedthe rule'sperformanceabove the levelattributable to itstruepre
dictive power, if it possessed any. The opposite is the case for the rule
with the worst observed performance. It was most likely negatively im
pacted by luck. By selecting the rule with the highest observed perfor
mance, the data miner winds up picking a rule that experienced a large
component of good luck. Because luck cannot be counted on to repeat,
thisrule's out-of-sampleperformanceislikelytosettlebacktowardalevel
thatis representative ofits inherentpredictivepower. Thismakesitprob
ablethatout-of-sampleperformancewillbelowerthanthe levelofperfor
mance that allowed the rule to win the performance competition. The
bottom line: out-of-sample performance deterioration ofthe best rule is
most probably a fall from an unrealistically high expectation rather than
anactualdeclineinthe rule'spredictivepower.
DATA HI ING
Data mining is the extraction of knowledge, in the form of patterns,
rules, models, functions, and such, from large databases. The limita
tions andbiases ofhumanintelligence make this task nearly impossible
for the unaided mind when the knowledge involves multiple variables,
nonlinear relationships, orhigh levels ofnoise. Thus, datamining relies
oncomputerizedalgorithmsfor knowledge extraction. An excellentdis
cussion of the principal methods used in data mining can be found in
Elements ofStatistical Learning,J4 Predictive Data Mining: A Practi
cal Guide,15 and Data Mining: Practical Machine Learning Tools and
Techniques.
16
Data Mining as a Multiple Comparison Procedure
Data mining is based on a problem solving approach called a multiple
comparison procedurel7 (MCP). The basic idea behind an MCP is to test
many different solutions to the problem and pick the one that performs
thebestaccordingtosomecriterion.Threeelementsare required to apply
an MCP: (1) a well-defined problem, (2) a set ofcandidate solutions, and
(3) a figure of merit or scoring function that quantifies the goodness of
each candidate's performance. After all scores are in, they are compared



==================================================
                     PAGE 281                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 265
and the candidate with the highest score (best-observed performance) is
selectedas the bestsolutionfortheproblem.
Considerhowthisproblem-solvingparadigm appliesin the contextof
rule datamining:
1. The problem: timinglongandshortpositionsinafinancialmarket to
generateprofits.
2. A set ofcandidate solutions (solution universe orsolution space):
asetofrulesproposed bythe objectivetechnician.
3. Figure ofmerit: a measure offinancial performance such as the av
erage rate ofreturn overa historical test period, Sharpe ratio, return
to UlcerIndex.
IS
Theperformancesofall rules aredetermined bybacktesting, andthe
rule achieving the highestperformanceisselected.
Rule Data Mining as a Specification Search
Datamining can be understood as a specificationsearch. Thatis to say, it
isahuntforthespecificationsoftherule thatproducesthehighestperfor
mance. The specifications are a setofmathematical and/or logical opera
tions applied to one or more market data series thus transforming them
intoa timeseriesofmarketpositionsdictated bythe rule.
Suppose the rule defined below turned out to be the best perform
ing rule:
Hold alongpositionin theS&P500iftheratio ofDowJones Trans
portationAverageclosedivided by theS&P500closeisgreaterthan
its 50-daymoving average, elseholda shortposition.
The rule is specified by two mathematical operators: ratio and mov
ing average; two logical operators: the inequality operator greater-than,
the conditionalif, then, else;asinglenumericalconstant:50; andtwo data
series: Dow Jones Transports and S&P 500. Datamining discovered that
this set ofspecifications produced better performance than any alterna
tivesetofspecificationsthatwastested.
Types ofSearches
Datamining searches range from the simple to the sophisticated. One of
the ways in which they differ is how broadly the search universe is de
fined. This section considers three definitions of the search universe,



==================================================
                     PAGE 282                     
==================================================

266 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
startingwiththenarrowestsearchandprogressingtothe broadest. Allap
proaches to data mining, whether they search simply and narrowly or
broadlyusingthemostadvancedmethodsaresubjectto data-miningbias.
'-arameter Optimization. The narrowest form ofdata mining is pa
rameteroptimization. Here, the searchuniverseisconfined to ruleswith
the same form differing only in terms of their parameter values. Thus,
the search is restricted to finding the parametervalue(s) ofthe highest
performingrule ofaparticularform.
Anexampleofaruleformwouldbethedual-moving-averagecrossover
rule.Itisspecifiedbytwoparameters;look-backperiodsfortheshort-term
andlong-termmovingaverage. Signalsaregivenwhentheshort-termmov
ingaveragecrossesabove(buy) and below(sell) alonger-termmovingav
erage. Parameter optimization searches for the pair of parameter values
thatyields thehighestperformance.
The maximum number of dual-moving-average crossover rules that
can besearchedisequaltotheproductofthe numberofvaluestestedfor
the short-term parameter and number ofvalues tested for the long-term
parameter.Asearchthatconsidersallthesecombinationsisreferred to as
anexhaustive orbrute-forcesearch.
There are more intelligentsearch methods that restrict the search to
combinations that are more likely to yield good results. This is accom
plishedbyusingtheperformancesofparametercombinationstestedearly
in the searchto guide the combinations thatwill be tested at laterstages.
One such intelligent search method is the genetic algorithm, a technique
based loosely on the principles of biological evolution. Its demonstrated
ability to find parametercombinations thatare close to optimal relatively
quicklymakesitespeciallyusefulwhen thenumberofpossibleparameter
combinationsis high. Genetic algorithms have also provento be effective
when performance is impacted strongly by randomness, which renders
more conventionalguided search methods based on calculus impractical.
Excellent discussions of optimization methods can be found in Pardo,
\9
and a review ofvarious advanced methods can be found in Katz and Mc
zo z1
Cormick and Kaufman.
Rule Searching. A broaderversion ofdata mining is rule searching.
Here, the universe ofrules differ in theirconceptualfonn as well as their
parametervalues. The dual-moving-average crossover rule is one formal
ism for trend-following. Thus, it is simply one form in the broader cate
gory of trend-following rules that also include channel breakouts,
moving-average bands, and so forth. The general category of trend
following rules is merely one technical analysis category among others
that include counter-trend (mean-reversion) rules,2z extreme value rules,



==================================================
                     PAGE 283                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 267
divergencerules,23diffusionrules, andsoforth. Eachcategorycanbereal
izedwithnumerousspecificrule forms.
PartTwo ofthis book presents a data-mining case studythatis based
onrulesearching. Thestudywillfocus onthreerule categoriesorthemes:
trends, extreme-values and transitions, and divergence. Eachtheme is re
alized withaspecificruleform, whichis definedinChapter8.
Though rulesearchingconsidersamultitudeofruleforms, eachrule's
complexity remains fixed throughout the course ofthe search. Complex
ity refers to the number of parameters required to specify the rule. In
otherwordsrulesearching, as defined herein, doesnotinvolvecombining
simplerules toproducemore complexrules
Rule Induction with Variable Complexity. The broadest and
most ambitious form of data mining is rule induction. Here the search
considers rules of undefined complexity. As the search proceeds, rules
of ever-greater complexity are considered. A complex rule might be
thought ofas a composition ofa simplerrule conjoined bylogical oper
ators or combined with a mathematical function as in a multivariate
model. Thus rule-induction data mining is concerned with finding the
rule ofoptimal complexity.
In contrast to less ambitious forms ofdata mining in which a rule's
complexityis definedatthe outsetofthesearch, rule induction uses ma
chine learning (autonomous induction) to find the degree ofcomplexity
that produces the best performance. One scheme for rule induction
starts by testing individual rules. Next, pairs ofrules are considered to
see if their performance is better than the best individual rule. Gradu
ally, progressively more complex rules are tested and evaluated. In ef
fect, rule-induction methods learn how to combine rules to optimize
performance.
Geneticalgorithms, neuralnetworks, recursivepartitioning, kernel re
gression, support-vector machines, and boosted trees are some of the
methodsusedforthese mostambitiousofdata-miningventures. Anexcel
lent discussion ofvarious methods and supporting statistical theory can
be found in The Elements ofStatistical Learning by Hastie, Tibshirani,
24
and Friedman.
OBJECTIVE 'fA RESEARCH
Thegoalofthe objective technicianisthe discoveryofa rule(s) thatwill
be profitable in the future. The research method is back testing, which
produces an observable measure of performance. On the basis of this



==================================================
                     PAGE 284                     
==================================================

268 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
statistic, an inference is made about a population parameter, the rule's
future or expected performance. It can be said, therefore, that the
essence ofobjectiveTAisstatisticalinference.
Why Objective Technicians MustData Mine
The problem ofout-of-sample perfoffilance failure has encouraged some
practitioners ofobjective TA to reject data mining. This is neither a wise
nor viable position. Today, an objective technician who refuses to data
mine is like the taxi driver who refuses to abandon the horse-drawn car
riage-acharmingrelic butno longersuitablefor getting to one's destina
tionefficiently.
Several factors compel the adoption of data mining as the preferred
method ofknowledge acquisition. First, it works. Experiments presented
later in this chapter will show that under fairly general conditions, the
greaterthe number ofrules backtested the greater the likelihood offind
inga goodrule.
Second, technological trends favor data mining. The cost effective
ness ofpersonal computers, the availability ofpowerful back-testing and
data-mining software, and the availability of historical databases now
make data mining practical for individuals. As recently as a decade ago,
the costslimiteddataminingto institutional investors.
Third, at its current stage of evolution, TA lacks the theoretical
foundation that would permit a more traditional scientific approach to
knowledge acquisition. In developed sciences, such as physics, a single
hypothesis can be deduced from established theory and its predictions
can be tested against new observations. In the absence of theory, a
data-mining approach is called for, in which a multitude ofhypotheses
(rules) are proposed and tested. The risk of this shotgun approach is
fool's gold-rules thatfit the data by accident. Steps that can minimize
this risk are discussed laterin this chapter.
Single-Rule Back-Testing versus Data Mining
Not all back testing is data mining. When just a single rule is proposed
and back tested, there is no data mining. This mode ofresearch is illus
trated inFigure 6.2. Ifthe rule's backtestproves to be unsatisfactory, the
research stops and other more practical means of earning a living are
considered.
Datamining involves the back testing ofmany rules and picking one
based on its superior performance. This process is illustrated in Figure
6.3. Note that as depicted here an initial unsatisfactory performance by



==================================================
                     PAGE 285                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 269
Get
Job
Satisfactory? NO- Working
at
YES McDonalds
I"IGURE; 6.2 Single rule backtesting.
Start
Satisfactory7 NO-----.t Data
Mining
YES
FIGURE; 6.3 Rule Data mining.
thefirstrule tested doesnotstop theresearch. The ruleisrefinedoranew
rule isdefined, and itistested and itsperfonnance isevaluated. The cycle
continues until a rule with good perfonnance is produced. This process
may involve testing tens, hundreds, thousands, or even greater numbers
ofrules.



==================================================
                     PAGE 286                     
==================================================

270 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Legitimate Uses ofObserved Performance
For this discussion, the assumed performance statistic is the mean rate
ofreturn over the back-test period. The observed performance statistic
can play a legitimate role both in single-rule back testing and in data
mining. However, the roles are different. In single-rule back testing, ob
served performance serves as an estimator of future performance. In
datamining, observed performanceserves asa selectioncriterion. Prob
lems arise for the data miner when observed performance is asked to
play both roles.
In single-rule back testing, the tested rule's mean return can legiti
mately be used as an unbiased estimate ofthe rule's expected return. In
otherwords, the tested rule's mostlikelyfuture return isits back tested
return. This is merely a restatement ofa something covered in Chapter
4. There, we learned thata sample mean provides an unbiased estimate
ofthe mean ofthe parentpopulation that gave birth to the sample. The
sample mean may err due to the random variation ofsampling. It may
be greater than the population mean or less than it, but neither erroris
morelikely. Thisprinciplepertainsto the caseofasingle-rule backtest.
The rule's mean return in a back test is an unbiased estimate ofits ex
pected return in the future. And although its historic mean return is in
deed its most likely return in the future, its performance may prove to
be higher or lower, with neither being more likely. This is illustrated in
Figure 6.4.
In data mining, the back-testperformance statistic plays a very dif
ferent role than itdoes in single-rule backtesting. In datamining, back-
Cumulative
Gains
$
Expected
Performance
Observed
Performance
Back-Test
+/
Random Variation
~----In-Sample----......I---Future---.
Time
FIGURE 6.4 Expected performance for single rule backtest.



==================================================
                     PAGE 287                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 271
tested performance serves as a selection criterion. That is to say, it is
used to identify the best rule. The mean returns ofall back-tested rules
are compared and the one with the highest return is selected. This, too,
is a perfectly legitimate use of the back test (observed) performance
statistic.
Itis legitimate in the sense that the rule with the highest back-tested
mean return isinfactthe rule thatis mostlikelytoperformthebestinthe
future. Thisiscertainlynotguaranteed, butitisthemostreasonableinfer
ence that can be made. Aformal mathematicalproofofthis statement is
offeredbyWhite.25
The Data Miner's Mistake: Misuse
ofObserved Performance
Let's recap two key points. In a single-rule back test, past performance
can be used as an unbiased estimate offuture performance. In multiple
rulebacktesting(i.e., datamining), pastperformancecan be used asase
lectioncriterion.
The data miner's mistake is using the best rule's back-tested perfor
mance to estimate its expected performance. This is not a legitinlate use
of back-tested performance because the back-tested performance ofthe
best-performingrule ispositivelybiased. Thatis, the level ofperformance
that allowed the rule to win the performance competition overstates its
truepredictivepoweranditsexpectedperformance.Thisisthedata-mining
bias. This conceptisillustratedinFigure6.5.
Cumulative
Gains \O~
~'I;;
e''(;T
~;.,.9
$ ~t;e Data
~~ Mining._-.
Observed ......__.._.------- _B..i.a.s.-
Performance I Expected
Best-Rule II Performance
I ofBest·Rule
I
I
I
I
I
1+-----ln.Sampie----......I..~I---Future---.
Time
FIGURE 6.5 Expected performance ofbest data-mined rule.



==================================================
                     PAGE 288                     
==================================================

272 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The bestrule'sperformance does nottruly deteriorate outofsample.
It only appears that way as the out-of-sample performance assumes a
level that reflects the rule's true predictive power without the good luck
that allowed it to beat the other rules. Its in-sample performance was a
combination ofsome level ofpredictive power, possibly zero, and a big
component of good luck. The good luck that shined on the rule during
the backtestis nowshining elsewhere. In a like manner, the monkeydid
not suffer a loss ofliterary skill on the night ofthe big show. His perfor
mance merely reflected his true literary ability minus the good luck that
allowed him to produce letters that happened to match a segment of
Shakespearianprose.
DATA MINING AND S1J\TISTICAL INFERENCE
This section discusses the connection between data-mining bias and sta
tistical inference. This sectionwill coverthe following points: (1) the dis
tinction between biased and unbiased estimators, (2) the distinction
between random error and systematic error (i.e., bias), (3) the fact that
unbiasedestimatorsareafflicted with random errorbutbiasedestimators
are afflicted with both random error and systematic error, (4) statistical
statements hold true for a large number ofobservations, such as a large
number ofestimates, and (5) the data-mining bias is an effect thatshows
up generally overmanyinstances ofdatamining; thus we cannotsaythat
anyparticulardata-mined resultisbiased.
Unbiased Error and Systematic Error
All scientific observations are subject to error. Erroris definedas the dif
ference between an observedvalueandthetruevalue:
Error = observed - true
A positive error is said to occur when the observed value is greater
thantruevalue. Negative erroristhe reverse. Ifa scale indicates aperson
weighs 140Ibsbuttheyactuallyweight 150, theerroris negative 10Ibs.
There are two distinct types of error: unbiased and biased (system
atic). All observations are infected with some degree of unbiased error.
Nomeasuringinstrumentortechniqueisperfect. Thistype oferrorhasan
expectedvalue ofzero. Thismeansthatifa large numberofobservations
are made on some phenomenon, such as the return of a rule, and those
observations are afflicted only with unbiased error, the observed values



==================================================
                     PAGE 289                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 273
will be haphazardlydistributedaboutthe truevalue. Ifanaveragewereto
be computed for all these errors the average would be approximately
zero. SeeFigure 6.6.
Incontrast, observations afflicted with systematic errortend to lie on
one side ofthe truth. Such observations are said to be biased. When the
errorsofmanybiasedobservationsareaveraged, theaverageerrorwillbe
distinctly nonzero. See Figure 6.7. In the illustration the observations are
positivelybiasedand theiraverageerror(observed- true) ispositive.
Suppose a chemistobserves the weightofa residue left bya chemical
reactionononehundredseparateinstances.Unbiasederrormightbeattrib
utable to random variations in laboratory humidity over the course ofthe
one hundred weighings. The amount ofmoisture in the residue affects its
weight. Systematic error, ifpresent, could be due to an imperfection in the
scale, alwayscausingthe observedweighttobelowerthanthe trueweight.
Unbiased and Biased Statistics
Interpreting a large sample ofobservations is difficult. As was discussed
in Chapter4, a sensiblefirst step is datareduction. This reduces the large
True
Probability
....,.. &
~.,.. MostLikely Distribution
Of
". Observed
Observed
Value ..... ......... Values
FIGURE 6.6 Unbiased observations.
Probability
Most
Distribution
Likely Of
Observed Observed
~ ~~I.~e Values
.. : .
.-,'" True
Value
FIGURE 6.7 Observations with systematic error.



==================================================
                     PAGE 290                     
==================================================

274 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
setofmeasurementsto a smallersetofsummarystatistics: samplemean,
sample variance, and other computed measures that describe the entire
setofobservations.
Chapter4alsopointedoutthatasamplestatistic, suchasthemean, is
subjectto aparticulartypeofrandom errorcalledsamplingerror. Thiser
ror is unbiased. Here, the term error refers to the deviation between a
sample mean and the mean of population from which the sample was
taken. Because the sample does not represent the population perfectly,
the mean ofthe sample will deviate to some degree from the meanofthe
population.
Relating this backto objectiveTA, a rule back test produces a large
sample of observations-the rule's daily, weekly, or monthly returns.
This sample is made more intelligible by reducing it to a performance
statistic (e.g., annualized average return, Sharpe ratio, and such). As is
true ofany sample statistic, the performance statistic is subject to ran
dom error. However, the statistic may also be subject to systematic er
ror orbias.
To state the obvious, a historical performance statistic cannot be put
in the bankor be used to buya Ferrari. Its sole economic utility is the in
ference it allows us to make about future performance of the rule that
produced it. Objective technicians use back-tested performance to make
an inference about a rule's expected performance in the form ofa confi
dence interval or a hypothesis test. In either case, the accuracy ofthe in
ference will depend on the type oferror: unbiased or systematic, and its
magnitude.
Biased statistics are afflicted with systematic error. As stated previ
ously in a single-rule back test, the mean return is an unbiased statistic.
Therefore, inferencesaboutthe rule's expected return based on back-test
performance will be subject only to a form ofunbiased error called sam
plingvariability.
This, however, is notthe casefor the bestrule found viadatamining.
The observed average return of the best-performing rule is a positively
biased statistic. As a result, inferences based on itwill be systematically
in error. This means that when conducting a hypothesis test, one would
be prone to reject the null hypothesis more frequently than the signifi
cance level would suggest. For example, at a significance level of 0.05
one expects to reject the null hypothesis in error only 5 times in 100.
However, ifthe observed performance is positively biased, then H will
o
be rejected more frequently than it should, perhaps far more frequently.
This will result in trading using rules that appear to have predictive
power but in fact do not. The question is: What causes the back-tested
return ofthe bestrule, the rule pickedbythe dataminer, to overstateits
true predictivepower?



==================================================
                     PAGE 291                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 275
The Mean versus the Maximum
Single-rule backtestersand dataminersare lookingattwo entirely differ
entstatistics. Thesingle-rulebacktesterisobservingthe mean ofa single
sample. The data miner is observing the maximum mean among a multi
tude ofsamplemeans.Itiseasyto overlookthefactthattheseare twoen
tirelydifferentstatistics.
To beclear, in the caseofa single-rule backtestthere isonesetofre
sults-the rule's daily returns generated over the back-test period. They
are summarized by a performance statistic (e.g., mean daily return). In
datamining, many rules are backtested, andsothere are manysets ofre
sults and many performance statistics. If50 rules have been back tested
the dataminerhas the opportunityto observe50meanreturnspriortose
lectingthe rule thatproducedthe maximum meanreturn.
The set of 50 means returns can be considered as a set of observa
tions, which, in tum, could be summarized with a statistic. For example
one could compute the mean ofthe means. Thatis the mean return ofall
50 rules. Anotherstatistic that could be computed for this set ofobserva
tions wouldbetheminimummean-themeanreturn ofthe rule thatdid
the worst. Yetanotherstatisticwould bethemaximummean-themean
return ofthe rule thatdidthe best.
Thestatisticobservedbythe datamineris the maximummeanfrom
among the50rules. ThisisillustratedinFigure 6.8, where all rules are as
sumed to be useless and have an expectedreturn equal to zero. Each dot
represents the observed mean return of a different rule. Note that the
mean return ofthe rule that had the maximum return (37 percent) is not
at all representative ofthat rule's expected return (0 percent). It simply
gotluckyin the backtest. When many rules are backtested, the one with
Mean
Of
FiftyMeans
Maximum Mean
Among
FiftyMeans
+37%
-30% 0 +30%
Observed Rate of Return
FIGURE: 6.8 Observed performance of50 rule back tests.



==================================================
                     PAGE 292                     
==================================================

276 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
thehighestaveragereturnwillalmostalwayshavebeen thebeneficiaryof
good luck. This is why its back-tested performance is likely to overesti
mate its expected performance. In a different sample, with less luck, the
rule is likelyto doworse.
This is the key lesson to take away from this: The observed perfor
mance ofthe best-performingrule, amongst a large set ofrules, is a posi
tivelybiasedestimate ofthatbestrule's expectedperformance because it
incorporatesasubstantialdegreeofgood luck. Dataminerswhodon'tun
derstand this are likely to be disappointed with this rule's out-of-sample
performance.
Sound Inference Requires the Correct
Sampling Distribution
Sound statistical inference depends on using the correctsampling distri
bution. Each test statistic has a sampling distribution that is appropriate
for testing its statistical significance. The sampling distribution that
would be correct for testing the significance ofa single sample mean or
constructingitsconfidenceintervalwould notbecorrectifthe teststatis
tic being observed were, in fact, a maximum mean among a multitude of
samplemeans.
Therefore, to make sound inferences, the data miner requires the
samplingdistribution ofthe maximum mean amonga multitude ofmeans
because that is the statistic being considered when evaluating the best
rule found by datamining. The central tendency ofthe sampling distribu
tionofthemaximummeanreflects therolethatgoodluckcanplayindata
mining. Thecentraltendencyofasamplingdistribution ofasinglesample
meandoesnot.
Now let's considerhow all this impacts a test ofsignificance. As dis
cussed in Chapter5, in a traditional testofsignificance, the null hypothe
sis asserts that the trading rule has an expected return equal to zero or
less. The teststatistic is the rule's observed mean return. In this case let's
assume the return was +10percentannualized. The sampling distribution
of the test statistic is centered at the hypothesized value ofzero. The p
value is the area of the sampling distribution that is equal to or greater
than a 10 percent return. This area represents the probability that, if the
rule's expected return were truly equal to zero, the rule could have pro
duceda returnof+10percentorhigherbychance.Ifthep-valueissmaller
than a preset value, such as 0.05, the null hypothesis would be rejected
and the alternative hypothesis, that the rule's expected return is greater
thanzero, would beaccepted. Solongastheperformanceofonlyonerule
was beingevaluatedthis isallwelland good.
Nowconsidersignificance testingin the dataminingcase. Continuing



==================================================
                     PAGE 293                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 277
with the data mining example from the prior section, assume that 50
rules have been back tested. The rule with best performance earned an
nualized return of +37 percent. A traditional test of significance, with a
significance level of0.05, would look like the one in Figure 6.9. The sam
pling distributionshown is centered at zero, reflecting the null's assump
tion that the expected return of a rule with no predictive power would
generatea return ofzero. Thissamplingdistributiondoes notaccountfor
the data-mining bias. Experimentalresults presentedlaterin thischapter
willshowthatthis assumption iswrong. Theywill demonstrate thateven
when all rules tested during data mining have expected returns equal to
zero, the best performing rule is likely to display a much higher perfor
mance than zero.
ote in Figure 6.9 that the observed performance of +37 percent
falls far outin the right tail ofthe sampling distribution giving a p-value
of less than 0.05. On the basis of this evidence, the null hypothesis
would be rejected and the inference would be that the rule has an ex
pected return greater than zero (i.e., has predictive power). This con
clusion is wrong!
If, however, the observed performance ofthis best rule of 50 were
to be subjected to a more advanced test of statistical significance that
does take into account the data-mining bias, the picture would look
considerably different. This is shown Figure 6.10, where the observed
mean return for best-rule-of-50 is compared to the correctsamplingdis
tribution. This is the sampling distribution of the statistic maximum
mean among 50 means. This sampling distribution properly reflects
the biasingeffects ofdatamining. Note that the sampling distributionis
no longercentered at zero. Rather, itis centered at +33 percent. Against
f
Observed
Performance
Best Rule of
50
Relative Sampling Distribution 37%
OfASingleSample
Freq. Mean
·70·60·50 -40·30-20,10 0 10 20 30 40 50 60 70
FIG RE 6.9 Traditional sampling distribution (does not accountfor data-mining
bias).



==================================================
                     PAGE 294                     
==================================================

278 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Null
Hypothesis
Expected= 33%
f
Relative
Sampling Distribution
Freq.
OfMulti-Mean
Maximum
-70-60-50 -40-30-20-10 0 10 20 30 40 50 60 70
FIGURE 6.10 Correct sampling distribution (accounts for data-mining bias).
this backdrop, the best rule's performance no longer appears signifi
cant. Thefraction ofthesamplingdistribution thatisequalto orgreater
than the observed return of37percent is nearly half(0.45) ofthe distri
bution total area. In other words, if the expected return ofeach ofthe
50 rules were zero, then there is a 0.45 probability that the mean return
of the best rule would be greater than or equal to 37 percent due to
luck. Fromthis, itisclearthat37percentisnotastatisticallysignificant
result. Theparticularsample ofdatain which the rule excelledjusthap
penedto favor the rule.
Figure 6.10 shows that randomness (good luck) can inflate the per
formance ofa rule with no predictive power. It turns out that random
ness is only one oftwo factors thatjointly cause the data-mining bias.
The other cause is the selection principle that underlies all multiple
comparison procedures: picking the candidate with the best-observed
performance.
DA11\·M1MNG BIAS: AN EFFECT WITH lWO CAUSES
The data-mining bias is the result of a conjoint effect: (1) randomness
and (2) the selection imperative ofdatamining orany multiple compari
son procedure-picking the candidate with the best-observed perfor
mance. This section will examine how these two factors combine to
causetheobservedperformanceofthebestruleto overstateitsexpected
performance.



==================================================
                     PAGE 295                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 279
Two Components ofObserved Performance
Theobservedperfonnanceofarulecanbefactored intotwocomponents.
One componentofobserved perfonnance is attributable to the rule's true
predictivepower, ifithas any. Thisisthe componentofperfonnance due
to a recurringfeature ofmarketbehaviorthatisexploitedbythe rule and
thatshould continueto manifestinthe immediatepracticalfuture. Thisis
the rule's expectedperfonnance.
The second component of observed perfonnance is attributable to
randomness. Randomness can manifest as either good luck or bad luck.
Good luck boosts observed perfonnance above expected perfonnance
whereas bad luck pushes observed perfonnance below expected. The
component of observed perfonnance attributable to randomness cannot
beexpectedto repeatinthe immediatepracticalfuture.
Thisdiscussion issummedup bythe equationinFigure 6.11.
The Spectrum ofRandomness
Itisuseful tothinkofaspectrum ofrandomness. Thisconceptisdepicted
in Figure 6.12.26 At one end of the spectrum, observed perfonnance is
'+1_
=
Observed I Expected lIRandomnes)J
Performance ~erformanceJ
FIGURE 6.1J The two components ofobserved performance.
Merit Luck
Dominates Dominates
FIGURE 6.12 Spectrum of randomness-relative contributions of merit versus
luck in observed outcomes.



==================================================
                     PAGE 296                     
==================================================

280 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
dominated byrandomness. At this extreme, allperformances are purelya
matter ofluck. Here we find literaryworks ofmonkeys dancing onword
processors and outcomes oflotteryplayers. Atthe other end ofthe spec
trum, observed performance is dominated by merit or systematic behav
ior. At this extreme, it is onlymerit that matters. Producing a valid proof
of a mathematical theorem resides here. Very nearby are the observed
performances of concert-level musicians. Also, in this neighborhood we
find the the laws ofphysics whose highly accurate predictions are made
possiblebythe orderlybehaviorofcertainaspectsofnature.
Towardthe middle ofthe randomness spectrumlie the mostinterest
ingdatamining problems. The farther to the random end ofthe spectrum
onegoes, the greaterthe risk ofdata-miningbias. Towardthe right endof
the continuum, wefind a region that containsthe TA rules. The complex
ityand randomness offinancial markets assure us that eventhe mostpo
tent rules will be weakly predictive. In this zone, the magnitude of the
data-miningbiaswillbelarge.
Now we come to an important principle. The larger the contribution
of randomness (luck) relative to merit in observed performance, the
larger will be the magnitude ofthe data-mining bias. The reason is this:
The greaterthe role ofluck relative to merit, the greaterthe chance that
one ofthe many candidate rules will experience an extraordinarily lucky
performance. Thisisthe candidatethatwillbeselectedbythedataminer.
However, in situations where observed performance is strictly orprimar
ilyduetoacandidate'struemerit, thedata-miningbiaswillbenonexistent
orverysmall. Inthesecases, a candidate'spastperformancewill beareli
ablepredictoroffuture performance andthe dataminerwillrarelyfillthe
hopperwithfool's gold.
Becausefinancial marketsareextremelydifficulttopredict, mostofa
rule's observed performance will be due to randomness rather than its
predictivepower. Thusthe equation previouslygiveninFigure 6.11 might
be better illustrated as it is in Figure 6.13. The large role ofrandomness
relative to merit (Le., predictive power) makes it likely that TA rule data
miningwillbeinfectedwithalarge bias.
The range of problems to which multiple comparison procedures
mightbeapplied can bevisualizedas lyingalongthespectrumofrandom
ness. Toward the random end ofthe spectrum is a range associated with
Observed = (predictiVe) +/_ [Randomness]
Performance
Power
FIGURE 6.13 Relative contributions ofrandomness and predictive power in ob
served performance.



==================================================
                     PAGE 297                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 281
TA rule data mining. I used a range to represent rule data mining rather
than a single location to indicate that the level of randomness will vary
from oneminingventure to the next. As will be demonstratedlaterinthis
chapter,the levelofrandomness inanygivendataminingventureisdeter
mined by five independent factors.27 When these factors are taken into
consideration, it becomespossible to develop statistical significance pro
cedures that are better able to deal with the data-mining bias. In other
words, it becomes possible to compute the statistical significance of or
confidence interval for a rule that was discovered by data mining. These
proceduresalleviatethe keyproblemfacing objectiveTApractitionersus
ingdataminingfor knowledge discovery.
The Effectiveness ofMultiple Comparison
Procedures under Differing Conditions
ofRandomness
Multiplecomparisonproceduresare appliedtoproblemswhere abestso
lutionissought.Auniverse ofcandidate solutionsisproposed.Afigure of
meritquantifies the observed performance ofeach candidate and the one
withthe best-observedperformanceisselected.
Many people presume that MCP delivers on two promises: (1) The
candidate with the highest observed performance is most likely to per
fornl the bestin the future, and (2) the observedperformance ofthe best
perfornling candidate is a reliable estimate of its future performance. It
does deliveronthefirst promise. However, in the domain ofTArule back
testing, itdoes notdeliveronthesecond.
With regard to the first promise, that the candidate with the highest
observed performance isalso the one mostlikely to do bestin the future,
this was proved to be true by White28 as the number ofobservations ap
proaches infinity. White showedthat, as the samplesize approachesinfin
ity, the probability that the candidate rule with the highest expected
return (Le., the truly bestrule) will make itselfknown by having the best
observed performance approaches 1.0. Thistells us thatthe basiclogic of
data mining is sound! The rule with the highest observed performance is
the rule thatshould be selected. The validity ofthis assumption will also
be demonstrated by mathematical experiments presented later in this
chapter: Experimental Investigation of the Data-Mining Bias. These re
sultsshowthatwhenasufficientnumberofobservationsareusedtocom
putea rule'sperformancestatistic (e.g., itsmean return), the rulewiththe
highestobserved performance does have a higher expected return than a
rule picked at random from the universe of rules tested. Data mining
must, at the very least, pass this minimal test ofefficacy for it to be con
sidered aworthwhile research method, and itdoes!



==================================================
                     PAGE 298                     
==================================================

282 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
With regard to the secondpromise, that the observed performance of
theselectedcandidateisa reliable estimate ofitsfuture performance, the
newsisnotgood.JensenandCohen29pointoutthat, whenMCPisapplied
in situations where randomness plays a significant role in observed per
formance, the observed performance of the best-performing candidate
overestimates its expected (future) performance. In other words, when
observedperformanceissignificantlydeterminedbyluck,itisgivesapos
itivelybiasedestimate. Thisisthe caseinTArule datamining.
Observed performance is a reliable estimate of the best candidate's
future performance in situations where randomness is nonexistent or so
lowthatitcanneverinfluencethe choice ofthewiningcandidate. Suchis
the case in a music competition or a competition at doing mathematical
proofs, whereitis merit, notluck, thatcarriesthe day.
The bottom line: the MCP selected candidate is the candidate most
likely to do best in the future so long as a large number ofobservations
are used to computetheperformancestatistic. Nevertheless, in problems
where randomness has a significant impact, observed performance is
positivelybiased, andtheselectedcandidate'sfutureperformancewill, in
all probability, be worse than the performance that allowed it to win the
competition.
MCP Efficacy in Low Randomness Situations
First, considerthe applicationofMCP to problemswithlowrandomness.
Atthis end ofthe spectrum, a candidate's observed performance is domi
natedbymerit, and Mepis effectiveinidentifyingsuperiormerit.
This problem is exemplified by the task ofhiring a new first violinist
for asymphonyorchestra.3oThe universe ofcandidatesconsists oftheset
ofmusicians applyingfor thejob. Each is asked to perform a challenging
composition, without prior rehearsal, for a panel ofjudges. The judges'
evaluation is the figure of merit. This acid test of instrumental compe
tence, known as sightreading, is effective because greatperformances do
notoccurbyluck.Ifluckisafactor, itisaminorone. Forexample, agreat
musicianmayhavean offdaybecauseofa maritalspatoraflattireonthe
way to the audition, but even these random influences will have only a
smalleffectona trulymeritoriousmusician.
In this situation, observed performance is an accurate indicator of
true merit and an excellent predictor of future performance. This is
depicted in Figure 6.14. Each candidate's merit, which is equivalent to
his expected performance, is indicated by the arrow. Both musicians
are excellent, although one is slightly better. The distribution of possi
ble performance surrounding each candidate's merit is narrow, indicat
ing the minor impact of randomness on observed performance. Note



==================================================
                     PAGE 299                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 283
Expected
Performance
HigherMerit
Candidate
Probability
Expected
Performance
Lesser
Candidate
l\/.l\
J.
I
I
Poor Excellent
Performance
FIGURE 6.14 Low randomness-small merit differential. True merit shines
through thin fog ofrandomness.
that there is no overlap in the distributions so that even if the better
candidate has an unlucky performance (i.e., at the extreme low end of
the distribution) and the lesser candidate has an extremely lucky one,
the bettercandidate will still be selected. This is anotherway ofsaying
that randomness is unlikely to inflate the inferior competitor's perfor
mance sufficiently such that it would be selected over the higher merit
candidate.
MCP Efficacy in High Randomness
Now consider a situation at the opposite end of the randomness spec
trum, TA rule data mining. Here, randomness has a major impact on ob
servedperformance,foreven the mostpotentTArules ormodelspossess
relatively little predictive power. This is a consequence of the complex
and highly random nature offinancial markets. The probability distribu
tion ofobserved performance ofa rule with an expectedreturn ofzerois
illustrated in Figure 6.15. Although zero is the most likely return, far
higherorlowerreturns arepossibledue togoodorbadluck. Ifthe ruleis
unlucky in the back test, observed performance will be negative. Even
more problematic for the objective technician is when good luck shines
onthe worthless rule and it earns a positive rate ofreturn. This may fool
the objective technician into believing that TA gold has been found. In
fact, the rule's signals coincided favorably with the market's fluctuations
purelybyaccident.
Although an extremelypositive ornegative average returnis unlikely
for an individual rule, it becomes increasingly likely as more rules are
back tested. Just as the greater the number ofpeople playing the lottery



==================================================
                     PAGE 300                     
==================================================

284 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Probability
Possible
But
Rare
Bad luck Good Luck
NegativeReturns o PositiveReturns
Annualized Return
FIGURE 6.15 Probabilitydistribution ofobserved performance.
increases the chance that there will be someone lucky enough to win
twice, the greater the number ofrules back tested increases the chance
that a rule will have an extraordinarilylucky observed performance. This
is the rule thatwouldbepicked bythe dataminer. The bottomline: when
observedperformance isprimarily due to randomness, itis likelythat the
bestobserved performance, amonga large setofobserved performances,
will belargelyaneffectofrandomness.
To see how luck might impact a data miner, imagine that you are ob
serving a data miner at work. Furthermore, imagine that you are in the
fortunate position of knowing what no data miner ever knows, the true
expected return ofeach rule that is back tested. Suppose the data miner
backtests twelve differentrules, andyou know that each ofthe rules has
an expected return of 0 percent. All the data miner knows are the ob
served returns produced by each rule's back test. In Figure 6.16, each
rule's observed performance is depicted by an arrow on the probability
distribution. Each distribution is centered at zero, reflecting the fact that
each rule has an expected rate ofreturn ofzero. Notice that one rule got
lucky, producing an observed return of +60 percent. This is the rule that
wouldbeselectedbythe dataminer. Inthiscase, the data-miningbiaswas
apositive60percent.
Ifthe data miner were to test an even greater number of rules, the
chance of an even more positive observed performance would be in
creased. As willbe discussed, the numberofrules testedduring datamin
ing is one offive factors that impact the size ofthe data-mining bias. In
Figure 6.17, 30rules are tested, and onemanaged to produce an observed
performance of100percent. Thisis the rule thatwould beselected bythe
dataminer. You knowitishighlyprobablethatitsfuture performancewill
be disappointing.



==================================================
                     PAGE 301                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 285
r·····················································.
60%
FIGURE 6.•6 Twelve different rules (each has an expected return of0 percent).
FIG RE 6.17 Datamining more rules increases probabilityofmore extreme luck.



==================================================
                     PAGE 302                     
==================================================

286 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The Risk ofPicking an Inferior Rule
Thepriorexamplesassumed thatallrules wereofequalmerit(allhadex
pectedreturnsequaltozero). Naturally, thedataminerhopesthatthereis
at least one superior rule amongst all those tested, and that the superior
rule's observedperformance will identifyitassuch. Unfortunately, this is
notalwaysthe case.
Thisisanothernegativeconsequenceofhighrandomness.Thesuperior
rule, theonewithhighestexpectedreturn, maynotgetpicked, because an
inferiorrule'sluckyperformancewinsthedataminingcompetition.Anintu
itivesenseofthelikelihoodofthisunfortunateresultisportrayedinFigure
6.18. Itshows the distribution ofobserved performance for two rules with
nearly identical expected performances, but one is indeed superior. How
ever, the considerable overlap ofthe distributions gives a sense there is a
substantial chance the rule of lesser merit will produce the highest ob
servedperformance,causingittogetpicked.
However, when the difference in expected returns (merit) between
the trulybestrule andthe nextbestrule islarge enough, the dataminer
can be more confident that best-observed performance will reliably
point to the superiorrule. This is illustrated in the Figure 6.19. In other
words, when the difference in meritbetween the bestrule and its near
est competitor is large, merit will be more likely to shine through the
fog ofrandomness. Thisisthe illuminationthatguidesthe dataminerto
the gold.
Practicallyspeaking,the dataminerisneverawareoftheriskofpick
inganinferiorrule. Thatwouldrequire knowledgeoftruemerit(expected
return), which is a population parameter and hence never known. The
Probability
r--Expeeted Return--l
i i
lesserMeritCandidate
Low o High
Performance
FIGURE 6.18 High randomness and small meritdifference.



==================================================
                     PAGE 303                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 287
ProbabiIity
r------------------------------l
I Expected I Expected
j Return j Bad Return
! i
InferiorRule Luck Superior Rule
~---------------1--------------~
//T"~_
Low o High
+20%
Performance
1~IGUHE 6.19 High randomness and large merit difference (superiority over
comes randomness).
dataminernevergetstoseethefull consequences ofthe data-miningbias
until, as thesongsays, thefuture ispast.
Five Factors Determine the Magnitude
ofthe Data-Mining Bias
Letus recap brieflywhathas beenestablishedthusfar:
• The data-mining bias is defined as the expected difference between
the observed performance ofa rule thatwins the datamining compe
titionand itstrue expectedperformance.
• Observedperformance refersto the level ofperformanceachievedby
a rule in back testing. Expected performancerefers to the rule's theo
reticalperformanceinthefuture.
• The observed perfomlance of the highest-perfomling rule found by
datamining is positivelybiased. Therefore, its expected performance
out ofsample will be less than the in-sample observed performance
thatallowed itto beatotherrulestested.
• Observed performance is a combination of randomness and predic
tive power. The greater the relative contribution ofrandomness, the
largerwill bethe magnitudethe data-miningbias.
The relationship betweenthe data-mining biasandthe relative contri
butionofrandomnessisillustratedinFigure6.20. Asexplainedinthenext
section, the degree of randomness encountered in a given data mining
venturedepends onfive factors thatcharacterizethe venture.



==================================================
                     PAGE 304                     
==================================================

288 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Large
Data
Mining
Bias
No Bias
MERIT , LUCK
Spectrum ofRandomness
FIGURE 6.20 Relationship ofdata-mining bias to degree ofrandomness.
The Five Factors Defined. Five factors determine the degree of
data-miningbias. Theyare:
1. Number of rules back-tested: This refers to the total number of
rulesbacktestedduringthe data-miningprocessenrouteto discover
ingthehighest-performingrule. Thelargerthenumberofrulestested,
the largerthe data-miningbias.
2. The number ofobservations used to compute the performance
statistic: The larger the number of observations, the smaller the
data-miningbias.
3. Correlationamong rule returns: Thisreferstothedegree towhich
the performance histories ofthe rules testedare correlatedwitheach
other. Thelesscorrelatedtheyare, the largerthe data-miningbias.
4. Presence of positive outlier returns: This refers to presence of
verylarge returns in a rule's performance history, for example a very
large positive return on a particular day. When these are present, the
data-mining bias tends to be larger, although this effect is reduced
whenthenumberofpositiveoutliersissmallrelativetothetotalnum
ber ofobservations that are used to compute the performance statis
tic. In other words, more observations dilute the biasing effect of
positiveoutliers.
5. Variation in expected returns among the rules: This refersto the
variationintrue merit(expectedreturn) amongthe rules backtested.
The lower the variation, the greater the data-mining bias. In other
words, when the setofrules tested have similardegrees ofpredictive
power, the data-miningbiaswillbelarger.
How Each Factor Impacts the Data-ll1ining Bias. Figures 6.21
through6.25 depictthe relationships betweeneachofthefive factors and



==================================================
                     PAGE 305                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 289
Large
OM Bias
O"'--------- ~
1 Many
Log ofNumber ofRules Back-Tested
FIGURE 6.21 Numberofrules back tested (1).
Large
OM Bias
Ol- ...._- _-.._---
~
1 Many
NumberofObservations
FIGURE 6.22 Numberofobservations usedtocompute performance statistic(2).
Large
...._--....._-_._--......._- .
.....
OM Bias
OL-_-+---t---+---t---+-_
0.2 0.4 0.6 0.8 1.0
Correlation Coeffient
FIGURE 6.23 Correlation among rule returns (3).



==================================================
                     PAGE 306                     
==================================================

290 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Large
DM Bias
OL-
____+_
Low High
Number& Magnitude ofOutliers Relative to NumberofObservations Used to
Compute Performance Statistic
FIGURE 6.24 Presence ofpositive outlier returns (4).
Large
....
DM Bias
OL-_----'-__--'-__-'-_-'__
--L____+
Low Variance High Variance
(All Rules'"Merit) (One VerySuperior Rule)
FIGURE 6.25 Variance in expected ROI-degree of difference in merit in rule
universe(S).
the magnitudeofthe data-miningbias. Theserelationshipspertainspecifi
callyto dataminingventures where the performance statistic being used
is the rule's mean rate of return. Because the sampling distribution of
otherperformancestatistics, suchas the Sharperatio, willdifferfrom the
distributionofthe meanreturn, the relationships between thefive factors
andthe dataminingbiasmayalsobe different.
The astute reader will wonder how such curves could have been de
veloped, given that a rule's expectedperformance is never known (itis a
population parameter) but must be known to measure the data-mining
bias. Thereaderiscorrect; the data-miningbiasis neverknownforactual
rules. However, it can be known for artificial rules and cantherefore be
investigated. An artificial rule is a computer-simulated trading signal



==================================================
                     PAGE 307                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 291
whose accuracy (fraction of correct signals) can be set experimentally.
This makes it possible to know the expected return ofthe artificial rule.
Theprecedingcurveswere based on asetoftestsdone on artificial rules.
The results ofthese tests, which are described in the next section, show
how thefive factors impactthe data-miningbias.
EXPERlMEN1i\L INVESTIGUION
OF THE DA1l\-M1NING BIAS
Scientistsmake observations and draw inferencesfrom them. Soundinfer
encesdependonaccurateobservations. Thus, akeytaskisdeterminingthe
accuracyoftheprocedureusedto makeobservations. Theprocessofmea
suring the random andsystematicerrorthatmaybe presentinan observa
tional procedureis called calibration. Onewayto calibratethe accuracyof
aprocedureisto testitonaproblemwhosecorrectansweris known. This
makesitpossibletomeasureitsrandomand/orsystematicerrors.
ObjectivetechniciansarethescientistsoftheTAworld.Theirprimary
procedureisrulebacktesting. Theobservableproducedisaperformance
statistic. On the basis ofthis statistic, an inference is made abouta rule's
predictive power or expected performance. Therefore, objective techni
cians are properly concerned with the random and or systematic error
that that may be present in performance statistics obtained by back test
ing. Chapter 4showed thatperformance statistics are subject to random
errorduetosamplingvariability. Thischapterisconcernedwithaform of
systematicerrorthatstemsfrom datamining-thedata-miningbias.
This section describes the results ofexperiments that investigate the
data-mining bias by examining how each ofthe five factors impacts the
sizeofthebias. Thisisdone bydatamininga universeofartificial trading
rules (ATRs). Unlike real TA rules, ATRs are ideal for this purpose be
causetheirexpectedreturn can be known becauseitisunderexperimen
tal control. This allows us to measure the data-mining bias associated
with the rule that had the best-observed performance. This, in turn, tells
ushowaccuratelyitsobservedperformance,astatisticwhichisknownto
the data miner, portrays its expected performance, the population para
meterthe dataminerwishes to knOw.
Artificial Trading Rules and
Simulated Performance Histories
TheATRperformancehistoriesproduced inthese experiments are com
posed ofmonthly returns. The expected return ofan ATR is controlled



==================================================
                     PAGE 308                     
==================================================

292 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
by manipulating the probability of a profitable monthly return. This
probability can be expected to manifest over a very large number of
months, a manifestation of the Law of Large Numbers. However, over
anysmallsample ofmonths, theactualfraction ofprofitablemonthswill
vary randomlyaboutthe specified probability level. Thisvariationintro
duces the crucial element ofrandomness into the experiments. An ATR
performance history can be generated for any specified number of
months (e.g., 24).
When the probability ofa profitable month is known, as it is with an
ATR, itsexpectedreturn canbedeterminedprecisely. This, inturn, allows
the data-mining bias associated with the rule to be measured. The ex
pected return ofan ATR is given bythe formula used to compute the ex
pectedvalueofa randomvariable:
EV = p(profitable month) x average gain - p(losing month)
x average loss
The average gain and average loss are also known because the ATRs
wereappliedto the absolutemonthlypercentage changesfor the S&P500
overthe periodAugust 1928through April 2003. The term absolutemeans
the signofthe S&P'sactual monthlychangewasignored. IntheATR tests
the algebraic sign (+ or-) ofthe ATR's monthlyreturn wasdeterminedby
a random process thatisdescribed below. Overthistime period, compris
ing nearly900monthlyobservations, the average absolute monthlyreturn
for the S&P 500was equal to 3.97percent. Thus, theformula for calculat
inganATR'sexpectedreturnis:
ER = ppm x 3.97 - (1 - ppm) x 3.97
Whereppmisdefinedastheprobabilityofaprofitablemonth.
ATR performance histories were generated by Monte Carlo simula
tion. Specifically, the absolute monthlychangesinthe S&P500were sam
pled with replacement from the 900-month history. A monthly return,
without its sign, was chosen at random, and a roulette wheel, simulated
by a computer, determined if that particular month was a gain or loss.
This represented onemonthoftheATR'sperformancehistory. The proba
bility ofa profitable month was set by the experimenter. Aprobability of
0.70would be as ifthe roulette wheel had 100slotswith 70 designated as
profit and 30 designated as loss. This procedure for generating monthly
ATR returnswas repeatedfor aspecified numberofmonths, anothervari
ableunderexperimental control. Theperformancehistoryis thensumma
rized withastatistic-meanmonthlyreturn, annualized. Thisprocedureis
illustratedinFigure 6.26.



==================================================
                     PAGE 309                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 293
n-month mean
Return = 7
Absolute
IMonthly
j
Returns
S&P 500 Random Device ATR
With Performance
1928-2003
Probability of History
Random Profitable month Of
Sampling Set Experimentally M months
With
Replacement
FIGURE 6.26 Monte Carlo generation ofATR performance histories.
Let'sconsiderwhattheexpectedreturnforanATRwouldbeunderfour
differentsettingsforprobabilityofamonthlygain: 1.0,0.63,0.50, and0.0.An
ATR with p(gain) setat 1.0 (all months profitable) would have an expected
monthly return equal to the S&P 500's average absolute monthly return of
+3.97 percentper month or a 47.6 percent annualized noncompounded re
turn. This value can be obtained by plugging into the formula for the ex
pected value ofa random variable. The same formula tells us that an ATR
withprobabilityofagainmonthsetto0.63wouldhaveanexpectedreturnof
+12.4percent. AnATRwithprobabilityofgainsetto 0.50would haveanex
pected returnofzerowhereasa probabilityofgainequalto zero wouldearn
areturnof-3.97percentpermonthor-47.6percentperyear.
Bear in mind that these are expected values that would be earned
over a very large number ofmonths. Over any small number ofmonths,
the ATR'sobservedmean return canvaryfrom thisvalue. Thesmallerthe
number ofmonths comprising the ATR's performance history, the larger
will be the randomvariationfrom theexpectedmean return.
To simulate the effect of data mining, for example picking the best
performingATRof10, 10ATRperformance histories weregenerated. The
one with highest mean return was selected, and its observed return was
noted. This process was repeated a total of 10,000 times. The 10,000 ob
servations were used to form the sampling distributionfor the statistic
mean returnofthehighestperformingATRselectedfrom 10.
Experiment Set 1: Data MiningATRs ofEqual Merit
In the first set ofexperiments, all ATRs were specified to have equal pre
dictive power. This was accomplished by setting p(gain) to 0.50 for all
ATRs, thusgivingthemallanexpected return ofzero.



==================================================
                     PAGE 310                     
==================================================

294 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Factor 1: umbel' of Rules Tested. All else being equal, the more
rules back tested to find the best rule, the larger the data-mining bias.
More monkeys dancing on keyboards increases the probability that one
will get lucky enough to type something that appears literate. Similarly,
back testing a larger number ofrules increases the chance that one will
enjoyextraordinaryluck.
In the tests thatfollow, each ATR was simulated overa 24-monthpe
riod. First let's look at the no-data-mining case-only one rule is back
tested. Thereis no dataminingand no data-miningbias. AlthoughanATR
with p(gain) set at 0.50 has an expected return equal to zero, sampling
variability makes it possible for any given 24-month history to produce a
meanreturnthatvariesaboveand belowzero.
To demonstrate this, 1,000 ATR performance histories, each of 24
months' duration, were produced by computer simulation. The sampling
distribution for the statistic mean annualized return is shown in Figure
6.27. Asexpected, the distributionis centeredatzero, whichisindeedthe
expected return of an ATR with p(gain) set 0.50. Also as expected, the
meanreturndisplaysawiderangeofvariationaroundzerobecause24ob
servations is a relatively small sample size. Afew ATRs, those out in the
right tail ofthe sampling distribution, had very good luck (more than 50
months wound up profitable). In the left tail ofthe distribution, we find
the performance histories with bad luck. However, it is the center ofthe
samplingdistribution thattells the story. On average, under the condition
ofno data mining, an ATR with no predictive power can be expected to
250
200
150
100
50
-48 -38 -29 -19 -9.6 0 9.6 19 29 38 48
FIGURE 6.27 Mean annualized percent return (single ATR, 24 month history,
1,000replications).



==================================================
                     PAGE 311                     
==================================================

Data-Mining Bias; The Fool's ColdofObjective TA 295
earn a mean return of zero. AB the next tests will show, when a best
performingATRispickedfrom two ormoreATRs, thebestislikelytoshow
areturngreaterthanzero,eventhoughithasanexpectedreturnofzero.
Nowwe will be datamining bypickinga bestATRfrom two ormore.
To examine the relationship betweenthe size ofthe datamining bias and
the numberofrules from which the bestis selected we will vary the size
ofthe ATR universe-the numberfrom which the best is picked. Specifi
cally, the bias is measured for the bestATR of2, 10, 50, and 400. For ex
ample, ifthe numberofATRswassetat 10, the best-performing one of10
waspicked, and its observed mean return was noted. This procedurewas
repeated 10,000 times and the sampling distribution for the statistic (ob
served return ofthe bestATR) was plotted. All performance histories are
24months, all rules are settop(gain) = 0.50 (expectedreturn = 0), andall
rules are set to have independent returns (no correlation). Because all
ATRs had expected returns equal to zero, the data-mining bias is simply
equal to the observed return ofthe best performer averaged over 10,000
replications.
Figure 6.28 shows the sampling distribution of observed perfor
manceforthe bestATRoutoftwo. The samplingdistributionis centered
at +8.5 percent. In other words, when only two ATRs are tested and the
best is selected, the bias is a positive 8.5 percent. The data miner would
be expecting +8.5 percent in the future, but we know the true expected
return is0percent.
IfthenumberofATRsisincreasedto 10,and the bestisselected, the
data-mining bias increases to +22 percent. However, we know that the
selected ATR has a p(gain) of0.50 and an expected return of0 percent.
Observed
Expected Return
Return Best
f Best ATR
ATR Of
2Rules
Relative
Freq.
DataMining
Bias
+8.5%
0
-70-60-50 -40-30-20-10 10 20 30 40 50 60 70
FIGURE 6.28 Sampling distribution-mean return bestoftwo ATRs.



==================================================
                     PAGE 312                     
==================================================

296 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Figure 6.29issimilarto 6.28, exceptitshowsthesamplingdistributionfor
the mean observed return for the best-of-lO ATRs. The observed perfor
mance ofthe best-of-10ATRsis biasedby +22 percent.
Whenthe size ofthe data-mined universe is expanded to 50ATRs the
biasincreasesto +33percent. Figure6.30showsthesamplingdistribution
forthe mean return ofthe best-of-50ATRs.
A similar plot, in Figure 6.31, shows that the data-mining bias in
creasesto a +48percentforthe best-of-400ATRs.
The relationship betweennumberofATRs tested and the data-mining
bias can be seen in Figure 6.32. It summarizes the results obtained for
Observed
Expected Return
Return Best
f Best ATR
ATR Of
10
Relative
Freq.
DataMining
Bias
+22%
-70-60-50 ~0-30-20-10
FIG RE 6.29 Sampling distribution-mean return best of 10ATRs.
Observed
Expected Return
Return Best
f Best ATR
ATR Of
50
Relative
Freq.
DataMining
Bias
+33%
-70-60-50 -40-30-20-10
FIGURE 6.30 Sampling distribution-mean return best of50 ATRs.



==================================================
                     PAGE 313                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 297
Observed
Expected Return
Return Best
f Best ATR
ATR Of
400
Relative
Freq.
DataMining
Bias
+48%
-70-60-50 -40-30-20-10 0 10 20 30 40 50 60 70
I~IGURE6.31 Sampling distribution-mean return bestof400 ATRs.
60 %
I..···....
SO%
...... <)
D
M 40%
B 30%
................
I
A 20%
S
10% ?....
2 10 50 400
Number ofRules Tested (log scale)
FIGURE 6.32 Data-mining bias versus number of rules tested (annualized per
cent return): 24-month performance history.
best-of2, 10, 50, and400ATRs. Theverticalaxis representsthemagnitude
ofthe data-miningbias-the differentialbetweenthe observedreturnsfor
the bestperformingATRand itsexpectedreturn. The horizontalaxis rep
resents the numberofATRs backtested to find the best. The relationship
is summarized by fitting a curve through the four datapoints. Itis nearly
linear when number ofrules tested is plotted in log terms-log (baselO).
Themainpointtotakeawayfrom theseexperimentsisthis: Thelargerthe
numberofrulestested tofind abestrule, thegreaterthe data-miningbias.



==================================================
                     PAGE 314                     
==================================================

298 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Itshouldbepointedoutthattheparticularmagnitudesofdata-mining
biasshown in theprecedingtestsare valid onlyfor theparticularsofthis
test: a particular set of S&P 500 monthly returns, a 24-month perfor
mance history, all rules have independentreturns, andallrules havingan
expected return equal to zero. In a different data-mining venture, with
different particulars, the same principle would apply (more rules pro
duce a bigger bias) but the specific levels ofthe data-mining bias would
be different.
Forexample, hadalongerperformancehistorybeenused to compute
mean return-48 months instead of24-the distribution ofmean ATR re
turns would have clustered more tightly around the expected return of
zero. Thisis merelya manifestation ofthe LawofLarge Numbers. Conse
quently, the bias associated with the highest performingATR would have
been less. This tells us that the number ofobservations used to compute
theperformancestatisticis an importantfactor indetermining the magni
tude ofthe data-miningbias.
Let'sconsiderwhatwassaidintheprevioussentence. Thegreaterthe
numberofmonthlyobservations usedto computetheperformancestatis
tic, the smaller the dispersion of the statistic's sampling distribution. In
otherwords, the largerthe sample size used to computethe performance
statistic, the less the degree ofrandomness in observed performance and
the less the opportunity for an extraordinarily lucky performance. What
everreduces the degree ofrandomness in observed performance reduces
the data-miningbias.
The importance of sample size on data-mining bias can be seen in
Figure 6.33. Itis similarto the preceding Figure 6.32. Itis a plot ofdata
mining bias, represented on the vertical axis, as a function ofnumber of
ATR's comparedtofind the bestone, onthe horizontal axis. However, in
this plotthere are four curvesinstead ofone. Each curveis based on us
ing a different number of monthly observations to compute each rule's
mean return; 10, 24, 100, and 1,000 months. The dotted line for 24
months is the same curve seen in Figure 6.32. Two points are worthy of
note. First, all curves rise as the number of rules back tested is in
creased. This is consistent with the finding discussed in the preceding
section-the data-mining biasincreasesasthe numberofATR's testedis
increased. Second, andperhapsmostimportant, is thefact thatthe mag
nitudeofthebiasisreducedbyincreasingthe numberofmonthsusedto
compute the mean return. For example, when only 10 months of data
are used, the bias for the best-of-1,024 ATRs is approximately 84 per
cent. When 1,000 months ofdata are used, the bias for the best of 1,024
rulesshrinkstolessthan 12percent. Inthe nextsection,wewillseewhy
the numberofobservationsissoimportantindeterminingthesizeofthe
data-mining bias.



==================================================
                     PAGE 315                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 299
96
D 10 mos.
84
M
72
B 60 24 mos.
I
48
A
S 36
24 --- - 1-00'mos.
%
12 1000mos.
Yr.
4 16 64 256 1024
NumberofATRs(logscale)
FIGURE 6.33 Data-mining bias versus numberofATRs tested for different sam
ple sizes.
Factor 2: Number of Observations Used to (;ompute the Per
formance Statistic. Figure 6.33 tells us that increasing the number
of observations used to compute the performance statistic reduces the
magnitudeofthe data-miningbias. Infact, ofallthefactors impactingthe
sizeofthebias, sample sizemaybe the mostimportant. The moreobser
vations used, the less opportunity there is for a few lucky observations
(profitable months) to result ina high mean return. We sawthis in Chap
ter 4, where it was demonstrated that the sampling distribution is re
duced in width when a larger number of observations are used to
compute the sample statistic. With only a few observations, a sample
mean can stray considerably above or below the true population mean.
The message conveyedby a wide sampling distribution is that there is a
greater opportunity for a rule to generate a very profitable return in a
back test by luck rather than by predictive power. This is illustrated in
Figure 6.34, which shows two sampling distributions for the same rule,
for which expected return is zero. Note the sampling distribution based
on fewer observations is wider. A mean return computed from a short
performance historyis more likelyto produce a very lucky result than a
mean computedfrom a largernumberofobservations (Le., a longerper
formance history).
The next set of experiments examine the effect of the number of
monthly observations used to compute mean ATR return on the size of
the data-mining bias. The numberofmonths wasvaried from onemonth
to 1,000 months, in 50-month increments, with data-mining bias com
puted ateachincrement. This was done for two cases: the best-of-lOand



==================================================
                     PAGE 316                     
==================================================

300 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Sample Mean Computed From
Large NumberofObservations
LuckyRule
Appears
ModestlyProfitable
"--"~
-10% 0 +i'0%
Sample Mean Computed from
Small NumberofObservations LuckyRule
___ Appears
~tfi"b1'
-30% -10% 0 +10% +30% .------
FIGURE 6.34 Narrowversus wide sampling distributions.
the best-of-lOO ATRs. As in prior experiments all ATRs were set to have
expectedreturnsequalto zero.
Figure 6.35 shows the relationship between the data-mining bias on
the vertical axis versus the number of observations used to compute
ATR mean returns on the horizontal axis. Because the expected return
for all ATRs is equal to zero, the data-miningbiasis equal to the average
observed performance of the best. Thus, the vertical axis, which is la
beled data-mining bias, couldas easilyhave been labeledaverageper
formance of the best rule. Note the steep decline in the magnitude of
96
D 84
M
72
B
60
I ObservedPerformance
A 48 Best-ot-100ATRs
S
36 ObservedPerformance
Best-ot-10ATRs
% 24
Yr. ....
12 '
-"" --------...
200 400 600 800 1000
NumberotMonthsUsedToComputeMeanATRReturn
FIGURE 6.35 Data-mining bias versus numberobservations.



==================================================
                     PAGE 317                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 301
the biasas thenumberofobservationsused to computethe meanreturn
is increased. This is the Law ofLarge Numbers working in favor ofthe
dataminer.
The lesson here is this: Be very skeptical of performance statistics
computed from a small number ofsignals or time intervals. Another im
portant message from Figure 6.35 is that when the number of observa
tions (months) becomes quite large, 600 or so, the difference in the bias
between best-of-lO and the best-of-lOO ATRs becomes tiny. This finding
hasvery important implications for the data miner-when the numberof
observations is sufficient, one can data mine a much larger number of
rules without significantly increasing the data-mining bias. The Law of
Large Numbersrules!
FactoI' 3: Degl'ee of Rule COlTelation. The third factor affecting
thesize ofthe data-miningbias isthe degree ofsimilarityamongthe rules
tested. Rules aresaidtobesimilarwhentheygenerateperformancehisto
ries that are strongly correlated. That is to say, their monthly or daily re
turns are correlated. The stronger the correlation between the rules
tested, thesmallerwillbethe magnitudeofthebias. Conversely,the lower
thecorrelation(i.e., thegreaterthedegreeofstatisticalindependence)be
tween rules returns, the largerwill bethe data-miningbias.
This makes sense because increased correlation among the rules has
the consequence of shrinking the effective number of rules being back
tested. Imaginealargesetofrules thatarecompletelyidentical. Naturally,
they will generate perfectly correlated performance histories. In effect,
this large setofrules is reallyjustone rule, and we already know that the
data-mining bias shrinks to zero when only one rule is backtested. Back
ing off from this extreme case, when rules are highly similar, and thus
have highlycorrelatedreturns, thechance ofanextraordinarilyluckyper
formance isreduced. Themoredissimilartherules, the greaterwillbethe
opportunity for one to have a great coincidental fit to the historical data
and achieve high performance. So high correlation amongst the rules
shrinks the effective number ofrules back tested and hence shrinks the
data-miningbias.
Inpracticalterms, rule correlationismostlikelyto behighwhendata
mining involves optimizing the parameters of a specific rule form. Sup
poseadualmovingaveragecrossoverrule is beingoptimized. Thismeans
that every rule tested is the same except for the parameter values-the
numberofdays usedto computetheshort-term and long-termmovingav
erages. The rule using values of26 days and 55 days will produce returns
thatarehighlycorrelatedwitharule usingvaluesof27and55.
Figure6.36showstherelationshipbetweenthemagnitudeofthedata
mining bias for the best rule (vertical axis) and the degree ofcorrelation



==================================================
                     PAGE 318                     
==================================================

302 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
24
Bestof1000ATRs
Data 18 Bestof100ATRs
Mining
Bias 12 --B-e-st-o-f-1-0ATRs
-----
%
Yr. 6
,
,
\
0-'--------'----'---------'----'--------'-
0.2 0.4 0.6 0.8 1.0
Rule Correlation
FIGURE 6.36 Data-mining bias versus rule correlation <annualized percent re
turn numberofmonthlyobservations = 100).
(horizontal axis) among the rules. Rule correlation was simulated as fol
lows: AninitialATRperformancehistorywasgenerated. Inthegeneration
ofa secondATR history, a biased random device was consulted to deter
minethemonthlyreturnsofATRhistory2. Thebiaswassettothedesired
levelofcorrelation. Forexample,ifa .7correlationwasdesired,acoinflip
was simulated with the probability ofheads =0.70. Ifthe coin landed on
heads, then the monthly returnfor the secondATRwould be the sameas
the monthly return ofthe initialATR. This was continued for subsequent
ATRhistories.
Each ATR was simulated over a 100-month history. Three different
tests were run, each based on a different numberofATRs: 10, 100, 1,000.
In other words, one test measures the data-mining bias associated with
the bestATR rule outof100. Asin priortests, all rules had expected re
turns equal to zero. The vertical axis of the plot represents the data
mining bias. Thefactorbeingexaminedhere, rule correlation, wasvaried
from 0 to 1.0. Note that the bias remains high until rule correlations ap
proach a level of1.0. Thus, rule correlation does nothave a majorimpact
inreducingthedata-miningbiasuntilthere isahighlevelofcorrelationin
their returns. Also note thatthe data-mining bias is higherfor the bestof
1,000rulesthanitisforbestof10.Thisissimplyamanifestationoffactor
1, the greater the number of rules from which the best is selected, the
greaterthe data-miningbias.
Fador 4: Presence of Positive Outliers in Rule Returns. A
sample ofrule returns (daily, weekly, ormonthly) thatcontains a few ex
tremelylargepositiveobservationshasthepotentialto createalargedata



==================================================
                     PAGE 319                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 303
mining bias. The size ofthe bias will depend on how extreme the values
are and the numberofobservations used to compute the mean. Ifa small
numberofobservations are used, one extremelypositive value can boost
the sample mean dramatically. Alarge numberofobservationswilllessen
the impactofan occasional extremevalue.
Distributionsthatcontainextremevaluesaresaid to have heavy tails.
The tails ofa distribution are the outerright and leftzones where the ex
treme but rare observations lie. Heavy-tailed distributions extend farther
awayfrom the distribution'scenterthanlight-taileddistributions.Thenor
mal distribution has light tails. Figure 6.37 illustrates this distinction. The
distribution in the upperportionofFigure 6.37has the normalbellshape.
Note how quickly the tails disappearas one moves awayfrom the distrib
ution'scenter. Thistells us thatextreme observationsarerare to the point
ofbeingvirtually nonexistent. People'sheightsarenorn1allydistributed.It
is rare toseesomeone oversevenfeet tall. No onehas everseen a person
over 10 feet. The distribution in the lower portion ofthe figure is heavy
tailed. Although extreme observations are also rare, they occurmore fre
quently thanin light-taileddistributions. Werepeople'sheightsdistributed
thisway, you mightmeetsomeone20feettall!
Asmentionedearlier,whenthe distributionofdaily(weekly,monthly)
returns contains extreme observations, a sample mean, computed from
these observations, can also take on an extremevalue. This occurs when
one ormore ofthe extreme values wind up in the sample and the sample
sizeissmall. From this itfollows thatheavytailsinthe return distribution
tend to cause tail heaviness of the sampling distribution of the mean.
I I
Light Tails
L
o
I I Outliers:
Heavy Tails
Extreme Positive
Months, Days, Etc.
o
I"IGUKE 6.37 Light-versus heavy-tailed return distributions.



==================================================
                     PAGE 320                     
==================================================

304 METHODOLOGICAL,PSYCHOLOGICAL. PHILOSOPHICAL, STATISTICALFOUNDATIONS
Nevertheless, thevalue ofasamplemean is neveras extreme as the most
extreme value found in a sample. This is simply the effect of averaging.
Therefore, the sampling distribution ofthe mean return will always have
lighter tails than the distribution ofinterval returns. This is illustrated in
Figure 6.38. 'I\vo return distributions are depicted. On the left is a return
distributionwithheavytails. Thesamplingdistributionofthe mean based
onthis distributionalso has heavy tails, althoughtheyare lighterthan the
return distributionitself. Onthe rightisa thin-tailed return distribution. A
samplingdistributionofthe mean basedonitnecessarilyhaslighttails.
Note that both of the sampling distributions in the Figure 6.38 are
based on the sanle sample size. Sanlple size is a separate factor influenc
ing tail heaviness of the sampling distribution. I wanted to hold sample
size constantsoasnotto confusethe issue.
Now let's considerthe effectofsample size on the sampling distribu
tion's tail heaviness. When more observations are used to compute the
samplemean, the effectoftailheavinessinthereturn distributionis mini
mized. The greaterthe numberofobservationsused to compute the sam
ple mean, the lighter will be the tails ofthe sampling distribution. This is
illustrated in Figure 6.39. The return distribution clearly has heavy tails.
Onthe leftis thesamplingdistribution ofthe mean based on asmallsam
ple size. Its tails, though notas heavy as the return distribution itself, are
still heavy. On the right is a sampling distribution of the mean derived
from thesamereturn distribution, exceptthatalargersamplesize is used
Return Distribution: HeavyTails Return Distribution: LightTails
- & -
o
~
Resulting Resulting
Sampling Distribution of Sampling Distribution of
the Mean the Mean
-~- - . . 1 . -
o o
FIGURE 6.38 Impact of extremes in return distribution on sampling distribu
tion: equal sample size.



==================================================
                     PAGE 321                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 305
Return Distribution: HeavyTails
T
/
\\
......................................................... .
o
Sampling Distribution Sampling Distribution
Small Sample Size Large Sample Size
(Tails Still Heavy) (LighterTails)
- . 4 . - - i -
o o
FIGURE 6.39 Impact of sample size on tail heaviness of the sampling
distribution.
to compute the sample mean. The tails ofthis sampling distribution are
lightenedasa result.
What is the relevance of all this to the data-mining bias? A heavy
tailed sampling distribution is saying that randomness is playing a large
role in the mean return. Randomness is always bad news for the data
miner.31 Imaginetwo rules. Bothhave expectedreturns ofzero. Onehas a
heavy-tailed sampling distribution. The other has a sampling distribution
with light tails. The heavy tails tell us that there is a greater probability
that the rule will produce a mean return, in a back test, that is much
higherthan its expected return. Ifthis rule is pickedbythe dataminer, its
data-mining bias will be large and its out-of-sample performance is likely
to be disappointing. This is illustrated in Figure 6.40. The heavy-tailed
sampling distribution shows that it is within the realm ofprobability for
the rule to earn mean return ofgreaterthan 30 percent in a back test. In
the case of the sampling distribution with light tails, a return that high
wouldhaveaprobabilityofalmostzero.
It is widely recognized that the distribution of stock market returns
and the returns of other financial asset have relatively heavy tails com
pared to a normal distribution.32 A rule that was short, by coincidence,
whenthe marketcrashed maywin the data-mining-performance competi
tion, butitwillhave wonforthewrong reason.
Theissue oftailheavinessanditsresultantimpactonthe data-mining



==================================================
                     PAGE 322                     
==================================================

306 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
-10% 0 +10%
Extraordinarily
Lucky
Back-Tests
-30% -10% 0 +10% +30%
FIGURE 6.40 Light-versus heavy-tailed sampling distributions.
bias was examined experimentally. The data-mining bias was compared
fortwosetsofATRs. Onesetusedthe S&P500monthly returns, adistrib
ution with tails that are heavier than those ofthe normal distribution, to
generate performance histories. The other set ofATRs used a return dis
tribution with intentionally light tails. As in the other experiments, the
ATRwiththebest-observedperformancewasselectedanditsdata-mining
biaswasmeasured. The resultsoftheseexperimentswereclear.Thedata
mining bias was much less pronounced for the ATRs that were based on
the light-tailed return distribution. This finding is ofpractical value only
when data-mining rules exit at fixed targets. This has the effect ofelimi
natingextremeoutcomes. Thisinturnhastheeffectofreducingtailheav
iness in the sampling distribution of the performance statistic. In most
instances, however, rule researchers are stuck with the return distribu
tionsproduced byfinancial marketbehavior. Becausemarketsaresubject
to extreme events, meanrule returns will tendtobeextremeas well.
Factor 5: Variation in Expected Returns among the ;\,1'Rs.
Thisfactorrefers to the degreeofvariationintheexpectedreturnsamong
the set ofrules considered during datamining. The ATR experiments dis
cussedthusfar have consideredATRs with uniform predictive power. All
have had expected returns set at zero. When all rules are ofequal merit,
be that merit none, low, orhigh, any differences in theirobserved perfor
mances are due entirely to luck. In such a case, the winner ofthe data
mining performance competition will simply be the luckiest rule. Thus,
the difference between its observed mean return and its expected return
will be large. Therefore, when all rules possess equal expected returns,
thedata-mining biaswillbe large.



==================================================
                     PAGE 323                     
==================================================

Data-Mining Bias.- The Fool's ColdofObjective TA 307
However, ifthe universe ofrules explored during data mining differs
withrespecttotheirexpectedreturns, the data-miningbiaswilltendtobe
smaller. Under certain conditions, itwill be very small. The following ex
perimentsinvestigatehowvariationinexpectedreturns with the rule uni
verse impacts the data-mining bias. This is accomplished by creating a
universeofATRsthathavedifferinglevelsofexpectedreturn. Recall, that
an ATR's expected return is entirely determined by its designated proba
bilityofaprofitablemonth.
Let'sfirst considerwhatthefollowing experimentsshouldshow. Sup
pose a universe ofATRs contains one that is truly superiorto all the oth
ers. Ithas an expectedreturn of+20 percent, whereas the others all have
expectedreturns ofzero. We wouldexpectitstruesuperiorityto reveal it
selfby producingthe highest-observed perfonnance mostofthe time (on
any given test it is not guaranteed to do so). Consequently, most of the
time thesuperiorATRwillgetpickedand the data-miningbiasassociated
withitwill tend to besmall. The biaswilltend to besmallbecause its ob
served perfonnance was earned the old-fashioned way, by dintofpredic
tivepowerratherthan luck.
Experiment 2: Data Mining ATRs with Differing
Expected Returns
Thefollowingtestsinvestigatethe data-miningbiaswithina universepop
ulated byATRs whose expectedreturns differ. Inthese tests, the majority
ofATRs have expected returns equal to zero orclose to zero. However, a
few ATRs with returns significantly greater than zero are sprinkled in.
This is intended to replicate the world a rule data miner can reasonably
hope to encounter-therereallyissomeTAgold in the mountains ofdata
although itis rare.
Inadditiontoinvestigatingthemagnitudeofthedata-miningbias,this
set oftests also addresses another question: Does data mining rest on a
sound premise?Thatis, does pickingthe rule with the best-observed per
fonnance do a betterjob of identifying authentic predictive power than
randomlyselectinga rulefrom the universeofrules. Dataminingmust, at
the very least, pass this minimal test ofefficacy to establish that it is a
reasonable researchapproach. Figure 6.41 shows the expected returnfor
an ATRgivenits specified probability ofa profitable month. Bearin mind
that this graphisspecific to the montWySPreturns usedinthese simula
tionexperiments.
Recall that an ATR's expected return is determined entirely by its
probability-of-profitable-month (PPM). The following tests assume that
ruleswithhighexpectedreturnsarerare. Tosimulatethisassumption, the
PPM assigned to each ATR was drawn at random from a distribution of



==================================================
                     PAGE 324                     
==================================================

308 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
60%
50%
40%
30%
20%
10%
0%
0.50 0.60 0.70 0.80 0.90 1.0
ProbabilityofProfitableMonth
FIGURE 6.41 Expected return (f) probabilityofprofitable month.
PPMs. The distribution used for this purpose has a shape similar to the
distribution ofwealth in a society. It has a long tail out to the right, indi
cating thatwealthyindividuals, like Bill Gates, are extremelyrate while a
great many are poor. The distribution ofexpected returns used for these
experiments was definedsuchthat an ATR with an expected return of19
percent per year or greater had a frequency of occurrence of about 1in
10,000. The expected returnforthe average ATRin the universe was +1.4
percentperyear. The approximate shape ofthe distribution used is illus
tratedinFigure6.42.
Although the specific assumptions underlying this distribution may
not be accurate, they are probably correctto within one order ofmagni
tude. This is based on the performance of the best hedge funds using
Frequencv
o
1.4% 19%
Expected ATR Return
FIGURE 6.42 Distribution ofATR expected returns.



==================================================
                     PAGE 325                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 309
technical analysis. However, for purposes of these experiments, the
specifics are not crucial. The real purpose here is to measure the data
miningbiasina universe ofruleswhoseexpectedreturns differand to in
vestigate how the bias can thwart the objective of data mining, finding
meritina universe whereitis rare.
Is Data Mining Based on a Sound Premise?
A Qualified Yes!
This section addresses the important fundamental question: Is data min
ing based on a sound premise? This is really two questions. First, is ob
served mean return a useful indicator ofrule merit in the sense that the
rule with the highest observed mean return has a higher expected return
than a rule picked at random? Ifa randomly selected rule is as likely to
perfoffil well in the future as a rule that performed well in a back test,
then data mining is a pointless activity. Second, is more data mining bet
ter? That is, does testinga largernumberofrules lead to the discovery of
a rule with higher expected returns? If so, then back testing 250 rules
would bea betterideathan backtestingonlyten.
Fortunately for the field ofdatamining, the answer to both questions
is a qualified yes. The qualification relates to the number ofobservations
thatare used to computethe rule's mean return orotherperformance sta
tistics. Ifa sufficient number ofobservations is used, then the answer to
both questionsisyes. Thisisverygood newsforthe dataminer. However,
when the number of observations is small, data mining does not work.
Onewould dojustaswellpickinga rule byguess, anddataminingalarger
numberofruleswill notproducebetterrules.
Because the number of observations used is crucial, no doubt
the reader would like to know how many are sufficient. Unfortunately,
there is no simple answer. The requisite numberdepends on the perfor
mance statistic and the nature of the raw data. The best I can do is to
providerepresentative charts thatgive a general sense ofrequisite sam
ple sizes.
Inpriorplots, theverticalaxis representedthe data-miningbiasinan
nualized percent return peryear. Take note, however, that in Figure 6.43,
the vertical axis represents the true merit or expected return ofthe rule
with the highest-observed performance-the rule that would be selected
bythedataminer. Clearly, sucha plotcouldnotbeproducedforactualTA
rules because their expected returns are never known. The horizontal
axis represents the number ofATRs tested to find the one with the best
observed performance. The set of numbers considered was: 1 (no data
mining), 2,4,8, 16,32, 64, 128,and 256. Therearethreecurvesontheplot.
Each curve is based on a different number ofmonthly observations used



==================================================
                     PAGE 326                     
==================================================

310 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
......
9.6% ................. n=1000
............................
7.2%
Expected
Return I'
:~::. !~',,~:I~O
:::
Average f-----.--t::0> 1 ::
Expected <]
Return n-2
l.4%/yr
1 32 64 128 256
NumberofATRs Tested To Find The Best
FIGURE 6.43 Expected return of best performing ATR versus number of ATRs
back tested.
to computethe ATR's mean annual return. The numberofmonthly obser
vations usedforthesethreecaseswere: 2, 100, and 1,000.
The second question posed was the following: Does testing more
rules leadto the discoveryofarule with ahigherexpectedreturn?Ifso,
the curve ofexpected return should rise as the number ofATRs exam
inedtofind the bestone goes up. In otherwords, the expectedreturn of
the ATR with the best-observed performance should be positively cor
related withthe numbertestedtofind the best. Onthe otherhand, ifthe
curve does not rise, it would be telling us that more searching does not
lead to rules of higher merit. This would mean that data mining is a
fruitless activity.
In Figure 6.43, two of the three curves rise-the one based on 100
monthly observations and the one based on 1,000. This tells us that back
testing more rules leads to the discovery ofhighermerit rules. The curve
based on 1,000 rises more steeply than the one based on 100. This is
telling us that when performance statistics are computed from a larger
number of observations, data mining more reliably leads to better rules.
This is good news for data miners who pay attention to the issue ofsam
plesize.
ThecurveinFigure 6.43thatdoesnotrise alsotellsastory: Whentoo
fewobservationsareusedtocomputetheperformancestatistic, datamin
ing does not work. That is to say, testing more rules does not lead to the
discoveryofbetterrules. Thefiat curveshowsthatwhenonlytwomonths
were used to compute the mean return ofeach ATR, the expected return
ofthe best performing ATR out of256 ATRs was no higher than the best



==================================================
                     PAGE 327                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 311
performingATR when only one was tested (i.e., no data mining). The im
portance ofsamplesize cannotbeoveremphasized!
Now let'sgo back and address thefirst question: Does dataminingse
lectruleswith higherexpected returns thansimplypickingone atrandom
from the universe? Figure 6.43 speaks to this question as well. Note the
curvebasedontwoobservationsremainsstuckat+1.4percentasthenum
berofATRstested isincreased.Areturnof+1.4percentistheexpectedre
turnoftheaverageATRintheATRuniverse. Inotherwords, ifonewere to
graban ATRatrandomfrom the universeand noteitsexpectedreturn and
repeatthismany times, onaverage theexpectedreturnoftherandomlyse
lected rule would be +1.4 percent. Figure 6.41 tells us that when onlytwo
months ofobservations are used, picking the ATR with the best-observed
performance is no betterthan pickinganATRatrandom. Thefact thatthe
curve is flat says that itdoes not matter how manyATRs are tested. How
ever, the curves based on 100 and 1,000 observations rise well above the
universeaverage. Forexample, whena bestispickedfrom 256ATRs using
1,000monthsofhistorytocomputetheobservedmeanreturn, itsexpected
return is+10percentayear, farabovetheuniverseaverageof+1.4percent.
The bottom line: When there aTe a sufficient number ofobservations a
rule's observedmte ofTeturn is a useful, thoughpositively biased, indi
catorofits expected rate ofTeturn. However, when there are toofew ob
servations, the observedTate ofreturn isvirtuaUyuseless.
Data-Mining Bias as a Function ofUniverse Size:
In a Universe of Variable Merit
The previous section established that the payoff from data mining in
creasesasthenumberofrulestested is increased, givensufficientsample
size. This shows that observed performance of the best rule is a useful,
though positively biased, gauge of its expected return. The question is:
How large is this bias associated with the best rule (highest observed re
turn) when the universe is composed ofrules thatdiffer in terms oftheir
expectedreturns?
The figures thatfollow (Figures 6.44, 6.45, 6.46, 6.47) are plots ofthe
data-mining bias (vertical axis) versus the number ofrules backtested in
a rule universe where merit is distributed according to the distribution
displayed in Figure 6.42. Each plot is based on a different number of
monthly observations for computing mean return: 2, 100, and 1,000
months, respectively. First, each curve is shown individually (Figures
6.44,6.45, and 6.46). Figure 6.47 shows the three curves placed on top of
one another. Itgives a sense ofthe relative magnitude ofthe data-mining
biasatdifferentsamplesizes.



==================================================
                     PAGE 328                     
==================================================

312 METHODOLOGICAL.PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
240
..
..··0
Data
Mining 180
Bias
Of
120
Best
ATR
Length ATR Performance
Annual
%Return 60 History: 2 Months
32 64 128 256
NumberofATRs Tested To Find Best
FIGURE 6.44 Data-mining bias versus number ofATRs tested in variable merit
rule universe using atwo-month performance history.
9.6
7.2
Data
Mining
Bias 4.8 .-
Of
Best Length ATR Performance
ATR 2.4 History: 100Months
Annual
% Return
1 32 64 128 256
NumberofATRs Tested To Find The Best
FIGURE 6.45 Data-mining bias versus number ofATRs tested in variable merit
rule universe using a 100 month performance history.
Figure6.44isbasedonusingonlytwomonthsofobservationstocom
pute each ATR's observed return. The figure clearly shows the problem
created by using a small sample size. For example, the observed perfor
manceofthebestperformingATRoutof256ATRsoverstatesitsexpected
return by over200percentperyear. Forshort performance histories, the
data-mining bias is extreme. However, for the 100-observation case,
shownin Figure 6.43, andfor the 1,000observation case shown in Figure
6.44, the bias is dramatically smaller. For example, the bias associated
with the best ATR of 256 is approximately 18 percent when 100 months
are usedandlessthan3percentwhen 1,000monthsareused.



==================================================
                     PAGE 329                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 313
3.0
Data
2.4
Mining
Bias
Of 1.8
Best
ATR 1.2 Length ATR Performance
Annual History:1000Months
% Return 0.6
1 32 64 128 256
NumberofATRs Tested To Find The Best
FIGURE 6.46 Data-mining bias versus number ofATRs tested in variable merit
rule universe using a 1,ODD-month performance history.
240
~~~ //./~~~~h~;~>
Data
Mining
Bias
Of
/ ••••••......
Best
ATR
Annual
% Return :~ ~~~.t~~~.1_0~ :::~~:n:t~~~:1:~:J
/ __.
1 3264 128 256
NumberofATRs Tested To Find The Best
I~IGURE 6.47 Data-mining bias versus number ofATRs tested in variable merit
rule universe.
There are two messages expressed by Figures 6.44, 6.45, and 6.46.
First, the numberofobservationshasamajorimpacton data-miningbias.
Second, when the number of observations used to compute rule perfor
mance is large, the bias levels out quickly. This is seen Figure 6.46 based
on 1,OOO-monthperformancehistories. Thedata-miningbiasofthe bestof
16ATRsis notmuchworse thanthe bestof256. Figure 6.47superimposes
thepriorthreefigures. ItshowsthatsearchingalargernumberofATRsto
find a best(i.e., more datamining) does notincreasethe magnitude ofthe
bias once the number ofATRs tested exceeds a modest number. In other
words, the penaltyfor testing more rules does not increase, once a rather



==================================================
                     PAGE 330                     
==================================================

3.4 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
low threshold number of rules has been exceeded, provided, of course,
that a sufficient number of observations has been used to compute the
performancestatistic.
How quickly the data-mining bias stabilizes once this threshold is
passed can be seen more clearly in Figure 6.48, which is a portion ofthe
curve for the 100-month observation case magnified. It shows the region
along the horizontal axis where the number ofATRs tested is between 1
and 40. After approximately30ATRs have been examined, increasing the
number tested has a minimal effect on the size of the data-mining bias.
This is good news for the dataminerwho utilizes an adequate number of
observations. The bottom line: testing a larger number ofrules is a good
thing, and the penaltyfor doingso drops offquicklyifasufficientnumber
ofobservationsareused to computetheperformancestatisticthatisused
forruleselection.
Data-Mining Bias as a Function ofNumber of
Observations: In a Universe ofVariable Merit
This section presents a series ofplots showing the relationship between
the magnitude ofthe data-mining bias (vertical axis) and the number of
monthly observations used to compute the observed performance statis
tic (horizontalaxis). Thiseffectwasvisible onpriorplotsbyshowingsep
arate curves ofthe bias versus number ofATRs for distinct values of 2,
100, and 1,000months. However, the plots presented in this sectionshow
the relationship between the bias and number of observations along a
continuumofthe numberofmonths.
16.8
Data
14.4
Mining
Bias 12.0
Of
9.6
Best
ATR 7.2
Length ATR Performance
Annual
4.8 History: 100 Months
% Return
2.4
10 20 30 40
Number ofATRs Tested To Find The Best
FIGURE 6.48 Data-mining bias stabilizes quicklywhen sample size is sufficient.



==================================================
                     PAGE 331                     
==================================================

Data-Mining Bias: The Fool's Cold ofObjective TA 315
Each plotshows the data-mining bias as a function ofnumber ofob
servations, withthatnumberrangingfrom 1month to 1,024months. What
wewouldexpecttosee, onthe basisofpreviousfindings, isforthebiasto
belargewhenthenumberofobservationsissmall.Thisisindeedwhatoc
curs. The message tothe dataminerisclear: Heedthe LawofLarge Num
bers. Figures 6.49, 6.50, and 6.51 show the data-mining bias versus the
number ofobservations used to compute mean ATR return for three dif
ferentcases: bestATRof2, bestof10,and bestof100,respectively. Figure
6.52showsthe three curvessuperimposed.
24
Data 21
Mining
18
Bias
Of 15
Best
12
ATR
Annual 9
% Return 6
3
200 400 600 800 1000
NumberofObservations to Compute Mean Return
FIGURE 6.49 Data-mining bias versus numberofobservations for best ATR of2.
60
Data
Mining 48
Bias
Of
36
Best
ATR
Annual 24
%Return
12
200 400 600 800 1000
NumberofObservations to Compute Mean Return
FIG HE6.50 Data-mining biasversus numberofobservationsforbestATRof10.



==================================================
                     PAGE 332                     
==================================================

316 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
96
Data 84
Mining
72
Bias
Of 60
Best
48
ATR
Annual 36
% Return
24
12
200 400 600 800 1000
NumberofObservations to Compute Mean Return
FIGURE 6.51 Data-mining bias versus number of observations for best ATR
of 100.
84
Data
Mining 72
Bias 60
Of
Best 48
ATR 36
Annual
% Return 24 ~B;stof2-'
12 _______1
200 400 600 800 1000
NumberofObservations to Compute Mean Return
FIGURE 6.52 Data-mining bias versus numberofobservations for bestATR of2,
10, and 100.
Observed Performance and Expected Performance
versus Number ofObservations in a Universe
ofVariable Merit
Figures6.53and 6.54 displaytwo curves. The uppercurveisthe observed
performance of the best-performing ATR amongst, say, 10 ATRs. The
lowercurveshowsthe expectedreturnforthat best-performingATR. The
vertical axis represents annualized return, both observed and expected.
The horizontal axis represents the number of monthly observations. So,
for example, in Figure 6.53, pointAonthe upper curverepresents the ob
servedperformanceofthebestATRoutof10when400observationswere



==================================================
                     PAGE 333                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 317
36
Observed
30 Performance
%
BestATR of 10
Return
24
Per ~------- _.- -:
Expected :
Year 18
: Performance :
~ BestATR of 10 ~
12
A ) _ _~
6 ......., .....•.~ A{ .
1 200 400 600 800 1000
Number ofObservations to Compute Mean Return
FIGURE 6.53 Observed and expected performance best ATR versus number of
observations based on 10ATRs.
Observed
60 Performance
%
Best ATR of 500
Return 48
Per ............................
~ Expected
Year
36 : Performance
24
...J.B.e.~~..~T.~..?~ ..5.~.O'.:
12
............................................................
..----
1 200 400 600 800 1000
NumberofObservations to Compute Mean Return
FIG RE 6.54 Observed and expected performance best ATR versus number of
observations.
used to compute theATR's observedmean return. PointBrepresents that
ATR's expected return. The vertical distance between them is the data
mining bias, given these specific conditions: best of 10 ATRs and 400
monthlyobservations.
Here is what we should anticipate, given the results of prior experi
ments. When the numberofobservationsis small, randomness will domi
nate merit, andobservedperformancewillbemuchgreaterthanexpected
performance-a large data-mining bias. This will show up on the plot as
the two curves beingfar apart. The large vertical gap between the curves
occurs when the performance statistic is based on few observations be
cause this permits one ATR to getvery lucky. However, as more observa-



==================================================
                     PAGE 334                     
==================================================

318 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
tionsare usedto computemeanreturn, the curvesshould convergeas ob
served performance more truly represents expected performance. How
ever,thecurveofobservedperformancewillalwayslieabovethecurveof
expectedperformancebecauseabest-of-selectioncriterionwillalwaysin
ducesomepositivebiaswhenluckplaysarole inobservedperformance.
These tests were run for two cases. Figure 6.53 shows the curves for
thebestATRof10. Figure6.54showsthemforthe bestATRof500.As ex
pected, the bias shrinks as the number ofobservations is increased (the
curves converge). Also, as expected, the bias is greater for the 500 ATR
case than for the 10 ATR case due to the greater opportunity for luck
whenmoreATRsareexaminedtofind abest.
Data-Mining Bias as a Function ofRule
Correlation: In a Universe ofVariable Merit Based
on 500 ATRs
We have previouslyseenthat when the returns ofthe candidate rules are
correlated, it reduces the data-mining bias. Correlation shrinks the bias
because correlation reduces the effective number of rules being exam
ined. In the extreme case, where all rules have perfectly correlated re
turns, there is really only one rule being tested. In this situation, no data
miningtakesplace, and sothe data-miningbiasshrivels to zero. These re
sults were for a universe ofrules ofequal merit; all had expected returns
ofzero. Nowresultsforavariable-merituniversearepresented.
Figure 6.55 plots the expected return (vertical axis) of the uni
verse's best-performingATRas the numberofATRsexamined (horizon
tal axis) is varied from 1 to 256. There are four curves, each
4.8
Expected 3.6
Return
%/ Year
2.4
......
1.2
1 4 16 6 4 256
NumberofATRs Tested To Find Best (log scale)
FIGURE 6.55 Expected return of best ATR versus number of ATRs. Tested at
four levels ofcorrelation: 0, 0.3, 0.6, 0.9. Numberofobservations= 100.



==================================================
                     PAGE 335                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 3.9
representing a different level of correlation in returns amongst the
ATRs from which the best is selected. The correlation levels are: 0.0,
0.3, 0.6, and 0.9. The number ofmonthly observations used to compute
ATR mean return issetat 100months.
The first thing to note about Figure 6.55 is that the higher the level of
correlation, the less the expected return ofthe best-performingone. This is
consistentwith the previousfinding thatwhenfewerATRs are back tested,
there is less opportunity to discover better ones (see Figure 6.43). Higher
correlation means we are effectively searching through a smaller universe
thantheactual numberofATRstestedwouldsuggest. Aspointedoutearlier,
intheextremecaseinwhichcorrelationis 1.0,thereisnodataminingatall.
When the correlation among the ATR returns is nearly perfect, 0.9, the ex
pectedreturnofthebestof256ATRsisonly+1.4percent.Thisisthesameas
anATRpickedatrandom.Inotherwords,atacorrelationof0.9theeffective
number ofATRs being tested is quite small (little data mining). Less data
miningmeanslessopportunitytofind goodrules. Conversely, whencorrela
tion among rule returns is zero, the effective size ofthe universe is maxi
mized-wereallyaresearchingamong256differentATRs.Consequently,the
expectedreturnofthe bestATRoutof256is nearly4percent, considerably
abovetheuniverseaverageof+1.4percent(dataminingworks!).
Summary of Findings Regarding the
Data-Mining Bias
This section has presented the results of numerous experiments. It is
worthwhile, at this point, to summarize ourfindings_ Theymake clearthe
nature of the problem that must be solved by the methods presented in
the nextsection.
1. The observed performance ofa rule discovered by datamining-that
is, the best performer in a set of back-tested rules-is positively bi
ased. Its expected future return is less than its observed historical
performance. Itslikelyperformanceinanysampleofdatafromthefu
ture will belowerthanthe back-testedperformancethatalloweditto
wintheperformance competition.
2. The magnitude ofthe data-mining bias is profoundly affected by the
number of observations used to compute the performance statistic
used to select the bestrule. The number ofobservations affects both
the width and tail thickness of the performance statistic's sampling
distribution.
a. The larger the number ofobservations, the narrower the sampling
distributionand thesmallerthe data-miningbias.



==================================================
                     PAGE 336                     
==================================================

320 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
b. The larger the number of observations, the lighter the tails ofthe
samplingdistributionandthesmallerthe data-miningbias.
3. The larger the number of rules searched, the larger the data-mining
bias.
4. Thesmallerthevariabilityintruequality(expectedreturn) amongthe
rules tested, the larger will be the data-mining bias. In other words,
the moreequivalentthepredictivepowerofthe rules backtested, the
greaterwill bethe bias. Thispointwas notexplored experimentally.
5. Data mining works when the sample size is large. The more rules
tested, the higherthe expectedperformance ofthe rule ultimatelyse
lected. Dataminingdoesnotworkwhenthesamplesizeis toosmall.
SOLUTIONS: DMLING WITH THE DATA-MINING BIAS
Dataminingworksbutitsresultsarepositivelybiased. Hence, ifthesetof
candidaterules examinedbythe dataminercontainsanyruleswithsupe
rior expected returns, data mining can be an effective way to find them.
However, their future performance will, in all probability, be less than
theirback-testedperformance. Thisout-of-sampleperformancedeteriora
tionissimplyafact oflifeinthe data-miningbusiness.
Aworse problem occurs when all candidate rules are worthless-all
have expectedreturns ofzeroorless. In this instance, the dataminerruns
theriskofbeingfooledbythe data-miningbiasand choosingarulewhose
expectedreturn iszero. Thisisthe fool's gold ofthe objective technician.
Thissectiondiscussessomeways toreduce this risk.
Three approaches have been proposed: out-of-sample testing, a data
mining correction factor proposed byMarkowitz and Xu, and randomiza
tion methods. Out-of-sample testing involves excluding one or more
subsets ofthe historical data from the data mining (out-of-sample). This
data is then used to evaluate the best rule discovered in the mined data
(in-sample). The selected rule's performance on the data that was insu
lated from mining operations provides an unbiased estimate of its ex
pected return in future data. A number of schemes for partitioning the
historical data into in-sample and out-of-sample segments have beenpro
posed. Isummarizeseveralofthemlater.
A second approach, one that has not been widely discussed outside
the academic literature, is based on randomization methods like boot
strappingand Monte Carlo. Thisapproachoffersa numberofadvantages.
First, it allows the data miner to test as many rules as desired. Testing a
greater number of rules increases the chance of finding superior rules.



==================================================
                     PAGE 337                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 321
Second, itdoes notrequire holdingdataaside, as isdone in out-of-sample
testing, thereby allowing all available historical data to be used for data
mining. Third, it permits significance testing, and, sometimes, approxi
mateconfidenceintervalscanbeproduced.
A third approach, the data-mining correction factor developed by
Markowitz andXu, deflates the observed performance ofthe rule thatdid
the best. The readerisdirected tosource materialfor this approach. As a
pointofinterest,limitedexperimentsconductedwiththismethodsuggest
it can work surprisinglywell, but it can also fail miserably, depending on
theconditions.
Out-of-Sample Testing
Out-of-sampletestingisbasedonthe validnotionthattheperformance of
a data-mined rule,33 in out-of-sample data, provides an unbiased estimate
ofthe rule'sfuture performance. Ofcourse, due tosamplingvariation, the
rule mayperformdifferentlyin thefuture than itdoes initsout-of-sample
test, but there is no reason to presume it will do worse. Because out-of
sampleperformanceisunbiased, itmakessensetoreservedataforout-of
sampletesting.
Thisraisesthe issueofhowtobestsegmentthe historicaldataintoin
sample and out-of-sample subsets. Various schemes have been proposed.
The simplest is to create two subsets; the early portion ofthe historical
datais used for datamining while the laterportion is reservedfor out-of
34
sample testing. See diagram Ain Figure 6.56. Amore sophisticated seg
mentationschemebreaksthehistoricaldataup ina checkerboardpattern
such that both in-sample and out-of-sample data come from all parts of
thehistory. See diagrams BandCinFigure6.56.
c.
B. ___I_n-:-S-=a-:m.:.!.p__le _1 Out-of-Sampie 1__I_n_-S_a_m.:.!.p__le _
A. In-Sample Out-of-Sample
---------Time----------+
FIGURE 6.56 In-sample and out-of-sample segmentation ofhistorical data.



==================================================
                     PAGE 338                     
==================================================

322 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Walk-Forwal'd Testing. Another data segmentation approach spe
cific to financial market trading applications is walk-forward testing. Itis
describedinPardo,35DeLaMaza,36KatzandMcCormick,37andKaufman.38
Itemploysamovingdatawindow, whichitselfisdivided intoanin-sample
and out-of-sample segment. The terminology used for the in-sample and
out-of-sample segments is somewhat different, however. Because walk
forward testing has a dynamic aspect, in which the rule is being modified
over time as the market evolves, the terminology alludes to a rule that is
learningfrom itsexperience. Thus the in-samplesegmentisreferred to as
the training data set because the best parameter values for the rule are
learnedin this portion ofthe data. The out-of-sample segmentis referred
to asthe testing dataset, because the parametervalues thatwerelearned
inthetrainingtestare testedforperformanceinthis datasegment.
The notion of a moving data window is well known in time series
analysis and technical analysis. Moving averages, moving max-min price
channels, and rate-of-change indicators, to name a few, employ a data
window thatslides along the time axis. Mathematical operations, such as
determining the maximum value orthe average value, are confined to the
datain the window.
In the context ofrule testing, the data window (training set + testing
set) that is walkedforward is sometimes referred to as afold (see Figure
6.57). An out-of-sample performance estimate is computed for each fold.
Because there are typically numerous folds and hence numerous out-of
sample estimates, it becomes possible to determine the variance of the
performance statistic. This allows a confidence interval to be computed.
Moreover, the moving data window adds a dynamic adaptive aspect to
rule testing. Anew rule can be formulated in each new realization ofthe
data window. This makes walk-forward testing especially attractive for
nonstationaryphenomenasuch asfinancial markets. Hsuand Kuan found
adaptive trading rules (they referred to them as learningstrategies) effec
39
tive in theirtestofover36,000objectiverules.
In a walk-forward scheme, the trainingsegment is usedfor datamin
ing, and then the best rule found there is tested on the testing segment.
This yields an unbiased estimate ofits out-of-sample performance. Then,
theentirewindowismovedforward and the processisrepeated.
The walk-forward process is illustrated in Figure 6.57. Note, the win
dow is moved forward by a sufficient amount so that testing data seg
mentsinseparatefolds do notto overlap. In this way, the ofout-of-sample
performance estimatesare independentofeach other.
Limitations of Data Out-of-Sample 'resting Methods. As anan
tidote to the data-mining bias, out-of-sample testing suffers from several
deficiencies. First and foremost, the virginal status of the data reserved



==================================================
                     PAGE 339                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 323
1-----------1 t-
Train Test Time----+
Fold
3
I---------l A
Train Test f-----Time----+
-----'------
Fold
2
1----1 A
Train Test f--------Time----+
Fold
1
K__ A
li_ra_in__ Test -----------Time----+
FIGURE 6.57 Walk-forward datatesting two data segments.
forout-of-sampletestinghasashortlifespan.Itislostassoonasitisused
onetime. Fromthatpointforward, itisno longerabletoprovideunbiased
estimatesofrule performance.
Asecond deficiency is that it eliminates certain portions of the data
from mining operations. Thus it reduces the amount ofdata available to
find patterns. When noise is high and information is low, more observa
tions areextremelyvaluable.
Third, the decision about how to apportion the databetween the in
sample and out-of-sample subsets is arbitrary. There is no theory that
suggests what fraction of the data should be assigned to training and
testing. Results can be very sensitive to these choices. Often it's a seat
of-the-pants call.
Markowitz/Xu Data Mining Correction Factor
A formula for estimating the data-mining bias has been proposed by
Markowitz and Xu40 (MX) and described for practical implementation in
anExcelspreadsheetbyDeLaMazaYMarkowitziswellknownforhispi
oneering work in modem portfolio theory, for which he shared the 1990
Nobelprizeineconomics.
The MX method does not require data segmentation. What is re
quired, though, is the complete setofintervalreturns (e.g., daily, weekly,
monthly, and such) for all rules examined by the data miner en route to
selectingabest-performingrule. Themethodcorrectsforthe data-mining
bias byshrinkingthe observedperformance ofthe bestrule backtoward
the average performance of all rules tested. The degree of shrinkage is
determined by several factors including: (1) the degree of variation in
each rule's daily returns around the grand mean daily return for all rules



==================================================
                     PAGE 340                     
==================================================

324 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
tested, (2) the degree ofvariation in each rule's mean return around the
mean return for all rules tested, (3) the number of rules examined, and
(4) the numberoftimeintervals (e.g., days) ofreturns.
The article by De LaMaza, which was referred to earlier, lays outthe
detailsforconstructingaspreadsheetthatcomputesthevalueofashrink
agefactor, B, whichis a numberbetweenzeroand one. Thisvalueis then
pluggedintothe formula
H' = R+ B(H- R)
Where
H' is the expected return for the best-performing rule after adjust
mentforthe data-miningbias.
R isthe average returnofallthe testedrules.
H isthe observedperformance ofthebestrule.
B istheshrinkagefactor.
Thecorrectedorshrunkenperformanceestimateforthe bestrule,H,
willliesomewherebetweenitsobservedperformanceandtheaveragefor
allrules tested. WhenB equalszero, then the expectedreturnfor the best
rule issimplyequaltotheaveragereturnofallrules (R). AsB approaches
a value of 1, the expected return of the best rule approaches its back
tested return. This occurs in a case where no data-mining bias was de
tectedandthusnoneedtoshrinkthe observedreturn ofthe bestrule.
The result given by MXis very approximate. Undersome conditions,
the shrunken performance estimate can be quite good. However, under
other conditions easily encountered in practice, its results can be seri
ouslyinerror. MXisbestusedasa roughguideline.
Randomization Methods
Chapter 5 introduced two randomization methods that can be used to test
the statistical significance ofa rule's observed performance in a back test.
They were bootstrapping and Monte Carlo permutation. As pointed out in
Chapter5, thesesignificancetestswereonlyvalidforasinglerule backtest,
thatis, where there was no dataminingusedto discoverthe rule. However,
with modifications described in this section, these procedures can also be
usedtotestthestatisticalsignificanceofarule discoveredwithdatamining.
Bothrandomizationmethodsallowthe dataminerto useallhistorical
datafor rule discovery. This circumvents the arbitrary decisions required
with out-of-sample testing: how to partition the data into in-sample and



==================================================
                     PAGE 341                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 325
out-of-sample, how much to use for training versus testing, and so on.
However, the randomization methods require that certain information
about each rule tested during data mining be retained. Because the spe
cific information that must be retained depends on whether Monte Carlo
or bootstrapping is used, the information requirements of each method
willbediscussedseparately.
Abriefreview is in orderto explain how randomization methods can
beusedforsignificancetesting.Thesamplingdistributionofa test-statistic
is the foundation of statistical inference. It describes the degree of ran
domvariation in astatistic. Thisdistributionisvitalinjudgingthestatisti
cal significance ofa rule's observed performance. Chapter 5 pointed out
that the sampling distribution can be derived by bootstrapping or the
Monte Carlopermutation.
Bootstrapping the Sampling Distribution: White's Reality
Check. White's Reality Check (WRC) was invented and patented by
economistHalbertWhite, aprofessorattheUniversityofCaliforniaatSan
Diego. It uses the bootstrap42 method to derive a sampling distribution
thatisappropriate totestthestatisticalsignificance ofthebestrule found
bydatamining. Thesoftware can be acquiredfrom Quantmetrics,43a firm
ownedbyDr. White.
Prior to WRC, bootstrapping could be used to generate the sampling
distribution to testthesignificance ofasinglerule. White'sinnovation, for
which he was granted a patent, allows the bootstrap to be applied to the
best rule found by data mining. Specifically, WRC permits the data miner
todevelopthesamplingdistributionforthebestofN-rules, whereN isthe
numberofrulestested, undertheassumptionthatalloftheruleshave ex
pectedreturnsofzero. Inotherwords, WRCgeneratesthesamplingdistri
bution to test the null hypothesis that all the rules examined during data
mininghaveexpectedreturns ofzero.
Let's take a highly simplified example to explain the procedure WRC
uses. Supposethe dataminerhas backtested onlytwo rules (N = 2) over
10 days. I will refer to the rules as R] and R • Over the lO-day period R
2 j
earned a mean daily return of1percent, whereasR earned a mean daily
2
return of2percent. The dataminerpicksrule 2as the bestperformer, but
thenwonders, asoneshould,ifthe2percentwastheresultofdata-mining
bias and ifits true expected return is actually zero. To generate the sam
pling distribution that would be appropriate for this data-mining venture,
thefollowing stepsaretaken:
1. The rules are tested over 10trading days. Thus 10slips ofpaperare
used to represent each date oftrading. They are numbered from 1
through 10andplaced in a bucketforsampling.



==================================================
                     PAGE 342                     
==================================================

326 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
2. Theslipsofpaperare thensampledwithreplacement10times. Each
time a date iswithdrawnfrom the bucket, the date numberis noted.
The number ofsamplings is 10 because that was the length ofeach
rule's performance history. This is a requirement of the Bootstrap
Theorem. Suppose the following dates were selected: 5, 3, 5, 5, 7, 2,
5,8,1,2.
3. Using the dates obtained in step 2, a pseudo-track record based on
theactual dailyreturns associated with these dates is createdforR
•
I
This procedure is also carried out for R We now have a randomly
.
2
generatedperformancehistoryoflength 10for eachrule.
4. Recall that our goal is to generate the sampling distribution for the
best-performing rule in a universe ofrules which have expected re
turns equal to zero. Because itispossible, in fact itis ourhope, that
at least one ofthe rules tested has an expected return greater than
zero, therawhistoricalreturnsofR) andR mustbeadjustedtohave
2
meanreturns ofzero. Toaccomplishthis, WRC determines themean
dailyreturnforeachruletestedandsubtractsthisamountfrom each
individual daily return.44 In this example, R)'s mean daily return is 1
percent and R 's is 2percent. Thus 1percent is deducted from each
2
ofR/s daily returns whereas2percentis deductedfrom eachofR 's
2
daily returns. This has the effect ofshifting the distribution ofdaily
returns for each rule so that itis centered overzero. These adjusted
dailyreturns are nowused.
5. The adjusted returns ofR for days 5, 3, 5, 5, 7, 2, 5, 8, 1, 2are aver
J
aged. The adjustedreturnsforR arealsoaveraged.
2
6. The larger ofthese two mean values is held aside as the first value
used toform thesamplingdistribution ofthe maximum mean return
ofN rules. InthiscaseN = 2.
7. Steps2 through6are repeated manytimes (e.g., 500ormore times).
8. The sampling distribution of the statistic, maximum mean return
in a universe of N (2) rules whose expected returns are zero is
formed from these 500values.
9. With the sampling distribution now in hand, the p-value can be
computed.
10. The p-valuewould simplybethe fraction ofthe 500valuesobtained
atstep 7thatexceedtheaveragereturn ofthetestedrule. (SeeChap
ter5for the procedure.)
The example describedabovewasintentionallyhighlysimplified. Ina
more realistic case the dataminer would examine a much larger number
ofrules overamuchlongertime period. Butthe procedurefor generating



==================================================
                     PAGE 343                     
==================================================

Data-Mining Bias: The Fool's GoldofObjective TA 327
the sampling distribution would be the same. All this is done automati
callybythe WRC software, onceithas beenprovidedwiththe intervalre
turns, (e.g., daily) of all rules tested. This set of data contains all the
informationWRC needs to derive the sampling distribution that is appro
priate for a particular data-mining venture. When many rules are tested
this can be a considerable amount ofdata. Currently, few data miners or
data-miningsystemssavethisvaluableinformation.
In PartTwo ofthis bookIwill performa case studyof6,402 rules for
trading the S&P 500 index. Aversion ofWRC that incorporates some re
centenhancements,whicharedescribedlater,willserveasonemethodto
testthestatisticalsignificance ofthe rules' observedperformances.
'fhe Monte Ca"'o Permutation Method. The Monte Carlo permu
tationmethod(MC) canalso beusedtogeneratethesamplingdistribution
required to evaluatethestatisticalsignificance ofrulesfound bydatamin
ing. As noted in Chapter5, ituses differentdatathan the bootstrap and it
testsadifferentformulation ofthe nullhypothesis, orH '
o
Intermsofthedata, MCusesthehistoricaltimesseriesofa rule's out
putvalues(i.e., +1,-1) aswellasthemarket's raw returns. Incontrast,the
bootstrap uses the rule's interval (e.g., daily) returns. In terms ofthe H '
o
bothWRCandMCtestthe notionthatallrules examinedduring datamin
ing have no predictive power. However, MC formulates H somewhat dif
o
ferently. Tosimulatetheperformanceofarule devoidofpredictivepower,
MC randomlypairsrule outputvalues withdaily marketprice changes. In
contrast to WRC, MC's random coupling of output values and market
changes is done without replacement. Presumably, if rule output values
are randomlyassigned tomarketchanges, theperformancewouldbecon
sistent with the null hypothesis that the rule is worthless. The return
earned bytherandompairingsbecomesthebenchmarkagainstwhichthe
actual rule returns are compared. Ifa rule has genuine predictive power,
thenitsinformedpairingofoutputvaluesandmarketchangesshouldgen
eratesignificantlybetterperformance.
The performance of the random pairing is simply the rule's value
(+1or-1) multipliedbythemarketchangerandomlyassignedtothatday.
Thus, ifa rule output of+1gets pairedwith a day when the market rose,
the rule earnsthemarket'sprice change. Likewise, ifa-1 rule outputgets
randomlypairedwithadayonwhichthemarketfell, itearnsthatpositive
amount that day. Losses occuron days when the sign ofthe rule's output
valueisoppositeto thatofthe market'sprice change.
Afterthefull historyofa rule's outputvalues have beenpairedwith a
marketprice change, the mean return is computed. This is done for each
rule intheuniverseofN rulesgivingN meanreturns. Themaximummean
return is selected, and this becomes the first value used to construct the



==================================================
                     PAGE 344                     
==================================================

328 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
MC sampling distribution. This entire process is repeated many times
(>500) with different random pairings each time. The sampling distribu
tionisgeneratedfrom this large numberofvalues.
Note thatthere isno need to repositioneachrule's returndistribution
to be centered overzero as is done with WRC because itassumes thatall
ruleshaveanexpectedreturn ofzero. Incontrast, the MCsamplingdistri
bution's mean is the expected return of a useless rule in a data-mining
venture. If, after randomly assigning rule values to market returns, the
sampling distribution is situated at a value greater than zero, so be it. p
values can be computed directlyfrom the MC generatedsamplingdistrib
ution, just as they would be with a sampling distribution generated by
alternativemethods. Thep-valueistheareainthe righttailofthe distribu
tionthatliesatorbeyondthe back-testedrule'smean return.
Alimitation ofthe MC method comparedto WRC is that it cannotbe
used to generate confidence intervals because it does not test a hypothe
sis about the rule's mean return. MC's null hypothesis is simply that all
rules tested have output values that are randomly correlated with future
marketbehavior.
To recap, the steps by which the MC permutation method generates
thesamplingdistributionfora dataminerare:
1. Obtainthe dailyrule outputstatesfor allN rules examined duringthe
data-miningventure.
2. Each rule's outputvalues are randomly paired with a scrambled ver
sion ofthe actual future market price changes. Note that it is impor
tant the same pairings be used for all rules. Thus, if a rule's output
value for day 7is paired with the market return for day 15, this same
pairingmustbe done for allcompetingrules. This isdone to preserve
correlationstructurethatmaybe presentin the rules, whichis oneof
thefive factors impactingthe data-miningbias.
3. Determinethe mean rate ofreturnforeachrule.
4. Out of these N rules, select the highest mean return. This value be
comesthefirstvaluefor thesampling distribution ofthe meanfor the
bestmeanamongN uselessrules.
5. Repeat steps 2, 3, and 4, M times (where M =500 or some large
number).
6. Formthe sampling distributionfrom theM values obtained insteps 2
through5.
7. Thep-valuefor a back-testedrules canbe determined bydetermining
the fraction of the values obtained by step 6 that are equal to or
greaterthan itsmean return.



==================================================
                     PAGE 345                     
==================================================

Data-Mining Bias: The Fool's ColdofObjective TA 329
Potential Flaws in Initial WRC and MCP Methods. Both WRC
and MC testthe nullhypothesisthatall rules inthe data-mined universe
are useless. The rule miner hopes to reject this hypothesis in favor of
the alternative hypothesis: that not all the rules examined are useless.
As pointed out earlier, the meaning ofthe term useless depends on the
randomization method used. In the context of WRC, a useless rule has
an expected return equal to zero. In the context of the MC, useless
refers to a rule whose outputvalues are randomly paired with the mar
ket'sfuture change.
As was discussed in Chapter5, a hypothesis testcan errin two ways.
Atype-I error occurswhen the null hypothesis is true (all rules tested re
allyareuseless) isrejectedin error. Thisoccurswhena rule thatisinfact
uselessmanaged to earninga highmean return and alowp-value byluck.
Thisisthe dataminer'sfool's gold.
A type-II error occurs when the null hypothesis is in fact false and
should be rejected, butthe rule's performance is too low to doso. Thatis
to say, the rule really does have predictive power, but its virtue remains
undetected becauseitexperiencedbadluckduringthe backtest. Thedata
minerwinds up leavingrealTAgoldin the ground.
The ability of a hypothesis test to avoid making a type-II error is
referred to as the test's power, not to be confused with a rule's predic
tive power. Here, we are speaking of the power of a statistical test to
detect afalse H ' So a hypothesis test is characterized by two measures
o
ofgoodness: its significance (probability ofmaking a type-I error) and
its power (probability of avoiding a type-II error). It is with respect to
the issue of power that initial versions of WRC and MCP are open to
criticism.
As pointed out by economistPeterHansen,45 WRC and, by extension,
the MCP would bothsuffer a loss ofpower (increased likelihood ofmak
ing a type-II error) when the data-mined universe contains rules that are
worsethanthebenchmark.Thisreferstoasituationinwhichoneormore
rules are actually worse than useless-they have expected returns that
are less than zero. This could occur, for example, ifone were to reverse
the outputvalue ofa rule that has an expectedreturnthat is truly greater
than zero. Because approximately halfofthe rules tested in Part Two of
this book are obtained in this fashion (i.e., inverse rules), the research
presentedinPartTwoofthisbookisvulnerable to theproblemdiscussed
byHansen.
To evaluate Hansen's criticism, the powerofboth WRC and MCPwas
evaluated by Dr. Timothy Masters, on an ATR universe that contained
rules that were both betterthan and worse than the benchmark. In other
words, the evaluation simulated exactly the case that troubles Hansen.
Thesetestsshowedthat hisconcernsarejustifiedfor asituationin which



==================================================
                     PAGE 346                     
==================================================

330 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
a rule with negative expected returns has an extremely large variance in
returns. This can drastically reduce the power of both WRC and MCP.
However,incasesthatoneismorelikelytoencounterinTArule datamin
ing, both methods are reasonably resistant to this problem. Therefore,
when there is one or more superior rules in the universe of examined
rules, bothWRCand theMCPmethodhaveareasonablechanceofdetect
ingthem.
Recent Enhancement to WRC and ~lCP. Despite the reasonable
powerofthe initialversionsofWRCandMCPmethods, arecentpaperby
Romano and Wolfl6 recommends a modification that enhances the power
ofWRCand thatalso appearsto enhancethepowerofMCP. Thus the Ro
mano and Wolf enhancement reduces the probability of a Type-II error.
This modification was introducedto the versions ofWRC and MC used in
Part Two's case study. However, at this time, there is no commercial ver
sion ofWRC with this enhancement. The MCP version with the enhance
ment is being placed in the public domain and codes for it will be made
availableonawebsite, www.evidencebasedta.com.



==================================================
                     PAGE 347                     
==================================================

Theories of
Nonrandom
Price Motion
At the risk of stating the obvious, ifmarket fluctuations were com
pletelyrandom, TAwould be pointless. TAis ajustifiable endeavor
if and only if price movements are nonrandom to some degree
some portion ofthe time. This chapter offers several theories explaining
why episodes ofnonrandom price movements oughtto occur in financial
marketprices.
Although the occurrence of nonrandom price motion is necessary
to justify TA, this alone is not sufficient to justify any specific TA
method. Each method must demonstrate, by objective evidence, its
ability to capture some part of the nonrandom price motion. Part Two
will evaluate a large number ofTA rules with respect to their ability to
dojustthat.
THE IMPORTANCE OF THEORY
Several new theories from the field of behavioral finance explain why
price movements are nonrandom to some degree and therefore poten
tially predictable. Thus, these novel theories take a position that is con
trary to the efficientmarkets hypothesis (EMH), a cornerstone offinance
theory for over 40 years. EMH contends that price changes in financial
markets are random and, therefore, unpredictable. Therefore, these new
theorieshold outhopeforTA.
33.



==================================================
                     PAGE 348                     
==================================================

332 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
The reader may wonder why theoretical support for nonrandornness
is even necessary. Ifa TA method has a profitable back test, isn't that all
that should be needed? It could be asserted thata significantly profitable
back test not only establishes that markets are nonrandom, it also estab
lishesthatthemethodisable to exploitsomeportionofthe market'snon
randomness.
This view misses the importance oftheoretical support. Even when
all statistical precautions have been taken, a profitable back test that is
not supported by sound theory is an isolated result and possibly still a
lucky one. Asuccessful back test always provokes the question: Will the
rule continue to workin thefuture? Theorycanbehelpful here becausea
backtestthatis consistentwithsound theory isless likelytobe astatisti
cal fluke. When a back test has theoretical foundation, it is no longer an
isolatedfact, butpartofalargercohesivepictureinwhichtheoryexplains
fact andfact confirmstheory.
For example, the profitability oftechnical trend-following systems in
commodity futures can be explained as compensation (i.e., a risk pre
mium)forprovidingavaluableserviceto commercialhedgers, thatofrisk
transference. In other words, economic theory predicts that futures mar
ketsshouldmanifestenoughprofitabletrendsto motivate trendfollowers
to accept the price risks hedgers wish to shed. This is discussed later in
I
this chapter.
SCIEN'flFIC l'HEORIES
In everydayspeech, a theoryisa speculative conjecture aboutwhysome
thingisthewayitis. AsexplainedinChapter3, ascientifictheoryissome
thing different. First, it offers a succinctexplanation for a broad range of
prior observations. Second, and most importantly, it makes specific pre
dictions thatare laterconfirmedbynewobservations.
Kepler's laws of planetary motion2 concisely explained a large
number of prior astronomical observations and made specific predic
tions that were subsequently confirmed by additional observations.
However, as good as Kepler's laws were, Newton's theory ofgravitation
was better because it was broader in scope. It not only explained why
Kepler's laws worked but explained a far wider variety of phenomena.
Theories of nonrandom price motion need not predict with the accu
racy ofphysicaltheories. However, to be useful, theymustnotonlysuc
cinctly describe a wide variety ofpreviously observed market behavior,
but must also make testable predictions confirmed by subsequent
observations.



==================================================
                     PAGE 349                     
==================================================

Theories ofNonrandom Price Motion 333
WHAT IS WRONG WIl'I. POPUlAR 1)\ THEORY?
Many TA texts offer no explanation about why their proposed methods
work. ThestatementofJohn Magee, authorofone ofTA:sseminal works3
is typical. He said, "We can never hope to know why the market behaves
as it does, we can only aspire to understand how. History obviously has
repetitive tendencies andthat'sgood enough."
SomeTAtextsdoofferexplanations, butthesearetypicallyad-hocra
tionales that generate no testable predictions. According to author John
Murphy, the cornerstone premise ofTA is "anything that can possibly af
fect the price-fundamentally, politically, psychologically, orotherwise
is actually reflected in the price of that market."4 One would be hard
pressedto extracttestablepredictionsfrom sucha vaguestatement.
Actually, this statement, which purports to explain why TA works,
contains a logical contradiction. For, ifit were true that price did reflect
(i.e., discount) all possible information, it would imply thatprice was de
void of any predictive information. To understand why, assume a hypo
thetical pricepatternhasjustoccurredinstockXYZ, whichiscurrentlyat
$50, and based on the pattern a move to $60 is implied. Then, by defini
tion, when the pattern occurs its predictive implication has yet to be re
flected inprices. However,ifthepremisethatpricereflectsallinformation
weretrue, thepricewouldalreadybeatthelevelpredictedbythepattern,
thusnegatingthe pattern'sforecasting ability.
This logical contradiction is even more apparent when it is pointed
outthat EMH, a school ofthoughtthatrejects the efficacyofTA, rests on
the very same premise; "prices fully reflect all available information."5
Technical analysis cannotbe based on the samepremise as its mortal en
emy. Thiscontradictionisan example ofcloudyuncritical thinking thatis
all too commonin thepopularversionofTA.
Fortunately for TA, its cornerstone premise that price reflects all in
formation appears to be contradicted by fact. One example is the so
called underreaction effect, which has been postulated by behavioral
finance. Itsays that, because prices sometimes fail to respond to new in
formation as rapidly as EMH theorists contend, a systematic price move
ment, ortrend, toward a price levelthatdoes reflectthe new information
occurs. Thefailure ofpricestorespond rapidlyiscausedbyaseveralcog
nitive errors that afflict investors, such as the conservatism bias and the
anchoringeffect. Theseare discussed laterin this chapter.
Another popularjustification of TA is based on pop psychology. By
poppsychology, Imeanprinciplesofhumanbehaviorthatseemplausible
but lack scientific support. According to noted economist and authority
in the field ofbehavioral finance, Robert Shiller, "In considering lessons
from psychology, it mustbe noted thatthe manypopularaccounts ofthe



==================================================
                     PAGE 350                     
==================================================

334 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
psychology ofinvesting are simplynot credible. Investors are said to be
euphoric or frenzied during booms or panic-stricken during market
crashes. Inbothboomsand crashes, investorsare descriedas blindlyfol
lowing the herd like so many sheep, with no minds of their own."6 The
fact is, people are more rational thanthese pop-psychologytheoriessug
gest. "Duringthe mostsignificantfinancial events,mostpeoplearepreoc
cupiedwith otherpersonal matters, not with thefinancial markets atall.
So itis hard to imagine that the market as a whole reflects the emotions
described bythesepsychologicaltheories."? We will need to lookbeyond
the platitudes ofpopulartextsfor TA'sjustification. Fortunately, theories
developed in the field ofbehavioralfinance and elsewhere are beginning
to offerthe theoreticalsupportTAneeds.
THE ENEMl"S POSITION: EFFIl:IENT MAR.KETS
AND RANDOM WALKS
Before discussingtheories thatexplainwhynonrandompricemovements
should exist, we need to consider the enemy's position, the EMH. Re
cently, somehavearguedthat EMH does notnecessarilyimplythatprices
follow unpredictable random walks,8 and that efficientmarkets and price
predictability can coexist. However, the pioneers of EMH asserted that
random walks were a necessary consequence of efficient markets. This
sectionstatestheircaseandexaminesitsweaknesses.
What Is an Efficient Market?
An efficient market is a market that cannot be beaten. In such a market,
nofundamental ortechnicalanalysisstrategy,formula, orsystemcanearn
a risk-adjusted rate of return that beats the market defined by a bench
mark index. If the market is indeed efficient, the risk-adjusted return
earned by buying and holding the market index is the best one can hope
for. This is so because prices in an efficient market properly reflect all
known and knowable information. Therefore, the current price provides
the bestestimate ofeachsecurity'svalue.
According to EMH, markets achieve a state of efficient pricing be
cause ofthevigorouseffortsofnumerousrationalinvestorsattemptingto
maximize their wealth. In their pursuit oftrue value, these investors are
constantly updating their beliefs with the latest information in a proba
bilistically correct mannerf! so as to project each security's future cash
flows. Although no single investor is all knowing, collectively they know
asmuchascanpossiblybeknown. Thisknowledgemotivatesinvestorsto



==================================================
                     PAGE 351                     
==================================================

Theories ofNonrandom Price Motion 335
buyandsellinsucha waythatpricessettleatthe equilibriumorrational
price level.
In such a world, prices change only when new information arrives.
When it does, prices change almostinstantly to a new rational price that
properly reflects the news. Thus, prices do not gradually trend from one
rational price level tothe next, givingthetrend analysta chance to geton
board. Notatall. In theworldpositedbyEMH,pricestraceastepfunction
as they move almost instantly from one rational level to the next. Ifthe
news isfavorable, pricesrise, andifit'sunfavorable, pricesfall. Becauseit
is not predictable whether the news will be favorable or unfavorable (it
wouldn't be news ifit was), price changes will be unpredictable as well.
Thisisillustratedin Figure 7.1.
The Consequences ofMarket Efficiency:
Good and Bad
Market efficiency has good and bad in1plications. They are good for the
economy as a whole but bad-very bad-for TA. They are good for the
economy because rational pricessendvitalsignals ofassetvalues that, in
tum, encourage the efficient allocation ofscarce resources, such as capi
talandlabor. Thisfosters economicgrowth.
10
However, pricesinanefficientmarketare unpredictable renderingall
forms ofTAuseless. AccordingtoPaulSanmelson,financialmarketprices
Price
Negative News Event
:-R~tio-n~l:
, Price ,-- -
1 1
Time
Positive News E\ent
FIGURE 7.1 Efficient market's response to positive and negative news.



==================================================
                     PAGE 352                     
==================================================

336 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
follow a random walk because ofthe actions ofmany intelligent/rational
investors. In seeking to maximize wealth, they buy undervalued assets
pushing their prices higher and sell overvalued assets pushing prices
lower. "Takento itslogical extreme, itmeans thata blindfoldedmonkey
11
selecting stocksby throwing darts at a newspaper'sfinancial pages could
dojustaswellas one carefullyselectedbytheexperts."
12
"EMH rules out the possibility oftrading systems, based on available
information, thathaveexpectedprofitsorreturns inexcessofequilibrium
expected profit or return."13 "In plain English, an average investor
whether an individual, pension fund, or a mutual fund---cannot hope to
consistently beat the market, and the vast resources that such investors
dedicate to analyzing, picking and trading securities are wasted."14 Al
though, underEMH,itisstillpossibletogenerate positivereturnsfrom an
investmentstrategy,whenthosereturnsareadjustedforrisk, theywillnot
besuperiortothereturnofbuyingandholdingthemarketindexportfolio.
The efficient markets hypothesis also declares that when there is no
news entering the market, prices tend to oscillate in a random and unbi
ased fashion above and belowthe rational price level. See Figure 7.2. Be
cause this level is itself subject to uncertainty, no technical or
fundamental indicator can reliably indicate when prices are above or be
low it. This means, for example, that in an efficient market stocks with a
lowpriceto bookratio, awell known fundamental indicator, are no more
likely to appreciate than stocks with a high price to book ratio. This is a
coldworldfor anyone lookingforanedge.
Price
-------.
: Rational rI
Price
I
Time
FIGURE 7.2 Efficient market hypothesis (random and unbiased pricing errors).



==================================================
                     PAGE 353                     
==================================================

Theories ofNonrandom Price Motion 337
False Notions ofMarketEfficiency
Therearea numberofcommonlyheldbutfalse notionsaboutmarketef
ficiency. First is the idea that efficiency requires the market price to be
equal to rational value at all times. In fact, market efficiency simply re
quires that price deviates from rational value in an unbiased fashion.
Therefore, positive deviations from rational value are just as likely as
negative deviations, and at any point in time there is a roughly equal
chance that the asset's price will be above its rational value as below it.
Moreover, these deviations are random. Thus the time series of pricing
errors in an efficientmarket is a random variable whose probability dis
tribution is centered at the rational price and is approximately symmet
ricalaboutit.
Asecond false notion about market efficiency is that it implies that
no one can beatthe market. Actually, in any given time period, about 50
percent ofall investors will outperform the benchmark index, while the
remaining 50 percent underperform. The notion that no single investor
or money manager can beat the market over the long term is equally
false. Given enough players, a small number are likely to have lengthy
streaks ofmarketbeatingreturns eveniftheirstrategiesare totallywith
out merit. Nassim Talebl5 shows that even if we assume money man
agers are no better than a blind monkey throwing dartsl6 (probability of
beating the market = 0.5) and the universe ofmanagers is large enough,
say 10,000, afterfive years there is likelyto be about312firms who have
beaten the market five years in a row. These lucky folks are the ones
who will send salesmen calling. The other 9,682 are less likely to knock
onyour door.
The Evidence in Favor ofEMU
As previously discussed, a scientific hypothesis plays a dual role: expla
nation and prediction. Its veracity is tested by comparing its predictions
with new observations. As discussed in Chapter3, predictions are mean
ingfulonlyiftheyarespecificenoughto offerthe possibility ofempirical
falsification. If new observations do contradict a hypothesis's predic
tions, the hypothesis is either reformulated and tested anew or thrown
out. However, what should be inferred when observations and predic
tionsagree?
Onthisissuethereisdebate. PhilosophersinthecampofDavidHume
contend that no amount of confirmatory evidence is ever sufficient to
prove a theory, but in the practical world of science, confirmation does
addstrengthto a hypothesis. Iftestaftertestyields observations that are
consistentwithitspredictions,thatmeanssomethingtomostscientists.



==================================================
                     PAGE 354                     
==================================================

338 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
So EMH advocates have putforward a roster ofsupporting evidence.
Theirevidence comes in twoflavors because EMH comes in two testable
flavors: semistrongand weak. The semistrongform asserts itis impossi
17
ble to beat the market with public information. Because this includes all
fundamental andtechnicaldata, thesemistrongformpredictsthatneither
fundamental nor technical analysis can beat the market. This informal
prediction is notsufficientlyspecific to be testable, butitis in agreement
with the observation that people have great difficulty profiting consis
tentlyfrom marketfluctuations-and alottry.
The semistrongform also makes several specific predictions thatare
testable. One isthata security'spricewill reactquicklyand accurately to
anynews thatbears onitsvalue. The properresponse ofan efficientmar
ketto newsisillustratedinFigure 7.3.
This prediction implies prices should neither overreact nor underre
act to news. This is testable in the following way. Imagine for a moment
that prices did not behave efficiently and did systematically over- or un
derreact to new information. If this were the case, it would imply that
price movements would be predictable. Tosee why, considerwhatwould
happen ifa stock overreacted to news either bygoing up too much in re
sponsetogoodnewsordowntoo muchinresponseto bad. BecauseEMH
says pricesmuststay closeto rationalvalues, overshootingrationalvalue
would necessarily lead to a corrective movement back toward rational
value. This correctionwould notbe random meandering butratherapur-
Price
:-R~tio-n~1 :
, Price ,-- -
1 1
Time
Bullish News Event
FIGURE 7.3 Efficient market response to bullish news.



==================================================
                     PAGE 355                     
==================================================

Theories ofNonrandom Price Motion 339
poseful movement, as if the price were being magnetically drawn to the
rational level. Such a movement would be, to some degree, predictable.
Because EMH denies predictability, it must also deny the possibility of
overreactions to news. Ahypothetical overreaction to bullish news and a
subsequent systematic movement back to rational levels is illustrated in
Figure 7.4.
By the same logic, EMH must also deny the possibility ofunderreac
tiontonews, wherepricesmovelessthanthenewsjustifies.Inthiscase,a
systematic price movement would also develop as prices continued to
trend (drift) toward the new rational level implied bythe news. Underre
action to bullish and bearish news events and the resultant nonrandom
movementtoward rationalvalueare illustratedinFigure 7.5.
Evidence supporting EMH's prediction that overreactions and under
reactionsdonotoccurispresentedintheeventstudiesofEugeneFama.18
He examinedalltypesofcorporatenews eventssuchas earnings and div
idendannouncements, takeovers, mergers, andsoforth andfoundthataf
ter an initial, almost instantaneous, nonexploitable price movement
triggered bythe news, there was noadditional movement. These observa
tions are consistent with EMH's prediction that the markets quickly and
accuratelydiscountnewsevents.
19
Asecond prediction ofEMH is that prices should change only when
news arrives. Itfollows then, that prices should not change dramatically
in the absence ofnews or in response to noninformative events.20 These
Nonrandom Price Movement Backto Rational Price Level
R;tio-n~i:
:
Price ,-
I
1 1
Time
FIGURE 7.4 Overreaction to bullish news (implies nonrandom price movements).



==================================================
                     PAGE 356                     
==================================================

340 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Price
Negative News
-------.
:Rational
I
I 1 Price r 1
Trend: A Gradual Adjustment to New
Rational Price Level
Time
FIGURE 7.5 Underreaction to news can explain nonrandom trends.
two predictions suggest yet a third that is most crucial to TA. EMH im
plies that stale information, information already in the public domain,
should have no predictive power and be of no value in making invest
ment profits. Clearly, all information used by TA qualifies as stale. Thus,
EMH predicts that all TA methods should fail. If this were to be con
firmed, it'slightsoutfor TA.
The prediction that all stale information strategies should have no
predictive power seems cut and dried until one tries to test it. Recall,
EMH says an investment strategy based on stale information should not
be able to generate excess profits after risk adjustment. So the mere fact
thata strategybased on stale informationmakes gains is notsufficientto
rebut EMH. To contradict EMH, it must be demonstrated that the gains
are excessive afteradjustingfor thestrategy'srisk.
The notion of judging investment strategies on the basis of risk
adjusted returns is entirely reasonable. For example, ifa strategy makes
20percentperyearandthe benchmarkmakes 10percent, butthestrategy
exposes the investor to three times the risk, the strategy has not beaten
the indexafterrisk adjustment. The questionis: How does one define and
quantify risk?
Quantifying risk requires a risk model. The most well known is the
capital asset pricing modeFl (CAPM), which explains systematic differ
encesin the returns ofsecurities in terms ofa single risk factor, the secu
rity's relativevolatility. Thisis tosay, the risk ofa stockisquantified byits



==================================================
                     PAGE 357                     
==================================================

Theories ofNonrandom Price Motion 341
volatilityrelativetothevolatilityofthemarketasa whole. Thus, ifastock
is twiceasvolatileas the marketindex, its returnsshould betwice thatof
themarket. Suchastockissaidtoearnexcessreturnswhenitearnsmore
than twice the market return. This approach works well when measuring
the risk ofa single stock or a portfolio oflong stockpositions. However,
CAPM cannot quantify the risk exposures ofmore complexstrategies. In
response to this limitation, capital market theorists have proposed other
risk models. One of the best known is APT (arbitrage pricing theory),
which attributes risk toseveralindependentrisk factors.22
However, EMHadvocateshave tacitlygiven themselvesthe libertyof
cookingup newriskfactorswhenevertheywish. Thismakesitnearlyim
possible to refute their claim that stale information strategies cannot
beat the market. For, it is always possible to propose a new risk model
that explains away a strategy's excess returns as compensation for risk.
As explained in Chapter 3, this is not a scientific sin ifthe risk factor(s)
were identified before the stale information strategy earning excess re
turns was discovered. However, proposing a new risk factor(s) after a
market-beating strategy has been discovered is nothing more than an ad
hoc hypothesis. This is an eXP.lanation cooked up after the fact for the
specificpurpose ofimmunizinga theory, inthis case EMH, from falsifica
tion. This practice is frowned upon in science for good reason. Also, as
discussed in Chapter 3, the information content ofa scientific theory is
roughly equivalent to the opportunities it presents for falsification. The
unbridled ability to explain away any and all dissonant evidence effec
tively drains the theory ofits information content. Thus, the freedom to
concoct new risk factors to explain each and every investment strategy
thatbeatsthemarketwithstaleinformationeffectivelyreduces EMHto a
meaningless theory.
The evidence confirming the weak form of EMH, which states that
stale prices and indicators derived from them-such as price momen
tum-are without value, is based on auto-correlation studies. They mea
sure the degree to which price changes are linearly correlated with prior
changes, atvariouslagintervals.23Thesestudieshaveconfirmedthatprice
changes (Le., returns) are indeed linearly independent. This means thata
linear function of current and past returns cannot be used to predict fu
ture returns.24 Based on these findings, it was concluded by EMH advo
cates, thatsecurityreturns areanunpredictable random walk.
However, autocorrelation studies are relatively weak tests in the
sense that they can only detect linear dependencies. Searching for lin
earstructure is merelyoneway to determineifa timeseriesis behaving
in a nonrandom fashion. Edgar Peters25 and Andrew Lo and A. Craig
MacKinlay have shownthatalternative dataanalysis methodsindicate
26
thatfinancial market time series do not behave like random walks. For



==================================================
                     PAGE 358                     
==================================================

342 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
example, the test statistic used by Lo and McKinlay, called the variance
ratio, can detect more complex (nonlinear) nonrandom behaviors that
would be invisible autocorrelation studies.
CII!\LLENGING EMU
Agood theory is consistent in two ways. Its logic is internally consistent
and it is consistent with observed evidence. Thus a theory can be chal
lenged eitherbyshowingthatitcontradictsitselforthatitiscontradicted
byevidence.
A theory is self-contradicting ifit implies or predicts something that
wouldcontradictthetheory. Anexamplewasgivenearlierinthis chapter:
I showed that a cornerstone premise of popular TA, which asserts that
price perfectly reflects all information, contradicts another ofits central
premises, thatprice containspredictive information.
The Smart versus Dumb Paradox
One implication of market efficiency is that knowledgeable investors
should not be able to earn higher returns than investors who are less
knowledgeable. This follows from the premise that market efficiency
means thatwhateverinformationis known orknowable hasalready been
reflectedinsecurityprices. Inan efficientmarket, there is no competitive
advantage to beingsmartordisadvantage to beingdumb.
However, this implication conflicts with another of EMH's assump
tions-that the arbitrage activities ofrational investors are able to drive
pricestoward rationallevels. (See the headingtitled "TheAssumptions of
EMH" laterin this chapter.)Arbitrage canonlyactasanenforcerofratio
nal prices if arbitrageurs have more trading capital and, thus, greater
price-movingpowerthan irrational, dumb investors. Having more trading
capitalimpliesthatsmartarbitrageurs musthave earnedhigherreturns in
the past than less well informed (dumb) investors. Either market prices
are setby the smartest, richest participants or they are not. EMH implies
both. Paradoxical, isn'tit!
The Cost ofInformation Paradox
Anotherlogicalinconsistency ofEMH has to do with the costofinforma
tion. Itis reasonable to assume that it coststime, money, and intelligence
to gatherinformationandprocessitintouseful investingstrategies.Atthe
same time EMH contends that such information cannot earn incremental



==================================================
                     PAGE 359                     
==================================================

Theories ofNonrandom Price Motion 343
returns for investors who incur these costs. Remember, EMH contends
that information is instantly reflected in prices. This implies that no mat
terhowwideordeeptheresearcheffort,whateverisdiscovered willbeof
novalue.
However, if EMH is correct that there is no payofffor information
gathering and processing, then investors would lack the motivation to
incurthe costs ofdoing so. With no one to dig out the information and
actonit, itwould notgetreflected inprice. In otherwords, ifitiscostly
to make markets informationally efficient, investors would be moti
vated to do it only ifthey were compensated with excess risk-adjusted
returns. Thus the contradiction-EMH requires that information seek
ers be compensated for their efforts and simultaneously denies that
they will be.
This paradox is argued persuasively by Grossman and Stiglitz in
their article "Onthe Impossibility ofInformationally EfficientMarkets."27
They contend that inefficiencies are necessary to motivate rational in
vestors to engage in information gathering and processing. The returns
they earn from the market's pricing errors are compensation for their ef
forts, which, in turn, have the effectofmovingprices toward rational lev
els. The gains of these rational investors are financed by the losses of
noise traders and liquidity traders. Noise traders are investors who buy
and sell based on signals that they think are infornlative butthatare not.
Rational investorsalsoearnprofitsfrom the losses ofinvestorswho trade
toincreasetheircashreserves(liquiditytraders). Onewayfortherational
investorto gain would be ifthere were a delay between the timeinforma
tionisdiscoveredandthetimeitisreflectedinprices, butEMHdeniesthe
possibility ofgradual price adjustments. This is yetanotherlogicalincon
sistencyofEMH.
Asimilarlogicappliesto the costsofactingoninformation(e.g., com
missions, slippage, bid-asked spreads). Unless investors were compen
sated for incurring trading costs there would be no point to their trading
activities, which are required by EMH to move the price to its rational
level. Thebottomlineisthatanyfactorthatlimitstrading-coststotrade,
costs to generate information, the rule that impairs short-selling, and so
forth-limits the ability ofthe market to attain efficiency. It simply does
not hold logicallythat there would be no compensation to those engaged
in trading.
The Assumptions ofEMU
To appreciate the arguments made by EMH's critics, it is necessary to
understand that EMH rests on three progressively weaker assumptions:
(1) investors are rational, (2) investors' pricing errors are random, and



==================================================
                     PAGE 360                     
==================================================

344 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
(3) there are always rational arbitrage investors to catch any pricing er
rors. Let'sexamine each oftheseina bitmore detail.
Firstis EMH's contentionthatinvestorsare, byand large, rational. To
recap, rational investors, in aggregate, are purported to value securities
correctly. Thismeanspriceswillreflect, asaccuratelyaspossible, thepre
sentdiscounted value ofa security'sfuture cashflows and risk character
istics. As rational investors learn new inforn1ation, they respond quickly,
biddingup priceswhen news is good and lettingthe price fall whennews
is bad. As a consequence, securityprices adjust, almost instantly, to new
information.
28
Even if this assumption were incorrect, EMH advocates still insist
that prices adjust quickly to rational levels anyway because oftheir sec
ond assumption: Thevaluationmistakesofindividualinvestorsare uncor
related. This means that ifone investorerroneouslyvalues a securitytoo
high, that error will be offset by another investor's error ofvaluing it too
low. Takeninaggregate, thesevaluationerrorsareself-cancelingand have
anaveragevalueofzero. Thatistosaypricingerrorsareunbiased. Thisis
presumedto betrue becausethe error-proneinvestors are likelytobe us
ing different, though useless, investment strategies. Therefore, their ac
tionsshouldbeuncorrelated. Toputitbluntly, as onefoolish investorgets
anuninformativesignaltobuy, anotherinvestmentfool isequallylikelyto
get an uninformative signal to sell. In effect they wind up trading with
eachother, leavingpricesverycloseto rationallevels.
Moreover, evenifthissecondassumptionfails to keeppricesatratio
nal levels, EMH invokes yet a third assumption. That is, ifthe valuation
mistakes ofirrational investors turn out to be biased (i.e., have an aver
age value that is greater than or less than zero), arbitrage investors will
come to the rescue. They will notice that prices have systematically di
verged from rational values. The arbitrageurs will buy when prices are
too loworsellwhenthey are too high, thusforcing pricesbackto where
theyshould be.
IntheEMHworld, arbitrageisas closeasitgetstoafreelunch. An ar
bitrage transactionincursno risk, requires no capital, and earnsaguaran
teed return. As envisioned by efficient market advocates, an arbitrage
tradegoessomethinglikethis: Considertwofinancial assets,stocksXand
Y, which sell at equal prices and which are equally risky. However, they
have differentexpectedfuture returns. Obviously, one ofthe two assetsis
improperlypriced.IfassetXhasahigherfuture return, thentotakeadvan
tageofthemispricing, arbitrageurswouldbuyassetXwhileshort-sellingY.
Withtheactivitiesoflike-mindedarbitrageurs, thepriceofeachstockwill
converge to its proper fundamental value.29 Market efficiency attained in
thismannerassumestherearealwaysarbitrageursready, willing, andable
to jump on these opportunities. The better ones become very wealthy,



==================================================
                     PAGE 361                     
==================================================

Theories ofNonrandom Price Motion 345
thereby enhancing their ability to drive prices to proper levels, while the
irrational investors eventually go broke losing their ability to push prices
awayfrom equilibriumlevels.The uninformed, irrationalinvestorsaredri
ven out ofthe market the way a weak species is driven from an ecosys
tem, while the rational, well-informedarbitrageursthrive.
Flaws in EM" Assumptions
This section considers each of the assumptions of EMH and how they
miss the mark.
Investors Arc Rational. Investors do notappear to be as rational as
EMHassumes. Manyinvestorsreactto irrelevantinformation, whatnoted
economist Fischer Black calls "noise signals."3o Though they think they
areactingintelligently, these investorscanexpectto earnthesamereturn
as an investorwho buys and sells based on a coin flip. Investors who fol
low the advice offinancial gurus who base their advice on noise signals
are noise traders byproxy.
In fact, investors are guilty ofnumerous departures from rationality.
For exanlple, they fail to diversify, they trade too actively, they increase
theirtaxliabilitiesbysellingappreciatedstocksbutholdontolosingposi
tions, and theytrade mutualfunds withhighfees. Moreover, theirfailures
are recurring. "Investors' deviations from the maxims ofeconomic ratio
nality turn out to be highlypervasive and systematic."31 These departures
from rationality can be classified into three areas: inaccurate risk assess
ments, poor probabilityjudgments, and irrational decision framing. I dis
cusseachofthesein turn.
Investors typically do not assess risks in confornlity with the norma
tive idealputforward byNeumannand Morgenstern, known as "expected
utility theory." In a perfectly rational world, an individual faced with
choosingamongseveral optionsselectsthe onewiththehighestexpected
value. Theexpectedvalueofa particularchoice isequal to a sum ofprod
ucts, whereeach productis theprobabilityofeachpossible outcomemul
tiplied by the value or utility ofthat outcome to the decision maker. The
problem is people do not actually behave this way. Research has shown
thatinvestorsmake numeroussystematicerrorswhenassessingrisks and
makingchoices.
These mistakes have been incorporated into a framework called
ProspectTheory, whichwasproposedbyKahnemanandTversky(1979).32
Itexplainshowpeopleactuallymakedecisionsunderconditionsofuncer
tainty. For example, itexplains that investors hold onto losingstocks but
sellprofitablepositionstoavoid thepsychicpainofrealizinga loss.Italso
explainswhyinvestorsovervaluelow-probabilitylong-shotspeculations.



==================================================
                     PAGE 362                     
==================================================

346 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
The second wayinvestors departfrom the rational ideal ofEMH is in
their judgments of probability. EMH assumes that, as investors receive
newinformation, theyupdate theirprobabilityassessmentsinaccordance
with Bayes' theorem, a formula for combiningprobabilities in a theoreti
cally correct way. However, to their detriment, investors do not behave
this way. One commonerror, discussed in Chapter2, is the crime ofsmall
numbers-drawing grand conclusions from small samples of data. This
explains why investors will rashly conclude that a corporation with a
short string ofsuccessful quarterly earnings reports is likely to continue
to grow rapidly. Their overreaction to this small sample ofpositive earn
ingschangescausesthestockto become overpriced.
Lastly, investors' decisions can be strongly impacted by how choices
are described (framed). By incorrectly framing a decision situation, in
vestors tend to misperceive the true expected value ofa choice. For ex
ample, whenchoicesareframed interms ofpotentialgains,investorstend
to choose the optionmostlikelyto earnagain, evenifitisinsignificantin
size. However, when the same choices are framed in tenns of potential
losses, investors willassume theriskofaverysignificantlossjusttoavoid
the certainty ofa small loss. This error, called the "disposition effect," is
thought to explain why investors sell winners quickly, so as to be sure to
earnagain, evenifit'ssmall, buthold onto losersevenifitmeans thereis
a chancethe loss will turn intoa muchlargerone.
InvcstOl' Errors Arc Unconclatcd. The assumption that investors'
errors are uncorrelated is contradicted by psychological research which
shows people do notdeviatefrom rationalityrandomlyin situations char
acterized by uncertainty. In reality, mostpeople tend to makesimilarmis
takes in such situations and hence their deviations from rationality are
correlated. Manyinvestorswillbetempted tobuythesamestockbecause
the stock appeals to a common heuristic rule ofjudgment that makes it
look like a candidate for further appreciation. For example, a company
that has three positive earning quarters can be perceived by many in
vestorstobeagrowthstockthatwarrantsahighprice-to-earningsratioor
PIE. This problem is made worse by social interactions, or herding, be
tween investors. "They follow each other's mistakes listening to rumors
and imitatingthe actionsoftheirfellow traders.
"33
Money managers, who should know better, make these mistakes as
well. They are guilty of creating portfolios that are excessively close to
the benchmarks against which they are measured, all in an effort to re
duce the chance they will underperform. Professional money managers
also engage in imitative behavior and move herd-like into the same
stocks, for fear offalling behind the pack. They dress up their portfolios



==================================================
                     PAGE 363                     
==================================================

Theories ofNonrandom Price Motion 347
by adding stocks that have done well and eliminate those that have not,
so year-end portfolio reports show the fund invested in the highest per
forming stocks.
Arbitrage Forces Prices to Rational Levels. This assumption
fails becausethepowerofarbitrageactivities topushpricestoward ratio
nallevelsislimited. First, no oneringsa bellwhensecuritiesbecomemis
priced. The rational price is the discounted value of a stream of future
cashflows, whichis, bydefinition, uncertain. Investorstry to estimate fu
tureearnings, buttheirforecasts arepronetosignificanterror.
Second, arbitrageurs do not have unlimited tolerance for adverse
price movement-for example, when an underpriced stock that has
been purchased continues to go lower, or an overpriced security that
has been sold short continues to soar. The actions ofnoise traders can
easily push prices further away from rational levels before they return
to them. Therefore, even ifan arbitrageur spots a truly mispriced secu
rity, the pricing error may grow larger before it is finally eradicated.
Should this adverse price movement become too large, the arbitrageur
may have to close the positionwitha loss. Ifthis happensoftenenough,
the noise traders candrive rational investors outofbusiness. This is re
34
ferred to as noise trader risk and it limits the motivation and commit
ments of arbitrage traders. Behavioral finance expert Andre Shleifer
says even an arbitrage trade that looks nearly perfect from the outside
isin realityquite riskysothe numberofinvestorswho goafteritwill be
limited. The bottom line is arbitrage activities cannot always enforce
rationalpricing.
Noisetraderriskwasafactorintheblow-up ofthe LongTerm Capital
Management (LTCM) hedge fund in the fall of 1998. The fund's crash
nearlytook the entirefinancial marketwithit. Ultimately, the mispricings
thatLTCM had identifiedwere corrected, but becausetheyhad overlever
aged their positions, the fund lacked the staying power to hold its posi
tionsthrough ashort-termperiodofevengreatermispricing.
Improper use of leverage is yet another factor that can impair the
role arbitrage plays in driving markets to efficiency. Thus, even ifarbi
trageurs can accurately identify over- and under-priced securities, if
they use too much leverage they can get wiped out. There is an optimal
leverage for favorable bets (i.e., speculations with a positive ex
pectancy).35 If this level is exceeded, the probability of ruin rises de
spite the positive expectation.
Ishowthis to mystudentsduring a classIcall Casino Night. We playa
game in which everyone starts offwith an imaginary $100. I have a friend
comeintoactascoinflipper. Beforeeachflip, thestudentsdeterminewhat



==================================================
                     PAGE 364                     
==================================================

348 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
fraction oftheir capital to bet on the next coin toss. Ifa head results, the
payoffistwicetheamountofthebet.Ifatailresults, onlytheamountofthe
betislost. Weplaythegamefor 75flips. Thisgame hasaveryfavorable ex
pectation.36 However, even in a game with such a favorable expectancy,
many students wind up losing money because they bet too aggressively
(Le., thefractionofcapitalwageredonabetistoolarge).Aformulaworked
outby Kelly, anelectricalengineeratBellLabsin the 1950s,nowknown as
the Kelly Criterion, specifiesthe optimalfraction to betoneachcoinflip so
as to maximize the growth rate ofthe bettor's capital. The optimalfraction
dependsontheprobabilityofawinandtheratiooftheaveragewintoaver
age loss. In this particulargame, the optimalfraction to wageron eachbet
is 0.25. Ifthis level is exceeded, the bettor faces greater risk without the
benefitofa fastergrowth ofcapital. Ifonewere to employa betfraction of
0.58 it is likely all funds would be lost, despite the favorable expectation.
Thisiswhathappensto anarbitrageurwithgoodinformationwho usestoo
muchleverage.
Anotherconstraintonanarbitrage's ability to enforce efficientpric
ing is the lack of perfect substitute securities. An ideal (riskless) arbi
trage transaction involves the simultaneous purchase and sale ofa pair
ofsecurities with identical future cash flows and identical risk charac
teristics. An arbitrage transaction based on securities that do not con
form to this ideal necessarily involves risk. And it is risk that limits the
degree to which arbitrage activity can force prices to efficient levels.
When a broad class ofassets, such as all stocks, become overpriced, as
they did in the spring of2000, there is no substitute security to use as
the long-side hedge to a short sale ofthe entire stock market. Even in
the case where the securities in an arbitrage transaction are close sub
stitutes, there can be substantial risks. Each stock has its own specific
oridiosyncraticrisks, and this makesarbitrageurs wary. Forexample, if
GM were undervalued and itwere purchased againstan offsettingshort
sale in Ford, there would be the risk that some bullish event unique to
Ford or a bearish event unique to GM would create a loss for the arbi
trage trader.
There are still other constraints that prevent arbitrage from playing
the price-policing role envisioned by EMH. Arbitrageurs do not have un
lin1ited funds to correctpricing errors nor do they have total freedom to
pursue any and allopportunities. Mostarbitrage investors manage money
for otherinvestorsintheform ofhedgefunds. The managementcontracts
under which the fund managers operate typically constrain their actions
to some degree. Without a completely free hand and unlimited funds,
somemispricings will remain unarbitraged.
As a result ofthese limits, the EMH assumption thatall arbitrage op
portunitieswill beeradicated appears to beanoversimplification.



==================================================
                     PAGE 365                     
==================================================

Theories ofNonrandom Price Motion 349
Empirical Challenges to EM"
The EMH is not only riddled with logical inconsistencies, its predictions
are contradicted by a growing body of empirical evidence. This section
summarizesthesefindings.
Excessh'e I-.'ice Volatility. If security prices were as tightly con
nectedtofundamentalvaluesasEMHasserts, thenthemagnitudeofprice
changes should be similar to the magnitude ofchanges in the underlying
fundamentals. However, studies show prices are far more volatile than
fundamentals. Forexan1ple, when fundamental value isdefined as the net
presentvalue offuture dividends,37 changes in fundamental value are un
able to explain the high volatility ofprices. The technology stock bubble
that ruptured in the spring of 2000 and the bubble in Japanese stock
prices in the late 1980s are examples ofprice movements that cannot be
explained bychangesinfundamentalvalue.
EMH also predicts that large price changes should occur only when
significant new information enters the market. The evidence does not
agree. For example, the stock market crash ofOctober 1987 was not ac
companied by news that would havejustified a price drop that exceeded
20 percent. A 1991 study by Cutler cited by Shleifer38 exantined the 50
largest one-day price movements since the end ofWorld War II. Many of
these price events were not associated with significant news announce
ments. In a similar study, Roll39 showed that fluctuations in the price of
frozen orangejuice futures often occurred without any developments in
the weather, the major fundamental influence on that market. Roll also
showed that individual stock movements are often not associated with
corporatenewsevents.
Evidence of I-.'ice Predictability with Stale Information. The
evidence most danming to EMH are studies showing that price move
ments can be predicted to a meaningful degree with publicly known
(stale) information. In otherwords, strategies based on stale information
cangeneraterisk-adjustedreturnsthatbeatthemarket.Ifitweretrue that
prices quickly incorporate all known information, as EMH asserts, this
should notbepossible.
How Cross-Sectional Predictability Studies Are Performed. The
April 2001 issue of Journal of Finance notes that many predictability
studies show that publicly available information is not fully reflected in
stock prices and that numerous strategies based on this information are
profitable.40These studies measure the degree to which indicators based
on public inforn1ation are able to forecast the relative performance of



==================================================
                     PAGE 366                     
==================================================

350 METHODOLOGICAL.PSYCHOLOGICAL. PHILOSOPHICAL.STATISTICALFOUNDATIONS
stocks.4l Indicators tested included a stock's price-to-earnings ratio,
its price-to-book-value ratio, its recent relative price performance, and
so forth.
Predictability studies employ a cross-sectional design. That is to
say, they examine a large cross section of stocks at a given point in
time, for example, all stocks in the S&P 500 Index as of December 31,
1999. For that date, the stocks are ranked on the basis ofan indicator42
thatis being examinedfor its predictivepower, such asa stock'srate of
return over the pastsix months (price momentum). With this done, the
stocks are arranged into a number of portfolios on the basis of rank,
with each portfolio containing an equal number of stocks. Ten portfo
lios is a common number. Thus portfolio 1 would contain the top 10
percent of all stocks ranked by their prior six-month rate of return.
That is to say, the portfolio is composed of those stocks whose prior
six-month rate ofreturn was in greater than the ninetieth percentile. A
second portfolio is formed containing stocks ranked from the eightieth
percentile to the eighty-ninthpercentile, andso onuntilafinal portfolio
is formed from the 10 percent ofstocks with the worst prior six-month
performance.
To determine ifsix-month momentum has predictive power, the fu
ture performance of the top decile portfolio (1) is compared to the
future performance of the bottom decile portfolio (10). The forward
prediction horizon is typically one time period. Therefore, using
monthly data, it would be one month.43Typically these studies quantify
the indicator's predictivepoweras the return earned bya longportfolio
versus a short portfolio. In other words, a long position is assumed to
be taken in all portfolio 1stocks and a short position in all portfolio 10
stocks. Forexample, ifin a given time period, the long portfolio earned
7percent while the short portfolio lost 4 percent (e.g., the stocks sold
short went up 4 percent), the long-versus-short strategy would have
earned 3 percent. Although in this example the indicator used to rank
stocks was the six-month price momentum, any item of information
that was known at the time the portfolios are constructed could be
used. I illustrate this concept in Figure 7.6, where the indicator used is
eachstock's PIE ratio.
Ijustdescribedhowa cross-sectionalstudyis carried outfor asingle
month (December 1999). However, cross-sectionalstudiesare carriedout
over many months (cross-sectional time series). Thus, the predictive
powerofanindicatorismeasuredastheaverage return onportfolio 1mi
nus the average return on portfolio 10 over an extended period oftime.
Such studies often examine a number of candidate predictor variables.
ThisideaisillustratedinFigure 7.7.



==================================================
                     PAGE 367                     
==================================================

Theories ofNonrandom Price Motion 351
NStocks
Ranked by
PE 10Ranked
for Month 1 Portfolios
Formed Future
1. Lowest PE Performance
Differential
2. 2nd Lowest PE 1. Lowest PE
3. 3rd Lowest PE 2. 2nd Lowest PE
1"'-. ../ Return Port. #1
• • t
Universe
N ----. • ----. • ----. I 1vs. 10 Diff. I
Stocks
• ~
•
- Return Port. #10
10. Highest PE
N. Highest PE
I~IGURE 7.fj Cross-sectional studyto determine the predictive powerofP!E ratio.
-
-
PE PIB Cap. Mom. Etc. -
I-- - etc.
Stock 1 -'-- I ~-- Manth 5
Stock 2 -f--- --f '-- -- Month /4
Stock 3 ........f--
'-- Month 3
Month 2
Time
Stock n /
Month 1
Predictor
Variables
FIGURI~7.7 Cross-sectional time series study.
Predictability Studies Contradicting Semistrong EMH. The semi
strong form of EMH is the boldest testable version of EMH.44 It asserts
that no information in the public domain, fundamental or technical, can
be used to generate risk-adjusted returns in excess ofthe market index.
The bottom line ofnumerous well-conducted cross-sectional time series
studies is this: Price movements are predictable to some degree with



==================================================
                     PAGE 368                     
==================================================

352 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
stale public information, and excess risk-adjusted returns are possible.
Here, Isummarizesome ofthese keyfindings:
• Small capitalization effect: A stock's total market capitalization,
defined as the number of shares outstanding multiplied by its price
45
pershare, is predictive offuture returns. Stocks in a portfolio com
posed of the lowest decile portfolio of market capitalization earned
about9 percentperyear more than stocks in the highestdecile port
folio.46 This effect is most pronounced in the month ofJanuary. Re
cent studies show this indicator's predictive power has disappeared
sincethe midto late 1980s.
• Price-to-earnings ratio effect: Stocks with low PIE ratios outper
47
form stockswithhigh PIE ratios.
• Price-to-book-value effect: Stockswith lowpriceto bookvalue ra
tios outperform stocks with high price to book value ratios.'ls The
cheapeststocks, in termsofprice tobook, outperformed the mostex
pensive stocksbyalmost20percentperyear.
• Earnings surprise with technical confirmation: Stocks reporting
unexpected earnings orgivingearnings guidance that is confirmed by
strong price and volume action on the day after the news is an
nounced earn an annualized return spread (longs - shorts) over the
49
next month ofover30 percent. This strategyis a marriage offunda
mental and technicalinformation.
•·redictability Studies Contl'adicting the Weak Form of EMU.
The weak form of EMH is the least bold version of the theory. It claims
thatonlya subsetofpublic information, pastprices, and price rehlrns50is
unhelpfulinearningexcessreturns. Thefollowing studiesshowthatstale
prices and other dataused in technical indicators are indeed useful. This
is very good news for TA and very bad news for EMH. The narrowest,
most timid, and most difficult to falsify version ofEMH has been contra
dicted with evidence.
• Momentum persistence: Jegadeesh and Titman (1993)51 showed
that price momentum measured over the past six to twelve months
persists. In otherwords, the stocks that have been strongestoverthe
past 6months, 7months, 8months, and so on, up to 12 months, tend
to outperform over the following 6 to 12 months. Their study simu
latedastrategyofholdinglongpositionsinstockswith the highestre
turns overthe prior6months (top decile) and holdingshortpositions
instockswiththeworstperformance (lowestdecile). Theportfolioof
long and short positions was then held for 6 months. This strategy



==================================================
                     PAGE 369                     
==================================================

Theories ofNonrandom Price Motion 353
earned an annualized return of 10 percent. In light of this evidence,
even one ofthe high priests ofEMH, Eugene Fama, had to admit that
52
past stock returns can predictfuture stock returns. Score a big one
forTA!
• Momentum reversal: Although stocks with strong trends overthe
past 6 to 12 months tend to maintain those trends over the follow
ing 6 to 12 months, something different is observed when momen
tum is measured over longer time spans. Strong trends measured
over the prior three to five years display a tendency to reverse. De
Bondt and Thaler tested a strategy of buying stocks with the most
negative five-year trends (losers) and selling short stocks with the
most positive five-year trends (winners). The long-versus-short
portfolio averaged an 8 percent annualized return over the next
three years,53 and most importantly, this return difference was not
attributable to risk. The prior losers (future winners) were not
more risky than the priorwinners (future losers).54 This contradicts
acentralproposition ofEMH thathigherreturns canonlybe earned
by assuming higher risks. Additionally, it bolsters the notion that a
verysimplisticform ofTAis useful.
• Nonreversing momentum: Whenastock'smomentum is measured
by its proximity to its 52-week high, rather than its prior rate ofre
turn, profits are greater and momentum does not reverse.55 The au
thor of this study speculates that investors become mentally
anchored to prior 52-week price highs. Anchoring is lrnown to pre
vent people from making appropriate adjustments to new informa
tion. The authorofthis study, Michael Cooper, conjectured that this
prevents stocks near their 52-week highs from responding to new
fundamental developmentsas rapidlyas theyshould. Retarded news
response engenderssystemic price trends (momentum) thatcorrect
the mispricing.
• Momentum confirmed by trading volume: Furthersupportfor the
validity ofTAcomesfrom studiesshowingthat, when tradingvolume
is used conjointly with price momentum, even higher returns can be
earned. Thatistosay, asynergismcanbe attainedbycombiningprice
and volumeindicators. The return ofthe combinationis2to 7percent
higher than the return can be earned using price momentum alone.56
Stocks with high volume and positive price momentum do better go
ing forward than stocks with positive price momentum alone. More
over, stocks with high volume and negative price momentum do
worse goingforward thanstocksthatmerelyhave negative price mo
mentum. Said differently, the discrimination power of price momen
tum isgreaterforhigh-volumestocks.



==================================================
                     PAGE 370                     
==================================================

354 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
EM" Digs for aDefense
When an established theory is contradicted with empirical evidence, its
supporters do not simply roll over and say never mind. The cognitive
mechanisms ofbeliefpersistence (see Chapter2) don'tworkthatwayfor
trained scientists any more than they do for common folk. Cynical ob
servers ofscience say adherents to a theory that has been falsified never
reallychange theirminds. They have far too much invested in itssurvival
to do that. With time, theyjustfade away.
With EMH having served, andserved well, for 40+ years, as the foun
dation offinance, its backers werejust not about to go away with heads
hanginglow.Twoofitsstandardbearers(readpallbearers),EugeneFama
and Kenneth French, said gains earned by stale-information strategies
such as price-to-bookvalue and market capitalization were nothing more
thanfair compensationfor risk. RecallthatEMH does notdenythe possi
bility that stale public information can earn profits. It merely says that
whenthose gains are adjustedfor risk, theywill notbe betterthan invest
inginanindexfund.
So long as the term risk isleftundefined, EMH defenders arefree to
conjure up new forms of risk after the fact. As Elliott wavers have
proven, after-the-fact fiddling allows any prior observations to be ex
plained or explained away. And that, it seems to me, is what Fama and
French did.57 They invented a new, ad hoc risk model using three risk
factors to replace the old standby, the capital asset pricing model,58
whichuses onlyone risk factor, astock'svolatility relative to the market
index. Quite conveniently, the two new risk factors that Fama and
French decided to add were the price-to-book ratio and market capital
ization. By citing these as proxies for risk, Fama and French neatly ex
plained away their predictive power. "According to the new risk model,
stocks of smaller firms (low-market cap) or firms with low market-to
book ratios are fundamentally riskier companies and thus must offer
higheraverage returnsto compensatethe investors willingto ownthem.
Conversely, large capitalization stocks because they are safer, and high
price-to-book ratio stocks, which are in effect growth stocks with more
certain future prospects, earn lower average returns because they ex
posetheirowners to lowerrisk."59
This was nothing more than an ad hoc after-the-fact explanation con
jured up to save a dying theory. Had Fama and French predicted that
price-to-book and market capitalization were valid risk factors before
these variables were discovered to earn excess returns, that would have
been a much differentstory. Itwouldhaveshownthat EMHwasapower
ful theory whose deductive consequences were confirmed by subsequent
observations. However, Fama and French did not do this. They invented



==================================================
                     PAGE 371                     
==================================================

Theories ofNonrandom Price Motion 355
the new risk factors after price-to-book and market cap had been shown
to produceexcessreturns.
Aspointedoutearlier, Famaand Frenchjustifiedtheirnewriskmodel
by suggesting that low market-cap and low price relative to book are sig
nalsofcorporationsathigherriskoffailure. Ifthiswere true, itwouldim
ply (predict) that value strategies (buying low price-to-book stocks) and
small cap strategies would earn subpar returns in bad economic tin1es,
whendistressedcompaniesare mostlikelytosuffer.
Empiricalevidencecontradictsthisprediction. A1994studyfound no
evidencethatvaluestrategiesdo worsewhen theeconomysuffers.5OAlso,
the disappearance ofthe excess return to small-cap stocks in the last 15
years presents a problem. Ifcap size were indeed a legitimate risk factor,
the returns to strategies based on it should continue to earn a risk pre
mium. Finally, neither the Fama-French risk model, nor any other EMH
model, isable to explain the predictivepowerofpricemomentum indica
tors orthe conjointeffectofpricemomentumandtradingvolume.
BEIIAViORAL FINANCE: ATHEORY OF NONRANDOl\1
PRICE MOTION
ewtheoryis neededwhenexistingtheory confrontsdissonantevidence.
EMH has now been bombarded with a lotofevidence thatcontradicts its
predictions. Itis timefor a newtheory.
The relatively newfield ofbehavioral finance has come forward with
several variations of a new theory of financial market behavior that ex
plains phenomena that EMH cannot. These theories are scientifically
meaningfulin thatthey do notsimplyexplain (fit) whathas occurred, but
they make testable predictions that have been confirn1ed by subsequent
observationalstudies.
Behavioral finance incorporates elements of cognitive psychology,
economics, and sociologyto explainwhy investors departfrom full ratio
nalityand therefore why markets departfrom full efficiency. Byconsider
ing the impact of emotions, cognitive errors, irrational preferences, and
the dynamics ofgroup behavior, behavioralfinance offers succinctexpla
nationsofexcessmarketvolatilityaswellas theexcessreturnsearned by
staleinformationstrategies.
Behavioral finance does not assume that all investors are irrational.
Rather, itviews the market as a mixture ofdecision makers who vary in
theirdegree ofrationality. When irrational investors (noise traders) trade
with rational investors (arbitrageurs), markets can depart from efficient
pricing. Infact, market efficiencyis a ratherspecial condition thatis less



==================================================
                     PAGE 372                     
==================================================

356 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
probable than other more plausible market conditions, where prices are
likelyto divergefrom rationallevelsandthus likely to experiencesystem
atic predictable movements toward those levels.51 This explains whysys
tematicstrategiesbasedonstaleinformation canprofit.
Thisisallquite ironicwith respect to TA. Asdiscussed inChapter2,
cognitive errors can explain how people form erroneous beliefs in the
validity ofsubjective TA in the absences ofsound supportive evidence
or even in the face of contradictory evidence. At the same time, how
ever, cognitive errors may also explain the existence of market ineffi
cienciesthatspawnthesystematicpricemovements thatallow objective
TA methods to work. Thatwhich explains the foolishness ofsubjective
TApractitioners also explains the reasonableness ofsome objective TA
methods.
Foundations ofBehavioral Finance
Behavioralfinance rests ontwofoundational pillars: The limited abilityof
arbitrage to correct pricing errors, and the limits of human rationality.52
When both notions are combined, behavioral finance is able to predict
specific departures from market efficiency that produce systematic price
movements. For example, under certain circumstances, trends are pre
dicted to persist, whereas, under other circumstances, trends are pre
dictedto reverse. We will considereach ofthesepillars.
Limits ofi\1·bitl'a~e. To recap briefly, arbitrage is not the perfect en
forcer ofefficientpricingthat EMH assumes. The lack ofperfectsecurity
substitutesturns arbitrage from a risk-free, no-investment-required trans
actioninto onethatrequires capital and incurs risk. Even where there are
verygood substitute securities, there is a risk thatpriceswill diverge fur
ther from rational values before returning to them. Moreover, the in
vestors who supply trading capital to arbitrageurs don't have unlimited
patience, or unlimited capital. Consequently, they do not grant unlimited
latitude to arbitrageurs in terms ofthe types ofopportunities that can be
consideredfor exploitation.
These constraints explain why security prices do not always react
properly to new information. Sometimes, prices underreact, and some
times, they overreact. Add to this the impactofnoise traders, who acton
uninformative signals,63and itbecomes clearhowsecurityprices can sys
tematicallydepartfrom rationallevelsforextendedperiodsoftime.
Limits of Human Rationality. The constraints on arbitrage predict
that inefficiencies will occur, but they alone do not predict under what
conditions the inefficiencies will manifest. For example, arbitrage con-



==================================================
                     PAGE 373                     
==================================================

Theories ofNonrandom Price Motion 357
straints do nottell usunderwhich circumstancesmarketsare more likely
to underreact to new information than overreact. That is where the sec
ond pillarofbehavioralfinance, the limits ofhuman rationality, comes in.
As we learned in Chapter 2, cognitive psychology has revealed that,
under conditions of uncertainty, human judgment tends to err in pre
dictable (systematic) ways. By taking into account the systematic errors
ofhuman judgment, behavioral finance can predict the type ofdeparture
from marketefficienciesthatare mostlikelyto occurina given setofcir
cumstances.
Ofthe twopillars ofbehavioralfinance, moreis understood aboutthe
limits of arbitrage than about investor irrationality. This is because arbi
trageurs are expected to be rational, and economic theory has a firmer
grasp onthe behaviorofrationalactorsthanirrationalones. Forexample,
itisnotyetclearwhichspecific cognitivebiasesand systematicjudgment
errors are mostimportant in finance, but a picture is gradually emerging.
This section describes the current state of behavioral finance's under
standing of irrational investor behavior and the systematic price move
mentsthatariseasa resultofthem.
Incontrastto traditionalfinance theory, behavioralfinance assertsin
vestors make biasedjudgments and choices. In effect, marketprices sys
tematically deviate from rational values because investors systematically
deviate from full rationality. However, these mistakes are nota sign ofig
norance, butrathera consequence ofthe generally effective ways human
intelligencehas developed to copewithcomplexityanduncertainty.
Behavioralfinance andEMHaresimilarin thattheybothcontendthat
themarketeventuallydoesgetitright-thatis, pricesultimatelyconverge
toward rational valuations. They merely differ about the character of
thesedeparturesand theirduration. EMHsayspricesdepartfromrational
levels randomly and briefly. Behavioralfinance says some departures are
systematic and last long enough to be exploited by certain investment
strategies.
Psychological Factors
In Chapter 2 we saw the way cognitive errors in1pact subjective techni
ciansand resultin erroneous beliefs. In the contextofbehavioralfinance,
we will consider how cognitive errors impact investors and result in sys
tematic price movements. The cognitive errors discussed in the sections
thatfollow arenotcompletelydistinct, nordo theyoperateindependently.
However, theyarepresented separatelyforthepurpose ofclarity.
ConservaUsm lIias, ConlirmaUon Bias, and Belief InerUa.
The conservatism bias64 is the tendency to give too little weight to new



==================================================
                     PAGE 374                     
==================================================

358 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
information. Consequently, people fail to modify their prior beliefs as
much asthe new informationwould warrant. Priorbeliefstend tobe con
served. Because ofthis, investors tend to underreact to new information
thatisrelevanttosecurityvaluesandsosecuritypricesfail to respond ad
equately. Over time, however, prices do systematically catch up with
value. This gradual, purposeful adjustment appears on a chart as a price
trend. ThisisillustratedinFigure 7.5.
Thisconservatismbias isencouragedbythe confirmationbias, which
causespeopleto acceptevidencethatisconsistentwithapriorbeliefand
reject or give too little credence to evidence that contradicts it. Thus,
when investors hold an existing view about a security and new informa
tion arrives thatconfirmsthatbelief, they will tend to give itmore weight
than itdeservesand overreactto it. Conversely,whennewinformationar
rives thatcontradicts the prior belief, they tend to treatitskeptically and
underreact.
The confirmation bias also causes investors' beliefs, whatever they
may be, to become more extreme over time. The reason is as follows:
Over time, news items arrive as a random mixture ofconfirmatory and
contradictory information. However, the confirmation bias causes them
to be treated differently. Confirmatory information is given credence,
thus strengthening the prior belief, while contradictory information will
tend not to be believed and thus have a negligible impact on the in
vestors' priorbelief. Consequently, overtime itis likelythatpriorbeliefs
strengthen.
However, priorbeliefscan besubjectto a radical and irrationalweak
ening ifseveral bits ofcontradictory information arrive in a streak. Even
though streaksarecommonina randomsequence, investorsmaycommit
the crimeofsmallnumbersandattributetoo muchsignificancewhensev
eral pieces ofinformation that contradictthe prior beliefs arrive sequen
tially. In other words, the random streak may be erroneously interpreted
asanauthentic trend. These ideasare illustrated in Figure 7.8.
1'00 !\'Iuch Anchoring and 1'00 Little Adjustment. In Chapter 2
wesawthat, inuncertainsituations, people rely on heuristics tosimplify
and speed complex cognitive tasks like estimating probabilities. One
heuristic, not discussed thus far, is called anchoring. Itis relied upon to
estimate quantities. The rule is applied as follows: An initial estimate of
the quantity is made, based on preliminary information called the an
chor. Then upward or downward adjustments are made to the initial es
timate based on additional information. The anchoring rule seems to
make sense.
However, inpractice, people commonlymake two mistakes when ap
plyingthisheuristic. First,initialestimatescanbestronglyinfluenced bya



==================================================
                     PAGE 375                     
==================================================

Theories ofNonrandom Price Motion 359
i 1
Confirmatory News Contradictory News
1
,--- 1 1
1
Strong 1 ' I ' ...- - I 1
1---; I 1 •
1 ,I ... I I
,1---' , i
,I ' - - - I
, i
Belief i
Strength
i
Confirmation Bias
& Crime of
Conservatism Bias Small Numbers
Weak
Time
FIGUKI~7.8 Revising beliefs.
completely irrelevant anchor. This occurs even when the anchor's irrele
vance is completely obvious. Second, even in cases where the initial esti
mate is based on relevant anchoring information, there is a tendency for
subsequent adjustments up or down to be too small (the conservatism
bias). In otherwords, the additional information is given too little weight
relative to the initialestimate.
The tendency to seize on an irrelevantanchorwas demonstrated in a
studyinwhichpeoplewereaskedtoestimatethe lengthoftheMississippi
River (actual length = 2,348 miles). Subjectswerefirst asked ifthe length
wasgreaterthanorlessthansomearbitrarylength. Thisnumberassumed
the role ofanchorin thesubject'sminds. When thequestionposedwas: Is
the length greater or less than 800 miles? Most people correctly said the
Mississippiwaslongerthan800miles. Subjectswerethenaskedtogivean
estimate ofits actual length. Othersubjects were given the number 5,000
milesinthegreaterthanorlessthanpartofthequestion.Again, mostpeo
ple correctlysaid the river's length wasless than 5,000miles, buttheires
timates of the river's actual length were, on average, greater than the
estimates given by subjects who were asked to make a comparison with
800 miles. This revealed that the subjects seized on the number ofmiles
giveninthequestionasananchor, andthenmadeinadequateadjustments



==================================================
                     PAGE 376                     
==================================================

360 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
from there. Ifthe figure 800 or 5,000 were not truly influencing the esti
mate, the estimates oflength would have been about the same, irrespec
tive of whether the question involved a comparison with 800 miles or
65
5,000miles. Theanchorhada stronginfluence.
Another experiment showed that people will anchor on a number
even when it is obviously irrelevant.66 Subjects were asked about facts
that are typically not part of common knowledge-for example, "What
is the percentage of African nations in the United Nations?" The an
swers to each question involved a percentage (0 to 100). Before each
question, a wheel offortune with 100 numbered slots was spun in front
of the subjects. The study showed that the subjects' answers were
strongly affected by the wheel's obviously random result. For example,
if the wheel landed on the number 10, the median estimate was 25 per
cent, but when the wheel stopped on 65 the median estimate was 45
percent.
The anchoring heuristic is thought to be related to investor underre
action. Underreactions to bullish news cause asset prices to remain too
cheap, whereas underreactions to bearish news leave prices too dear.
Over time, the market's temporary state of inefficiency is resolved as
prices drift (trend) to the rational level. Thus, anchoring canhelp explain
the occurrence ofpricetrends.
Anchoring may explain the profitability of a momentum strategy al
67
luded to earlier. It is based on a simple technical indicator, a stock's
proximity to its 52-week high. Because this information is available in
newspapersand onvarious websites, investorsmayfixate (anchor) onit.
In otherwords, investors may fixate oranchoron the 52-week high price
ifastockiscurrentlytradingnearthatlevel. Ineffect, thestockgetsstuck
nearits52-weekhigh. Thusthepricefails torespondproperlytonewbull
ish information, and it becomes temporarily underpriced. Ultimately the
pricing error is corrected as the stock responds with a systematic move
menthigher.
Anchol'in~ to Stol'ies. Investors not only become anchored to num
bers; theycangetstuckonintuitivelycompelling68stories, too. The effect
isthesame;pricesdo notrespond efficientlyto newinformationand thus
departfrom rationalvaluations.
Stories are compelling because "much ofhuman thinking that results
inactionis notquantitative, butinsteadtakes theform ofstorytellingand
justification."69As pointed outin Chapter2, investors rely more oncausal
narrativesthan onweighingand combiningevidenceto estimateprobabil
ities and make decisions. Peopleseemto needsimple rationales tojustify
7o
theiractions. The mostcompellingrationalesare plausible, easilyunder
stood, easilyretold, causeandeffectchainsfilled withconcreteand color-



==================================================
                     PAGE 377                     
==================================================

Theories ofNonrandom Price Motion 36.
ful details. These are the tales that captivate investors and incite buying
andselling. Theseare thestoriesthatgetstuckininvestors'minds.
Optimism and Ovel'Conlidence. To recap, people are generally too
confident about the quality and precision of their knowledge. Thus, in
vestors tend to be overconfident about their private interpretations of
public information and overly optimistic about the profits they will
achieve. The combined effect ofoverconfidence and overoptimism leads
investors to overreactto their private information, and in tum pushes se
curity prices too far. Overextended price movements lead to price rever
salsandsystematicmovementsbacktowardrationallevels.
'I'he Crime ofSmall Numbers (Sample Size Neglect). The crime
of small numbers is the failure to consider the number of observations
comprisingasamplebeingusedtoform aconclusion. Thus, itisfallacious
to judge whether a datum was produced by a random or nonrandom
processorto estimatea populationparameteronthe basisofasmallsam
ple. Forexample, ifasequence of10coinflips produces 7heads, itwould
be invalid to conclude the coin has a 0.7 probability ofproducing heads.
The true head rate for a coin can be reliably inferred from only a large
numberofflips.
Investors who neglectsample size are easily deceived when trying to
decide ifa time series is random ornot. Forexample, bygeneralizingtoo
hastilyfrom too little evidence, investors may incorrectly conclude thata
few quartersofpositiveearningsgrowthindicatethatthe companyhases
tablished a valid growth trend. We have already seen that a small cluster
ofpositive quarters can easily occur in a random-earnings stream. Thus,
the crin1e ofsmall numbers helps explain why investors may overreactto
ashortsequenceofgood earningsreports.
The crime ofsmall numbers can causetwo differentjudgmenterrors:
the gambler'sfallacyand the clusteringillusion. Whichofthese two errors
results depends on the observer's prior belief about the process being
watched. Suppose an investor is watching a sequence of price changes
thatare trulyindependentofeachother(arandomwalk).
Ifthe investorholds a prior beliefthat the process is random, the er
ror will most likely be the gambler's fallacy. Recall that on the basis of
commonsenseweexpectrandomprocessesto displaymoreflip-flops and
fewerstreaks (trends) thanactuallydo occur. As a result, astreakofposi
tive price changes will lead naive observers to falsely conclude that a
trend reversal (a negative price change) is due. Thus, afterfive heads, the
observer erroneously concludes a tail is more likely than 50/50, its true
likelihood. In reality, a random-walk sequence has no memory. Thus, the
occurrence ofa positive streak does not alter the probability ofthe next



==================================================
                     PAGE 378                     
==================================================

362 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
outcomeinanyway. Thefalse expectationofareversaliscalledthe"gam
bler's fallacy" because the statistically naive tend to make this mistake
when watchinga roulette wheel; asequence ofblackoutcomesis thought
to increasethe chance ofa red. Wrong!
The otherfallacystemmingfrom sample-size neglectis the clustering
illusion. Itoccurswhenan observerhas no priorbeliefaboutwhetherthe
process generating the data is random or nonrandom The clustering illu
sion is the misperception oforder (nonrandornness) in data that is actu
allya randomwalk. Again, imagine someone observingthe outcomes ofa
process that is truly a random walk trying to determine ifthe process is
random or nonrandom (orderly, systematic).71 Recall that small samples
of random walks often appear more trended (clustered) than common
sense would lead us to expect (the hot hand in basketball). As a result of
the clusteringillusion, a sequence ofpositive price orearnings changes is
wronglyinterpretedasa legitimate trend, whenitis nothingmore thanan
ordinarystreakina randomwalk.
Social Factors: Imitative Behavior, Herding,
and Information Cascades72
Wehavejustseenhowinvestorbehaviorviewedatthe leveloftheinclivid
ual investorcanexplainseveraltypes ofsystematicpricemovement. This
section examines investor behavior at the group level to explain system
atic price movements. In contrastto conventionalTA theories ofinvestor
psychology, wewillseethatsomegroup behaviorsarequite rational.
Whenfaced withuncertainchoices,peopleoften tolooktothe behav
ior ofothers for cues and imitate their actions. This is what is meant by
herd behavior. Ishouldclarifythatherd behavioris notdefinedbysimilar
ity ofaction. Similaractions bymanyindividuals can also occurwhen in
dividuals have made similar choices butthose choices have been arrived
at independently. Herd behavior refers specifically to similarity ofaction
arisingfrom imitation. As Iwill pointout, the choice to imitate can be en
tirelyrational.
There is a major difference between similar behaviors arising from
herdingandsimilarbehaviors arisingfrom manyindividual decision mak
ers making the same choice independently. Herd behaviorstopsthe diffu
sion ofinfonnationthroughouta group, butindependentdecision making
does not. When infonnation diffusion is impeded, it becomes more likely
that investorswill make the same mistakes and prices will systematically
divergefrom rationallevels.
To see why herd behavior impedes the spread of infonnation, con
sider the opposite case, where inclividuals evaluate infonnation indepen
dently and make autonomous choices. Suppose a large number of



==================================================
                     PAGE 379                     
==================================================

Theories ofNonrandom Price Motion 363
investors are independently evaluating a stock. Some may mistakenly
value it too high, while others mistakenly value it too low. Because the
mistakes are made independently, they will tend to be offsetting, making
theaveragevaluationerrorclosetozero. Here, eachinvestorhasmadean
individual appraisal and in effect acted on a unique signal. This makes it
far morelikelythatallthe availableinformationaboutthestockwill have
been consideredand builtinto the price ofthe stock. Evenifno single in
vestor was in full possession ofall the facts, the group, taken as a whole,
islikelytobenearlyfully informed.
In contrast, when investors elect to imitate the behavior of others
ratherthan decidingindependently, all therelevantinformationaboutthe
stockis less likely to be fully diffused throughout the entire group, mak
ing it more likely that the stock's price will not reflect all relevant infor
mation. However, an individual investor electing to imitate may be
making a perfectly rational choice. Not everyone has the time or exper
tise to evaluate a stock, so it can make sense to copy an investorwith a
goodtrackrecord, suchas Warren Buffett. Once Buffettdoeshisanalysis
and makeshisbuyorselldecision known, investorsmaycease theirown
efforts to gather information and evaluate the stock. This makes itmore
likely that, ifsome factor has escaped Buffett's notice, it will remain un
noticed bythe communityofinvestors. So here is a case where individu
als take actions thatare fully rational on an individual basis butharmful
to themasagroup.
Why Do We Imitate? Atone timeitwas thoughtthatimitativebehav
iorwas a consequence ofsocialpressure to conform. Early experimental
evidence seemed to confirmthis.73 In one study, an authentic subjectwas
asked to estimate the lengthoflinesegmentsin the presence offake sub
jectswhoalsogaveestimates. Thefakes, whowereactuallypartoftheex
periment, intentionally gave wrong answers. Even though the correct
answer was obvious, the authentic subjects often went along with the
group ratherthan give the obviously correctanswer. However, a laterex
periment,74 in which the authentic subjectwas isolated from the group of
confederates, showed the subject still imitated the group's obviously
wrong answer. This finding implied that social pressure does not explain
imitativebehavior. Abetterexplanationemerged. Thesubjectsseemedto
berelyingona socialheuristic: Whenone'sownjudgmentiscontradicted
by the majority, follow the majority. In other words, people operate with
an implicitrule: Themajorityisunlikelytobewrong.
"This behavior is a matter of rational calculation: in everyday living
we have learned that when a large group of people is unanimous in its
judgment on a question ofsimple fact, the members ofthe group are al
most certainly right. Similarly, most of us operate with the principle
"75



==================================================
                     PAGE 380                     
==================================================

364 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
that, when an expert tells us something that contradicts our common
sense, we tend to heed the expert. So it is entirely rational under condi
tions ofuncertainty to look to the advice ofexperts or the actions ofthe
majority.
Information Cascades and lIerd lIehavior. The uncertainties of
investing make it likely that investors would rely on an imitation heuris
tic ratherthan independentdecision making. This hasimportantimplica
tions for the emergence of systematic price behavior because imitation
gives rise to "information cascades. "76An information cascade is a chain
ofimitative behaviorthat was initiated by the action ofone orjusta few
individuals. Insome instances thatinitiating action may have been a ran
dom choice.
Theemergenceofan informationcascadecanbeillustratedwithahy
pothetical example. Suppose two new restaurants open for business on
the same day, right next door to each other. The first hungry customerto
arrive must choose one, and has little information onwhich to base a de
cision. Thatcustomer's choice is, therefore, a random decision. However,
whenthesecondcustomerarrives, there isanadditionalpieceofinforma
tion: one ofthe restaurants hasa patron. Thismaytriggerthesecond cus
tomer to go into the same place. Because the first customer's selection
wasaguess, we knowthatthesecondcustomer's choicewas based onan
uninformative signal. When the third customer arrives and sees one
restaurant empty and the other with two customers, the chance is in
creasedthat the third customerwill also make an imitative choice. Inthe
end, a cascade ofdependentchoices, triggeredbythefirst customer'sran
dom choice, may ultimately cause one establishment to thrive and the
other to fail, and it is entirely possible that the failing establishment of
feredbetterfood.
Thus, an initialrandom eventsetsthe courseofhistorydown onepar
ticularpath (restaurant1succeeds)ratherthananotherpath(restaurant2
succeeds). As time goes on, the likelihood that one restaurant will thrive
increases while the likelihood that the second establishment will fail in
creases.Inthesameway,aninitialrandompricemovementcantriggersuc
cessive rounds ofimitative investor behavior, resulting in a long-duration
largeamplitudepriceswing.
Now consider what would have happened if many customers had
madeindependentrestaurantchoices, allowing both establishmentsto be
sampled by many customers. By combining their findings, they would
have discovered whichwas the betterofthe two restaurants. However, in
aninformationcascade,imitativebehaviorpreventstheaccumulationand
sharing of many independent appraisals. Thus, information cascades
blockthe diffusion ofinformation and rationalchoice.



==================================================
                     PAGE 381                     
==================================================

Theories ofNonrandom Price Motion 365
Theinformation cascademodel revealstheflaw inthe commonsense
notion that financial market prices are determined by a process like vot
ing, in which many individuals make their own assessment. In an infor
77
mationcascade, investorsmakethe rationalchoice ofcopyingtheactions
ofothers, ratherthanexpendingthe considerable effortrequired to arrive
at an independent choice. Voters do one thing, and investors sometimes
dosomethingentirelydifferent.
'I'he ltil1'usion of InfOl'maUon Among Inveslol's. We have seen
thatinformation cascades can explainwhythe rational behaviorofindivid
ual investors can impede the diffusion ofinformation. This isimportantfor
TA because the rate at which infoffilationspreads among investors can ex
plain the occurrence ofsystematicprice movements. EMH assmnes almost
instantaneous information diffusion and, consequently, almost instanta
neous price adjustments. Such price adjustments are too quick to be ex
ploitedbyTAmethods. However,informationthatdiffusesslowlyallowsfor
gradual,systematicpricemovementsthatcanbeexploitedbyTAmethods.
Despite the prevalence ofadvanced communications technology, the
preferred method of exchanging investment information is still a good
story told person to person. This predilection, a result ofseveral million
yearsofevolution,hasbeenconfirmedbythestudiesofbehavioral-econo
mist RobertShiller. He has shownthatthe word-of-mouth effectisstrong
even anl0ng people who read a lot. A story about a hot new issue has
greater impact in conversation than a statistic about the high failure rate
ofnewcompanies.
The waystories spread among investors has been studied with mathe
maticalmodelssin1ilartothoseusedbyepidemiologiststostudythespread
ofdisease within a population. Unfortunately, these models have not been
asaccurateintheinvestordomainastheyhavebeenintherealmofbiology.
Thisisexplainedbythefactthatthemutationrateofcirculatingideasisfar
higherthanthemutationrate oforganisms.However, themodelshavebeen
enlighteninginanotherway; theyexplainhowstoriesspreadsorapidly.
One reason a "new" story can spread so rapidly is that it is not really
new. Investors' mindsarealreadyinfestedwith manyfan1iliarstoryscripts,
and it does not matter that the panoply of stories residing in investors'
minds contradict each other. For example, investors can be comfortable
withthe notion that the stockmarketcannotbe predicted andatthe same
time hold the opposing notion that it can. No doubt, most investors have
beenexposedtoexpertswhohaveespousedbothpointsofview. Ourminds
areable to acconunodatea widevarietyofcontradictoryideas becausethe
scriptsliethere inertwith no demand thatwe chooseonesideorthe other.
Nevertheless, this state of affairs can change rapidly. Even a slight
random perturbation in the news or the market's behavior can cause an



==================================================
                     PAGE 382                     
==================================================

366 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
idea that had been hibernating in investors' minds to spring to life and
consumetheirattention.
Shifts in Investor Attention. Where investors are focusing theirat
tention atany given momentcan shiftdramatically due to minorchanges
inthenews. "Thehumanbrainisstructuredtohaveessentiallyasinglefo
cus of attention at a time and to move rapidly from one focus to an
other. The brain had to evolve a capacityto selectively attend to onlya
"78
tinytrickleofthetorrentofinformationthatpoursinfromtheworld. This
filtering process is both a mark of human intelligence and a source of
judgment error. Numerous instances of expert error are attributable to
missing important details. However, before outcomes are known, we
neverknowwhichdetails deserve ourattention.
Amongtheautomatic unconsciousrulesusedbythe braintofilterrel
evant from irrelevant information is the rule to look to other people for
cues. In otherwords, we presume that whatgrabs the attention ofothers
must be worthy ofour attention as well. According to economist Robert
Shiller, "the phenomenon ofsocial attention is one ofthe great creations
ofbehavioral evolution and is critical for the functioning ofhuman soci
ety. Althoughcommunalattentionhasgreatsocialvalue, becauseitpro
"79
motes collaborative action, it has a downside. Itcan lead an entire group
to holdan incorrectviewand takesimilarmistakenactions.
In the opposite situation, where individuals form their own views in
dependently, errors ofattention would be random and self-canceling. Al
though this mode of thought makes it harder to organize joint efforts
around commonviewsandgoals,itislesslikelytooverlookimportantde
tails. Dramatic market movements can easily grab the attention of the
community ofinvestors and encourage them to act in ways that amplify
themovement, evenifthere isnofundamental reasonforit.
RobertShillerhasstudiedthewaydramaticpricemovementscapture
investor attention and trigger herd behavior. Even institutional investors,
who would be expectedto selectstocks insystematic ways based onsta
tisticallyvalidcharacteristics, areprone to buyingastocksimplybecause
it has experienced a rapid price increase.80 This mechanism can explain
howamarketboomcanbetriggeredjustbyavividinitialpricemovement
that calls attention to the stock market. Moreover, Shiller's studies show
thatinvestors are oftennotevenawarethatitwasadramaticpricemove
mentthatmotivatedthemto act.
'fhe Role of Feedback in Systematic Price Movements. Social
interaction among investors creates feedback. Feedback refers to
the channeling ofa system's output back into the system as input. See



==================================================
                     PAGE 383                     
==================================================

Theories ofNonrandom Price Motion 367
Figure 7.9, where two systems are compared-one with no feedback
and one withfeedback.
There are two types offeedback, positive and negative. One way to
think ofthe difference is asfollows. In the case ofnegative feedback, the
system output is multiplied by a negative number and the result is feed
back in. In the case ofpositive feedback the multiplieris a positive num
ber. ThisisillustratedinFigure 7.10.
System
External
----+ with ----+ Output
Input
No Feedback
System
External
with ----+ Output
Input
Feedback
G
1
Feedback loop 4
1
FIGUK~ 7.9 Feedback: output becomes input.
~
External
Input ---.~ ~ ----+ Output
~~I~P6Y
[
Negative Feedback Loop
~
External
Input ---.~ ~ ----+ Output
[
~~~~
Positive Feedback Loop
FIGUKI~7.10 Positive and negative feedback loops.



==================================================
                     PAGE 384                     
==================================================

368 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Negative feedback has the effect ofdampening system behavior, dri
ving the output backtoward a level ofequilibrium. Positive feedback has
the opposite effect, amplifyingasystem'sbehavior, thus drivingitsoutput
furtherfrom equilibrium. Thisisillustrated inFigure 7.11.
An example ofa negative feedback system is a household heating &
air conditioning system. When the temperature falls below a desired or
equilibrium level, the thermostat turns on the furnace, returning the sys
tem's output, the household temperature, back to the desired level. Tem
peratureabovethe desired levelturnsontheNC, bringingthetemperature
back down. This is all accomplished by feeding the system's output back
into thesystemasan input. Thisallows thesystemtoself-regulate its way
toward equilibrium.
Positive feedback is illustrated by the growing screech in a public
address system when the speakers are placed too close to the micro
phone. The normal hum corning out of the speakers (output) gets fed
back into the audio system via the microphone, where it is amplified by
thesystem. With eachcycle, the sound comesoutlouder, developinginto
a loudscreech. Positivefeedback isalso knownas thesnowball effector
viciouscycle.
Financial markets, like otherself-organizing self-regulated systems,
rely on a healthy balance between negative and positive feedback.sl Ar
bitrage provides negative feedback. Prices that are too high or too low
Negative
Feedback ----+
System
-AAr
Positive
Feedback ----+
----+
System
FIGURE 7.11 Positive and negative feedback loops: dampening versus
amplifying.



==================================================
                     PAGE 385                     
==================================================

Theories ofNonrandom Price Motion 369
triggerarbitrage trading thatpushes prices backtoward rational levels.
Positivefeedback occurs wheninvestordecisions are dominated byim
itative behavior rather than independent choice. In this regime, in
vestors will hop aboard an initial price movement, buying after first
signs ofstrength orselling afterfirst signs ofweakness, thus amplifying
an initialsmallprice movementinto a large-scaletrend. ATAapproach,
known as trend following, depends on large-scale price moves for its
profitability and thus is most effective during times when positive feed
backdominates.
Positive feedback can cause a vicious cycle ofprice increases or de
creases, which take prices far beyond rational levels even if the initial
price move that triggered the whole affair was justified. In other words,
price trends can go far above or below rational levels simply due to the
amplifyingeffectofpositivefeedback.
Positivefeedback canalsoresultfromtheywaypeopleadapttheirex
pectations to recent changes. Although it generally makes sense to alter
expectations when conditions change, people do it imperfectly. At times,
they adjustinsufficiently (the conservatism bias), whereas at othertimes
theyare overlysensitive to change andaltertheirexpectations more than
isjustified. Investors are especiallylikelyto overreactto new information
thatisprominent. Eitherasinglelargeprice changeorasequenceofsimi
lar changes can induce investors to alter their expectations too much.
These price changes can feed on themselves, thus generating price mo
mentum thatmayleadto a bubbleorcrash.
Ajudgment error that sometimes contributes to positive feedback is
mentalaccounting. Thisistheirrationaltendencytothinkaboutmoneyas
if it belonged in separate accounts, which should be treated differently.
Gains from prior speculative ventures may be assigned to the hot-action
account, whereas money accumulating in the home-equity account is
treated with greater conservatism. Rationally, all ofan investor's money,
regardless ofhow it was made or in which asset it is invested, should be
treated in the same way. A dollar is a dollar. However, speculative gains
from a recent market rise are often, irrationally, earmarked for further
speculation, thus addingfuel to arisingmarket.
At some point, the increasing expectations that fuel a bubble run
out. Common sense would suggest, and some analysts insist, that a
price bubble necessarily ends in market crashes (Le., a negative bub
ble). At times they do, but otherbubble termination patterns are possi
ble. Computer simulations ofsystems that incorporate feedback loops
showthatbubblesnotonlydevelop in ajaggedfashion with manyinter
vening pauses, but they can also deflate in a similar manner. Ifthe late
1990swas a price bubble, it seems to be ending with many intervening
pauses.



==================================================
                     PAGE 386                     
==================================================

370 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
Self-Organizing Ponzi Schemes
Thefeedback theoryofpricebubblesisappealingbecauseofitsplausibil
ityandthe numerous historicalexamplesthatseemto confirmitsvalidity.
However, it is hard to prove that a simple feedback mechanism involving
heightened investor focus, imitative behavior, and exaggerated investor
confidenceistrulyoperationalinfinancial markets.
Nevertheless, Yale economist Robert Shiller believes actual cases of
pyramidfrauds lrnown as Ponzi schemes offerevidence thatconfirmsthe
feedback theory. The scheme, named after the infamous Charles Ponzi,
who invented the idea in the 1920s, involves promising investors a high
rateofreturnfromsomesortofbusinessventureorinvestment. However,
the investors' money is never put to work in the way promised. Instead,
the commitments ofnew investors are used to payoff earlier investors.
This serves two purposes: it makes the venture look legitimate and it en
couragesthefirstinvestorstospreadthestoryoftheirsuccess.As the tale
gains popularity, new investors are motivated to make investments. Their
funds are, in turn, used to payoffa priorround ofinvestors, who now go
out and tell their story. This cycle of using Peter's investment to payoff
Paul to prove the success of the enterprise continues and grows. The
number ofpeople influenced by the scam follows a curve similar to the
diffusionofan infection throughoutapopulation. Ultimately, a saturation
level is reached. The supply of noninfected speculators gets exhausted,
and so there are no new investmentswith which to payoffthe lastround
ofinvestors. Atthispoint, theschemecrashes.
Recently a housewife from a small town in Alaska collected an esti
mated $10 to $15 million over a six-year period by promising a 50 per
cent return. Her business was purportedly based on unused frequent
flyer miles accumulated by large companies. In fact, this was very simi
larto Ponzi's originalscheme, which was based on internationalpostage
coupons.
Ponzi scams evolve ina typical pattern. Initially, investors are skepti
cal and invest only small amounts, but once early investors reap profits
and tell their success story, later-stage investors gain confidence and the
flow ofcapitalinto the scheme gains momentum. Frequentlythere are ra
tional skeptics issuing warnings, but the greedy pay no heed. Well
founded cynical arguments cannot compete with vivid accounts of fat
profits. The mothsdive intotheflame.
ShillercontendsthatspeculativebubblesarenaturallyoccurringPonzi
schemesthatemergespontaneouslyinfinancial markets, withoutrequiring
themachinationsofafraudulentpromoter.82Suchself-organizingphenom
enaarecommonincomplexsystems.Thereisnoneedforfalse storiesbe
cause there is always some buzz about the stock marketanyway. Atrend



==================================================
                     PAGE 387                     
==================================================

Theories ofNonrandom Price Motion 371
of rising prices acts as a call to action much like the gains of early in
vestors ina Ponzi scheme. The message is then amplified by Wall Street's
salesmen, and they need not tell lies. They simplypitch the upside poten
tial while downplaying the risk side of the story. The parallels between
Ponzi schemes and speculative bubbles are so clear in Shiller's opinion
thatthe burdenofproofisonthose who denythesimilarity.
Competing Hypotheses of Behavioral Finance
A mathematical model quantifies a scientific hypothesis about some as
pectoftheworld,Thisallowsquantitativepredictionstobemadethatcan
be tested against future observations. Of course, those proposing the
modelhopeitspredictionswillcoincidewithsaidobservations. Neverthe
less, iftheyshould clash, a true scientiststands ready toformulate a new
model that both explains the discordant observations and makes addi
tionalpredictionsthatcan betested againstfurther observations. Thiscy
cleofrefinementneverceasesbecause knowledge is nevercomplete.
Pseudoscientificstoriesresemblelegitin1atescientifichypotheses, but
onlysuperficially. Ascientific hypothesis is compact, yetitaccountsfor a
wide range of observations. Pseudoscientific stories tend to be compli
cated. Scientifichypothesesmakeprecisepredictionsthatcanbechecked
againstfuture observation. Pseudoscientificaccounts make vague predic
tions that are consistent with prior observations but never make predic
tions specific enough to permit clear refutation. Thus, pseudoscientific
accounts survive in the minds ofthe gullible, no matter how far off the
mark theymaybe.
On this basis, behavioral finance qualifies as science, albeit a young
one. Although as yet, there is no agreement on a single theory, there has
beensubstantialprogress. Within the lastdecade behavioral-financeprac
titioners have proposed several hypotheses to explain investor behavior
and key features ofmarketdynamics. These are importantto TAbecause
they provide a rationale for the occurrence of systematic price move
ments, the"without-which-not"ofTA.
The behavioral-finance hypotheses have been formulated as mathe
matical models, thus allowing them to generate testable predictions of
howmarketswould behaveifthe hypothesiswere correct. Thisopensthe
hypotheses to refutation ifmarkets do not manifest the predicted behav
iors. For example, if a model predicts the occurrence of trend reversals
similar to those actually observed in market data, the hypothesis gains
some confirmation. However, ifthe model's predicted scenarios are con
tradicted by actual market behavior, then the hypothesis would be re
futed. This makes the hypotheses of behavioral finance scientiiically
meaningful.



==================================================
                     PAGE 388                     
==================================================

372 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Recall that the random-walk model ofEMH predicts that systematic
price motion should not be observed in market behavior. In contrast, the
behavioral-finance hypotheses described later predict the occurrence of
systematic price motion and offeran explanation about why they should
occur. Even though none of the hypotheses provides a comprehensive
theory ofmarketbehavior, they have a commona message ofimportance
to TA: There is good reason to believe financial markets are not entirely
randomwalks.
Biased Interpretation of Public Information: 'I'he 1131'beris,
Shleifer, and Vishny (IISV) Hypothesis. Empirical studies show
that investors display two distinct errors in response to news releases.
Sometimestheyunderreact, and consequentlyprices changelessthanthe
new fundamentals say they should. At other times, investors overreact,
causing prices to change more than is justified by the news. Ultimately,
however, thesepricingerrorsarerectifiedbysystematicpricemovements
towardpricelevelsthataccuratelyreflectthenewinformation.
The puzzle is: Why do investors sometimes overreact while at other
times they underreact? Ahypothesis proposed by Barberis, Shleifer, and
Vishny83 (BSV) offers an explanation. First, BSV asserts that two distinct
cognitive errors are involved: conservatism bias and sample-size neglect.
Second, they assert that the particular error that is operative at a given
pointin timedepends oncircumstances.
To brieflyrecap: The conservatism bias describes the tendency to al
tera priorbeliefless than new information would warrant. Existingopin
ions are sticky. Sample-size neglect, also known as the crime of small
numbers, is the tendency to draw too grand a conclusion from a small
sample ofobservations. Thus, the conservatismbiasisthe opposite ofthe
crime of small numbers. In the former, new evidence is given too little
weight; inthe latter, itisgiven toomuch.
The conservatism biasisatworkwheninvestorsunderreactto news,
leaving prices too low aftergood news ortoo high after bad news. How
ever, as investors gradually realize the new information's true signifi
cance, prices make a gradual systematic march to a level that properly
reflects the news. Thus, some price trends can be explained by investor
underreaction.
The crime of small numbers explains investor overreaction. For ex
ample, ifinvestors observe a small number ofpositive earnings changes,
theymayconclude, toohastily, thata validgrowthpatternhasbeenestab
lished. This stimulates overly aggressive buying thatpushespriceshigher
thanwarranted. Investorscanoverreactinthesamewaytoafewnegative
observationsandpushthepricetoo low.



==================================================
                     PAGE 389                     
==================================================

Theories ofNonrandom Price Motion 373
The mathematical model put forward by BSV makes the following
simplifyingassumptions: (1) the markethasasinglestock, (2)itsearnings
stream follows a random walk, but (3) investors do not realize this,84 so
(4) investors labor under the false impression that the stock's earnings
streamshiftsbackand forth betweentwo distinctregimes, agrowth trend
phase and a mean-reverting (oscillating) phase, butthey do notconsider
the possibility that the earning may follow a random walk, (6) therefore,
investors believe theirjob is to figure out which ofthese two regimes is
operativeatany givenpointintime.
Let's see how BSV's mythical investor might react to the releases of
new earnings information. At any given time, the investors hold a belief
that the earnings are in either one oftwo nonrandom regimes: a growth
trend or mean reverting process. The possibility ofa random walk is not
considered. Recall thata random walk displays less mean reversion than
a true mean-reverting process and also displays streaks that are shorter
in duration than those found in a truly trending process. As we pick up
the story, an investor holds some prior beliefabout the earnings regime
and hears newsaboutearnings. Supposethe newsannouncementcontra
dicts the investor's belief. For example, if the investor had believed the
stock's earnings had been in a growth trend, and a negative earnings re
port can1e out, the investorwould be surprised. Similarly, ifthe investor
had been under the beliefthatearningswere in a mean-reverting regime,
a streak of similar changes would also be surprising. According to the
BSV hypothesis, both kinds ofsurprises would trigger an underreaction
by the investor due to the conservatism bias. In other words, regardless
ofwhetherthe investor'spriorbeliefwasinamean-revertingregin1e orin
a growth regime, news contradicting the beliefwould be given too little
weight causing prices to underreact to the new information. However, if
news that is at odds with a prior beliefcontinues to occur, the investor
will become convinced that the regime has switched, either from mean
reverting to growth or from growth to mean reverting. In this case, the
BSV hypothesis predicts that investors will switch from the mistake of
being too stuck with their prior beliefs and underreacting (the conser
vatism bias) to the mistake of committing the crime of small numbers
and overreacting. Thus, the BSV model accounts for both kinds of in
vestorerroras well as the systematicprice movement thatcorrects both
forms oferrors.
I illustrate the type of behavior predicted by BSV in Figure 7.12.
Here Iposit the case where investors initially believe that earnings are
in a mean-reverting regime. Thus, they expectpositive earnings reports
to be followed by a negative report. Forthis reason, they underreact to
the first two positive earnings reports. However, afterthe third positive



==================================================
                     PAGE 390                     
==================================================

374 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
Price
Conservatism Bias
-------.
: Rational
I
Price ~ -
Belief: Belief:
Earnings Mean Revert Positive Earnings Trend
Time
+ News + News + News
Event Event Event
#1 #2 #3
FIGURE 7.12 BSV: underreaction followed byoverreaction.
report, not at all unusual in a random process (three heads in a row),
they abandon their earlier beliefand adopt the idea that earnings have
entered a growth regime and have established an uptrend. Conse
quently they now expect that future reports will also be positive. This
causes investors to overreact, pushing the stock above a rational level.
The systematic price movements resulting from both types of investor
errorare highlighted in grey.
Thus BSV's model also shows howa feedback loop emergesfrom the
crimeofsmallnumbers. Wheninvestorsseeearningsmoveinthesamedi
rection several times in succession, they switch from doubting the trend
85
to assuming it is real. However, the conservatism bias initially impedes
theadoptionofthenewbelief. Theinteractionbetweenthe crimeofsmall
numbersandthe conservatismbias determines the speedatwhich specu
lative feedback develops.86 Simulations of the BSV model show several
kinds ofactual market behaviorsuch as price trendsfollowing surprising
earnings, price momentum, long-term reversals, and the forecasting
power offundamental ratios such as price-to-book. Thus, the BSV model
explains how investors' biased interpretations ofpublic information can
predict the occurrence ofphenomenathat are observed in actual market
data. This suggests that the model does capture some ofthe dynamics of
realfinancial markets.



==================================================
                     PAGE 391                     
==================================================

Theories ofNonrandom Price Motion 375
Biased Interpretation of Private Infol'mation: 'rhe Daniel, Hir
shleifel" and Subrahmanyam (OIlS) Hypothesis. An alternative
hypothesis proposed by Daniel, Hirshleifer, and Subrahmanyam (1998,
2001)87 (DHS) suggestsa differentexplanationfor systematicprice move
mentssuch astrend reversalsand trendpersistence (momentum).Incon
trast to BSV, the DHS hypothesis is founded upon investors' biased
interpretations of private research. In addition, DHS emphasizes some
what different cognitive errors: the endowment bias, the confirmation
bias (seeChapter2), and theself-attributionbias (seeChapter2).
Private research refers to an investor's independently derived inter
pretations ofpublic information (i.e., proprietaryresearch). The hypothe
sis put forward by DHS predicts that investors will be overly confident
aboutthe qualityoftheirprivate researchand hence willhavea tendency
to overreact to it. This prediction is based on the endowment bias-the
predisposition to place too high a value on what we own or create. This
overreactionpushes prices beyond rationallevels, which ultimatelyleads
to a price reversal back toward rational prices. So in DHS's view of the
world, price reversalsare an effectofoverconfidence in privatelyderived
signals.
DHS also explains price momentum (nonrandom price trends) as the
resultoftwo othercognitivebiases: confirmationbiasand self-attribution
bias. They assert price momentum develops because news releases and
price movements impact investors in a biased manner. Because of the
confirmation bias, news or price movements that confirm an investor's
prior beliefs, (i.e., those founded on their private signals and research),
will be given excessive weight. News events and/or price movements
that are confirmatory have the effect of increasing the investors' confi
dence that their private research was indeed accurate. This induces
them to take additional action (buying or selling), which amplifies the
momentum of the current price trend. The self-attribution bias also
causesinvestorsto viewconfirmatorynewsorpricemovementas asign
that their private research is good. Recall that self-attribution causes
people to take credit for outcomes, even ifthey occur because ofgood
luck, butto deny responsibility for bad outcomes by attributing them to
bad luck. Thus, both the confirmation bias and self-attribution bias in
duce investors to overreact to confirmatory evidence, thereby amplify
ing the market's positivefeedback.
Conversely, if public information or price behavior contradicts the
investor's private research, it will be given too little weight (confirma
tion bias) and investors will attribute it to bad luck rather than to poor
private research (self-attributionbias). Therefore, regardless ofwhether
the news or the price movement confirms or contradicts the investor's



==================================================
                     PAGE 392                     
==================================================

376 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
private information, the DHSmodelpredictsthatinvestors' confidencein
theirprivate research willincrease. "Thisunequal treatmentofnew infor
mation means initial overconfidence is on average followed by even
greateroverconfidence, thus generatingpricemomentum."88However, af
ter a sequence ofcontradictory public news signals, the investors' confi
dence in their private information will be shattered (crime of small
numbers), andpricetrendswillreverse.
Both DHS and BSVpredictthat investors are sometimes excessively
optimistic and sometimes excessivelypessimistic. In addition, these be
liefs are predictedto be subjectto reversal when a streak ofdisconfirm
inginformationarrives. IfDHSand BSVarevalid, thentheprofitsearned
by strategies that exploit trend reversals should be concentrated at the
times ofinformationreleases. This predicted consequence, which opens
both hypotheses to refutation, has been confirmed.89 Indeed, a large
fraction ofthe excess returns earned by buying value stocks (low price
to book, low PIE, and so forth) and buying the weakest stocks of the
past three to five years do occur near the times ofearnings information
releases.
News Traders and Momentum 'fraders: 'fhe liS lIypothesis.
Asjustdescribed, certaininstances ofpricemomentum can be explained
bypricescatchingupwithfundamentals dueto aninitialunderreactionto
news. Other trends can be explained by positive feedback, in which a
priceincreasestimulatesadditionalbuyingandpriceweaknessstimulates
additional selling.90 Thus, some investors look for signals in recent price
changesratherthannews.
Fromtheperspective ofTA, trendsspawnedbypositivefeedback can
beviewedasa cascade ofprogressivelylesssensitivetrend-followingsig
nalsbeingtriggered. Aninitialpricemovement(newsinspiredorrandom)
triggersthemostsensitivetrendsignals(e.g.,averyshort-termmovingav
erage crossoversignal). Transactions triggered by that signal push prices
enough to trigger a somewhat less sensitive trend signal (e.g., medium
term moving average), which in turn triggers even less sensitive signals
and so on. Whateverthe mechanism, investoractions become correlated,
forming a herdeffect.
The hypothesis proposed by Hong and Stein91 (HS) explains three
market phenomena: momentum, underreaction, and overreaction. To do
so, they postulate two classes ofinvestors, news watchers (fundamental
ists)andmomentumtraders (technicians).TheHShypothesisassertsthat
an interaction between these two classes of investors creates positive
feedback and, therefore, price momentum. The HS assumption that there
is more than one class of investors is grounded in the notion that in-



==================================================
                     PAGE 393                     
==================================================

Theories ofNonrandom Price Motion 377
vestors, bydintoftheirintellectuallimits, mustconfinetheirattentiontoa
restricted portion of the available infom1ation. News watchers confine
theirattention tofundamental developments. Onthebasisofthisinforma
tion, they derive private estin1ates offuture returns. In contrast, momen
tum traders confine their attention to past price changes and derive their
privateforecast offuture trends.
Hongand Steinpositthatnews watchers paylittle orno attention to
price changes. As a result, their private assessments ofthe news fail to
spread quickly throughout the community ofnews watchers. To under
stand how an exclusive focus on the fundamental news impedes the dif
fusion ofinformation, considerwhatwould happen ifthenews watchers
also paid attention to price action. Watching price movements would al
lowthem to infer, tosome degree, the private information ofothernews
watchers. Forexample, risingprices mightimplythatothernews watch
ers wereinterpretingthe newsasfavorable, whereasfallingpricesmight
imply the opposite. As we have seen, any investor behavior that slows
the process by which prices incorporate new information can cause
price trends. That is, instead ofprices instantaneously adjusting to new
information, they move gradually to a higher or lower level justified by
new information. For momentum (trend-following) strategies to work,
prices must change in a gradual enough fashion to allow signals based
on an initial price change to predict additional price change. Thus, the
failure of news traders to extract information from price action slows
the rate at which prices adjust to new information, thus resulting in
price trends.
Now consider the effectonmarketdynamics causedbymomentum
traders (trend-followers). The HS hypothesis claims that momentum
traders only pay attention to signals from price behavior. Trend follow
ingassumes that newlyemergingprice trends are an indication that im
portant new fundamental information is starting to diffuse throughout
the market. Momentum traders make the bet that the diffusion is in
complete and thatthe price trend will continue after the initial momen
tum signal. Actions taken by subsequent waves oftrend followers keep
prices moving, thus generating positive price feedback. Ultimately, as
more trend followers join the party, prices may carry further than fun
damentalsjustify. The overshooting occurs because momentum traders
are unable tojudge the extentto which the news has been diffused and
understood by all investors. When the news is fully diffused and all in
vestors have acted on it, no further price movement is fundamentally
justified. Thus, HS assert that the overshooting ofrational price levels,
resulting from the positive feedback ofmomentum traders, sets up the
conditions for a trend reversal back toward rational levels based on
fundamentals.



==================================================
                     PAGE 394                     
==================================================

378 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
'fhe Bottom Line. Behavioral finance has offered a number of
testable hypotheses that attempt to explain systematic price motion. Itis
notyetclearwhich, ifany, ofthese are correct. No doubtnew hypotheses
will be offered. So long as they generate testable (falsifiable) predictions,
theyshouldbeentertained. WhatisofimportancetoTAisthatthesetheo
ries suggest that systematic price movements should occur in financial
marketprices.
NOMlANDOM PRICE MOTIO I THE CO TEXT
OF EFFICIENT MARKETS
The preceding section explained how systematic price movement can
arise in markets that are not fully efficient. However, even ifmarkets are
fully efficient, the caseforTAisnotlost. Acasecambemadefor theexis
tence ofsystematic price motion in markets that are fully efficient. This
sectionpresentsthis case.
How Systematic Price Motion and
Market Efficiency Can Coexist
Some economists contend that systematic price movements are possible
even ifmarkets are fully efficient.92 In other words, the existence ofnon
random price behavior and the possibility of price prediction does not
necessarilyimplythatfinancialmarketsare inefficient.93
Other economists state the case more strongly, asserting that some
degree ofpredictability is not only possible but necessary for markets to
function properly. In their bookA Non-Random Walk Down Wall Street,
Andrew Lo and Craig MacKinlay say, "predictability is the oil that lubri
cates the gears ofcapitalism."94 A similar position is taken by Grossman
95
and Stiglitz. Theycontend thatefficientmarkets mustofferprofitoppor
tunities to motivateinvestorsto engagein the costlyactivities ofinforma
tionprocessingand trading. Foritis these veryactivities thatdrive prices
towardrationalvaluations.
In fact, there is only one special conditionunder which market effi
ciency necessarily implies that price movement must be random and,
therefore, unpredictable. This occurs when all investors have the same
attitude toward risk, a condition that is simply not plausible. Is there
any type ofrisk about which all people have the same attitude? To see
how far-fetched this assumption is, imagine for a moment a world in
which all people have the same attitude toward risk. There would be ei
ther too many testpilots and high steel workers ornot enough ofthem.



==================================================
                     PAGE 395                     
==================================================

Theories ofNonrandom Price Motion 379
Either everyone would be into skydiving and alligator wrestling, or no
one would.
It is far more realistic to assume a world ofinvestors with highly di
verse attitudes toward risk. In such a world, some investors would be
more adverse to risk than others, and they would be willing to pay other
investors to accept the burden ofadditional risk. Also, there would likely
berisk-tolerantinvestorslookingtoprofitfrom the opportunitytoassume
additional risk. All that would be needed in such a world is a mechanism
bywhich risk canbetransferredfrom the risk adverse tothe risktolerant
andfor compensationto be paidtothose willingto acceptsaidrisk.
Let's step away from markets for a moment to consider one mecha
nism bywhich the risk adverse can compensate the risk inclined-the in
surance premium. Homeowners dread the possibility of a fire, so they
enter into a transaction with a fire insurance company. The homeowner
transfers the financial risk ofa fire to the insurance company and, in ex
change, the insurancecompanyearnsapremium. Thisisaprofitablebusi
ness if the insurance company can predict the likelihood of a house
burning down (over a large number of homes-Law of Large Numbers)
andchargeapremiumthatcompensatesitforthatriskplusa bitmorefor
profit. The bottom line: The insurance company provides a risk accep
tanceserviceand iscompensatedfor doingso.
Participants infinancial markets have exposuresto various risks and
vary in their tolerance for them. These ingredients motivate transactions
thattransferriskfrom onepartyto another. Inthissituation, the investors
willing to beargreater risks can reasonably demand compensation in the
form ofa higher return than that offered by riskless investments, just as
test pilots and high steel workers can demand a premium salary for the
additionalrisks theybear. Infinancial marketterms, compensationforac
ceptingincreasedriskis calledariskpremiumoreconomicrent.
Financialmarketsofferseveralkinds ofriskpremiums:
• The equity market risk premium: Investors provide working capi
talfornewbusinessformation andarecompensatedbyreceivingare
turn that is above the risk-free rate for incurring the risk ofbusiness
failure, economicdownturns, andsuch.
• Commodityand currencyhedge risktransfer premium: Specula
tors assume long and short positions in futures to give commercial
hedgers (users and producers ofthe commodity) the ability to shed
the risk of price change. Compensation is in the form of profitable
trends that can be captured with relatively simple trend-following
strategies.
• Liquiditypremium: Investors assume positionsinless liquid securi
ties and in securities being aggressively sold by investors with acute



==================================================
                     PAGE 396                     
==================================================

380 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
short-term needs for cash. Compensation is in the form ofhigher re
turns for holding illiquid securities orin the form ofshort-term gains
for buyingstocks thathave recently beenveryweak (i.e., engagingin
countertrendstrategies).
• Information or price discovery premium for promoting market
efficiency: This premium compensates investors for making buy-and
selldecisionsthatmovepricestowardrationallevels. Becausethesede
cisions are oftenbased on complexmodels developed bysophisticated
research, this premium might also be called a complexity premium.96
Forexample, astrategythatsellsovervaluedstocksand buysunderval
uedstockshelpsmovepricesbacktorationalvalues(pricediscovery).
Thegoodnewsaboutreturns thatcomeintheform ofa riskpremium
isthattheyare more likelyto endureintothe future. Ofcourse, itisevery
analyst's dreamto discovertrue marketinefficienciesbecause the returns
earned from them do not entail additional risk.97 However, inefficiencies
are ephemeral. Sooner or laterthey tend to be discovered and wiped out
by other diligent researchers. However, returns thatarejustified as a risk
premium are more likelyto endure asthey representpaymentforservice.
How do risk premiums explain the existence of systematic price
movements? Such price movements can provide a mechanism for risk
acceptinginvestorstobecompensated. Theinvestorwhoiswillingtopro
videliquidityto astockholderwitha strongdesire tosellwinds up buying
stocks that have a systematic tendency to rise over the near term. These
stockscan beidentifiedwithcountertrendstrategies.98
Therefore, in the context of efficient markets, the profits-earned by
TAstrategies may be understood as risk premiums; compensation for the
beneficialeffectthestrategyconfersonotherinvestorsorthe marketasa
whole. In otherwords, TAsignalsmaygenerate profits becausetheyiden
tifyopportunitiesto assume risks thatotherinvestors wish toshed.
Ishouldpointoutthatthe merefact thatan investoris willingto bear
a risk does notguarantee thata return will be earned. The risk takermust
becleveraboutit.Justasacarelesshighsteelworkerortestpilotmaynot
live long enough to collect a wage that includes the hazardous-duty pre
mium, a careless seeker ofrisk premiums may not stay in the game long
enoughto collect, either.
Hedge Risk Premium and the Gains to
Trend-Following Commodity Futures
This section describes how the profits to trend followers in the com
modities markets may be explained as a risk transfer premium. The
commodities futures markets perform an economic function that is fun-



==================================================
                     PAGE 397                     
==================================================

Theories ofNonrandom Price Motion 381
damentally different from the stock and bond markets. The stock and
bond marketsprovidecompanieswitha mechanismto obtainequityand
debtfinancing99 and provide investors with a way to investtheir capital.
Because stocks and corporate bond investments expose investors to
risksthatexceedthe risk-free rate (governmenttreasurybills), investors
are compensated with a risk premium-the equityrisk premium and the
corporate-bond riskpremium.
The economicfunction ofthe futures markets has nothing to do with
raising capital and everything to do with price risk. Price changes, espe
cially large ones, are a source ofrisk and uncertainty to businesses that
produce or use commodities. The futures markets provide a means by
which these businesses, called commercial hedgers, can transfer price
riskto investors (speculators).
At first blush, it may seem puzzling that commercial hedgers would
even need investors to assume their price risk. Because some hedgers
need tosell, like thefarmer who growswheat, and some need to buy, like
the bread company that uses wheat, why don't hedgers simply contract
witheachother?Theydo, butoftenthere isanimbalanceintheirhedging
needs. Sometimes wheat farmers have more wheat to sell than bakery
companies and other commercial users of wheat need to buy. At other
times, the situation is the opposite. Thus there is often a gap between the
farmers' supply and the bakers' demand. This creates the needfor a third
group ofmarket participants who are willing to buy or sell. That is, they
are willing to fill the supply-demand gap that develops between commer
cialproducersand commercialusers.
The existence ofasupply-demand gap isa predicable consequence of
the law ofsupplyand demand; asthe price ofwheatrises, supplygoes up
while demand goes down. As the price falls, supply goes down while de
mand goes up. Data from the Commodity Futures Trading Commission
(CITC), which tracks the buying and selling ofcommercial hedgers, con
firms this is exactly what occurs in the futures markets. During rising
price trends, commercial hedge selling exceeds hedge buying (i.e.,
hedgers are net sellers). During falling trends, commercial hedge buying
exceedscommercialhedgeselling (hedgersare netbuyers).
In the futures markets, the amountsold and the amountboughtmust
beequaljustasinrealestatetransactions thenumberofhomessoldmust
equal the number ofhomes bought. It cannot be otherwise. Given that a
rising trend motivates more commercial hedge sellingthan hedge buying,
there is a need for additional buyers during rising markets to accommo
datetheexcesshedgeselling. Duringfallingtrends, excessivecommercial
hedge buyingcreatesthe needfor additionalsellers.
Enterthepricetrendspeculatorwhoiswillingtobuyduringuptrends
to meet the unfilled needs ofcommercial hedge sellers and willing to sell



==================================================
                     PAGE 398                     
==================================================

382 METHODOLOGICAL,PSYCHOLOGICAL, PHILOSOPHICAL, STATISTICALFOUNDATIONS
shortduring fallingpricetrends, to meetthe unmetneeds ofthe commer
cialhedge buyers. Inotherwordsthefutures markets needspeculatorsto
betrendfollowers. Anditistheinvisiblehandofthe marketplacethatcre
atesthe trendsto compensatethemforfilling this need.
However, trend followers are exposed to significant risks. For exam
ple, most trend-following systems generate unprofitable signals 50 to 65
percentofthetime. Theseoccurwhenmarketsaretrendless, whichisthe
caseamajorityofthetime. Duringthesetimes, thetrendfollowers experi
ence the dreaded equity drawdown. During drawdowns it is not unusual
forsuccessfultrendfollowers toloseupto30percentoftheirtradingcap
ital. Thus they need a strong motive to tolerate drawdown risk. That mo
tive is the opportunity, though not the guarantee, to profit from price
trends when they do occur. In other words, large-scale trends provide a
profit opportunity to trend followers who are adequately capitalized and
who manageleverage correctly.
The Mt. Lucas Management Index
ofTrend Following Returns
The risk premium earned by commoditytrend followers has beenquanti
fied with the creationofa benchmarkindexcalled the Mt. LucasManage
ment Index (MLM). It is a historical record of the returns that can be
earned by an extremely simplistic trend-following formula. In other
words, it assumes no specialized knowledge of a complex forecasting
model. If it did it could not legitimately be called a benchmark index,
whose function is to estimate the returns that can be earned by investing
inan assetclasswithnospecialskill.
The risk-adjustedreturns earned bythe MLM indexsuggestthatcom
modity futures markets contain systematic price movements that can be
exploited with relativelysimple TAmethods. The MLM indexmonthly re
turns, whichhavebeencomputedbackto the 1960s,arederived byapply
ing a 12-month moving-average crossover strategy to 25 commodities
markets. Atthe end ofeachmonth, the price ofthe nearbyfutures con
100
tractfor eachofthemarketsiscomparedto its 12-monthmovingaverage.
Ifthe price is greater than the moving average, a long position is held in
that marketfor the next month. Ifthe price is below the average, a short
positionisheld.
The annualized returnlOl and risk, as measured by thestandard devia
tion in annual returns, of the MLM index is compared with returns and
risks ofseveralotherasset-classbenchmarksinFigure 7.13.
The risk-adjusted excess return or Sharpe ratio for the MLM index,
and the other asset classes are shown Figure 7.14. The trend follower
earns a risk premium that is somewhat better than the other asset-class



==================================================
                     PAGE 399                     
==================================================

Theories ofNonrandom Price Motion 383
=
18
16 ,:::::::::. ~
-,:::::::::.
14 ~
=-
-
12
Annualized 10 - = I-
"--
Percent -
8 f-- l-
,:::::::::.
6 f-- f-- f---- f-- l-
4 f-- f-- I-- ~ I-
2 ~ f-- f---- f-- I-
'::::::.L
o
US Int. T-Bills Corp. MLM
Stocks Stocks Bonds Index
FIGURE 7.13 Returns and standard deviations for five benchmarks.
0.7 f'=
0.6 ~
0.5 f--
0.4 >-.-.c= :;::= f-- IORisk-Adj. ReturnI
0.3 l- f.----- f--
0.2 >-.- f---- f--
0.1 >-.- f---- f.----- I--
o
US Int. T-Bills Corp. MLM
Stocks Stocks Bonds Index
FIGURE 7.14 Risk-adjusted excess returns.
benchmarks.'02 The MLM index provides some evidence that the futures
marketsoffercompensationin theform ofsystematicpricemovements.
Ifa risk-transferpremium is a valid explanation oftrend-follower re
turns, thentrendfollowing in stocksshouldnot beas rewardingas itisin
the commodityfutures markets. Atrend follower in stocks is not provid
ing a risk-transferservice. There are datasuggestingthat trend following
103
in stocks is indeed a less rewarding enterprise. Lars Kestner compared
the performance oftrend-following systems for a portfolio offutures and
a portfolio of stocks. The futures considered were 29 commodities in 8
differentsectors. Thestockswere representedby31 large-capstocksin
104
9 different industry sectors,105 and 3 stock indices. Risk-adjusted perfor
mance (Sharpe ratio) was computed for 5 different trend-following sys
tems,106 over the periodJanuary 1, 1990 through December 31, 2001. The
Sharperatio averaged overthefive trend-followingsystemsinfutures was



==================================================
                     PAGE 400                     
==================================================

384 METHODOLOGICAL, PSYCHOLOGICAL, PHILOSOPHICAL,STATISTICALFOUNDATIONS
.604 versus .046 in stocks. These results support the notion that futures
trend followers are earning a risk premium that is not available to trend
followers instocks. See Figure 7.15.
Liquidity Premium and the Gains
to Counter Trend Trading in Stocks
Thestockmarketoffersrisk-takingstocktradersa differentform ofcom
pensation. Theycanearna premiumforprovidingliquidityto highly moti
vated sellers. In other words, there are systematic price movements in
stocks thatcan be exploited with countertrend strategies that buystocks
thathave beenveryweakoverthe recentpast.
Owners ofstockwithan urgentneed to liquidate theirholdings need
buyers. This suggests that the market should offer compensation to
traders who are willing to meet unmet needs for liquidity. Evidence pre
sented by Michael Cooper shows that buyers of oversold stocks can
earn above-average short-term returns.107 His study shows that stocks
that have displayed negative price momentum on declining trading vol
ume earn excess returns. In other words, the pattern identifies stocks
with distressed sellers in search ofbuyers. The excess returns appearto
be a liquidity premium. Cooper's study showed thatstocks that have de
clined sharply over the prior two weeks on declining volume display a
systematic tendency to rise over the following week. The declining vol
ume aspect of this pattern is plausible because it can be interpreted to
mean that there are insufficient buyers to meet the acute needs of the
sellers. To guard against the possibility of data mining, Cooper used a
walk-forward out-of-sample simulation to form portfolios of long and
short positions over the period 1978 to 1993. The long/short portfolios,
created from 300 large capitalization stocks, earned an annualized re
turn of44.95 percent versus a benchmark buy-and-hold return of 17.91
0.7
0.6 r"---".
-
0.5
Sharpe -
Ratio 0.4
-
0.3
0.2 -
0.1 -
F=?l
o
Futures Stocks
FIGURE 7.15 Risk-adjusted returns to trend-following futures versus stocks.



==================================================
                     PAGE 401                     
==================================================

Theories ofNonrandom Price Motion 385
percent. Again, we see evidence ofsystematicpricemovements thatcan
be explained as a risk premium, in this case a liquidity premium, which
can be exploitedwith TA.
Inthe contextofefficientequityandfutures markets, signalsgivenby
TA methods can be viewed as identifying opportunities to fill a market
need. In effect, they are want ads, "risk adopter for hire by hedgers," or
"desperately seeking liquidity provider-will pay." TA traders profiting
from these signals are not getting a free lunch. They are simply reading
the market's Help Wantedadvertisements.
(;ON(;LUSION
In this chapter, Ihave made a case for the existence ofnonrandom price
movements in financial markets. Without them, there can be nojustifica
tion for technical analysis. With them, TA has an opportunity to capture
some portion of that nonrandom behavior. Ultimately, it is up to each
methodofTAtoprovethatitcan.



==================================================
                     PAGE 402                     
==================================================





==================================================
                     PAGE 403                     
==================================================

Case Study:
Signal Rules
for the
S&P 500 Index



==================================================
                     PAGE 404                     
==================================================





==================================================
                     PAGE 405                     
==================================================

Case Study
of Rule Data
Mining for
the S&P 500
This chapter describes a case study in rule data mining, the results
ofwhich are reported in Chapter9. The study evaluates the statis
tical significance of 6,402 individual TA rules back tested on the
S&P 500 Index over the period from November 1, 1980 through July 1,
2005.
01\11\ MINING BIAS AND RULE EVALUATION
The primary purpose of the case study is to illustrate the application of
statistical methods that take into account the effects ofdata-mining bias.
To recap, dataminingisa processin which the profitabilityofmanyrules
is compared so that one or more superior rules can be selected. As
pointed outin Chapter6, this selection process causes an upward bias in
the performance oftheselectedrule(s). Inotherwords, the observedper
formance of the best rule(s) in the back test overstates its (their) ex
pectedperformance inthefuture. This bias complicatesthe evaluation of
statistical significance and may lead a dataminer to select a rule with no
predictive power (i.e., its past performance was pure luck). This is the
fool's gold ofthe objectivetechnician.
This problem can be minimized by using specialized statistical
inference tests. The case study illustrates the application of two such
methods: anenhancedversionofWhite'srealitycheckandMasters'sMonte
Carlopermutation method. Both take advantage ofa recent improvement,
l
389



==================================================
                     PAGE 406                     
==================================================

390 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
which reduces the probabilitythata good rule willbeoverlooked (TypeII
error). Thatistosay, the improvementincreasesthepowerofthe tests.
Asecondarypurpose ofthe case studywas the possible discovery of
one or more rules that generate statistically significant profits. However,
when the case study's rules were proposed, itwas unknown ifany would
befound tobestatisticallysignificant.
AVOIDANCE OF DATA SNOOPING BIAS
In addition to data mining bias, rule studies can also suffer from an even
more serious problem, the data-snooping bias. Data snooping refers to
using the results ofprior rule studies reported by other researchers. Be
cause these studies typically do not disclose the amount of data mining
that led to the discovery ofwhateverit was that was discovered, there is
nowayto take its effectsintoaccountand hence no wayto properlyeval
uate thestatistical significance ofthe results. As pointed outinChapter6,
depending on which method is being used-White's Reality Check orthe
Monte Carlo permutation method-information about each rule tested
mustbeavailable to constructtheappropriatesamplingdistribution.
Forexample, suppose the case studyin this bookhad included a rule
developed by Dr. Martin Zweigknown as thedouble 9:1upside/downside
volumerule. Thisrulesignalslongpositionswhenthe dailyratioofupside
to downside volume on the NYSE exceeds a threshold value of9 on two
instanceswithina three-monthtimewindow. Note, thisrulehasthreefree
parameters: the threshold value on the ratio (9), the numberofinstances
on which the ratio exceeds the threshold (2), and the maximum time sep
arationbetweenthe thresholdviolations(3months). According to testsof
this rule conducted by students in my technical analysis class, the signal
does have predictive power over the following 3, 6, 9, and 12 months. In
other words, the rule is statistically significant, but only under the as
sumption that Zweig did not engage in data mining to discover the para
meter values he recommends: 9, 2, and 3. Zweig has not reported if he
tested otherversions ofthe rule using different parameter combinations
or, if he did, how many combinations were tried to find the specific set
that defines his rule. IfZweig's rule were to be included in the case study
anditwasselectedasthe bestrule, itwouldbeimpossibleto take intoac
countthe trueamountofdataminingthatled to itsdiscovery.
In aneffortto avoiddata-snoopingbias, the casestudydid notexplic
itly include any rules discussed by other researchers. Even though it is
possible, perhaps even likely, that some ofthe study's rules were similar
to those tested in prior rule studies, these sin1ilarities were by accident,



==================================================
                     PAGE 407                     
==================================================

Case StudyofRule Data Mining foy the S&P 500 391
notbydesign. This precaution mitigated the data-snoopingbias butcould
not eliminate it entirely because, in proposing the 6,402 rules tested, I
could nothelp butbeaffectedbyrulestudiesIhavepreviouslyread.
ANALYZED Df\'fA SERIES
Althoughall rules were tested fortheirprofitabilityonthe S&P500Index,
the vast majority of rules utilized data series other than the S&P 500 to
generatebuy-and-sellsignals. Theseotherdataseriesincluded: othermar
ket indices (e.g., transportation stocks), market breadth (e.g., upside and
downside volume), indicators that combine price and volume (e.g., on
balancevolume),pricesofdebtinstruments(e.g.,BAAbonds),andinterest
rate spreads (duration spread between lO-year treasury notes and 90-day
treasurybills). Thisapproachisinthespiritofintermarketanalysisas dis
cussed byMurphy.2Allseriesandrulesaredetailedbelow.
TECIINICAL ANALYSIS THEMES
The 6,402 rules tested were derived from three broad themes of TA: (1)
trends, (2) extremesandtransitions, and(3) divergences. Trend rulesgen
erate long and short market positions based on the current trend ofthe
data series analyzed by the rule. Extreme and transition rules generate
long and short positions when the analyzed series reaches an extreme
high orlow value oras itmakes a transition between extremevalues. Di
vergence rules generate signalswhenthe S&P500Index trends in one di
rection while a companion data series trends in the other. The specific
data transformations and signal logic for each rule are described later in
this chapter.
Therulesemployseveralcommonanalysismethodsincludingmoving
averages, channel breakouts, and stochastics, which Ireferto as channel
normalization.
PERFORMANCE S1l\TISTlC: AVERAGE RETURN
The performance statistic used to evaluate each rule was its average re
turn over the period (1980 to 2005) when back tested on detrended S&P
500 data. Aspointedoutin Chapter1, detrendinginvolvessubtractingthe
S&P500's average dailyprice change overthe back-testperiod from each



==================================================
                     PAGE 408                     
==================================================

392 CASESTUDY: SIGNAL RULES FORTHES&P 500 INDEX
day's actual price change. This results in a new dataseries for which the
averagedailychangeisequalto zero.
AB discussed in Chapter 1, detrending eliminates anybenefitordetri
mentthatwouldaccrueto a rule'sperformanceasa resultofeitheralong
or short position bias. A binary rule can be position biased ifone of its
conditions (long or short) is restrictive relative to the other condition
(long or short). For example, ifthe long-position condition is more diffi
cultto satisfythan theshort-positioncondition, the rulewill haveashort
positionbiasandtend to spend a majorityofitstime in shortpositions. If
such a rule were to be applied to marketdatawith a positivetrend (aver
agedailypricechange> 0), itsaverage returnwouldbepenalized. Thisre
duction in the rule's performance would have nothing to do with its
predictivepowerand thus clouditsevaluation.
NO COMPLEX RULES WERE EVALUATED
To keep the scope ofthe case studymanageable, itwas restricted to tests
ofindividualrules. Complexrules, whicharederived bycombiningtwoor
more individual rules with mathematical and/or logical operators, were
not considered. The combining method can be as simple an unweighted
average or as complex as an arbitrarily nonlinear function derived with
sophisticated data-modelingsoftware.
Limiting the case study to individual rules was detrimental for two
reasons. First, few practitioners rely on a single rule to make decisions.
Second, complexrulesexploitinformationalsynergiesbetweenindividual
rules. Itis notsurprising, therefore, thatcomplexruleshavedemonstrated
higherlevels ofperformance. Atleastone study3hasshownthatcomplex
rules can produce good performance, even when the simple rules com
bined to form the complex rule are individually unprofitable. Combining
rules intuitively is extremely difficult, but there are now effective auto
mated methods4forsynthesizingcomplexrules.
THE CASE STUDY DEFINED IN S'mTISTICAL TERMS
Thefollowing section defines the casestudyin terms ofthe key elements
of a statistical study: the population at issue, the parameter of interest,
sample data used, the statistic of interest, the null and alternative hy
pothesesconsidered, and the designatedsignificancelevel.



==================================================
                     PAGE 409                     
==================================================

Case StudyofRule Data Mining for the S&P 500 393
The Population
The population atissueis the setofdaily returns thatwould be earned by
arule ifitssignalsweretobeappliedtotheS&P500overallpossiblereal
izations of the immediate practical future.5 This is an abstract popula
tion, in the sense that its observations have not yet occurred and it is
infinite insize.
Population Parameter
Thepopulationparameteristhe rule'sexpectedaverageannualizedreturn
in the immediatepracticalfuture.
The Sample
The sample consists of the daily returns earned by a rule applied to de
trendedS&P500Indexpricesoverthe back-testperiodfrom November1,
1980untilJuly 1,2005.
Sample Statistic (Test Statistic)
The sample statistic is the average annualized return earned by a rule
when applied to the detrendedS&P500pricedatafrom November 1, 1980
untilJuly 1, 2005.
The ull Hypothesis (8 )
0
Thenull hypothesisstatesthatall6,402rulestestedare withoutpredictive
power. This implies that any observed profits in a back test were due to
chance (samplingvariability).
In reality, there are two version of the null hypothesis in the case
study. This stems from the fact that two different methods were used to
assess statistical significance: White's reality check and Masters's Monte
Carlo permutation method. Although both methods assert that all rules
considered were devoid ofpredictivepower, they implementthisassump
tion differently. The null hypothesis tested byWhite's reality checkis that
all rules tested have expected returns equal to zero (or less). The null
tested by Masters's Monte Carlo permutation is that all rules generated
their long and short positions in a random fashion. In other words, long
and short were randomly permuted with the market's one-day-forward
pricechange.



==================================================
                     PAGE 410                     
==================================================

394 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
The Alternative Hypothesis
This study's alternative hypothesis asserts thata rule's back-tested prof
itability stems from genuine predictive power. Again, the two methods
assertthis inslightly different ways. In the case ofWhite's realitycheck,
the alternative hypothesis says that there is at least one rule within the
tested universe that has an expected return greaterthan zero. It should
be noted that White's reality check does notassert thatthe rule with the
highest observed performance is necessarily that bestrule (i.e., the rule
with the highest expected return). However, under fairly reasonable
conditions, they are one and the same.6 The alternative hypothesis de
claredbyMasters's methodis thata rule's back-testedprofitabilityis the
result ofan informative pairing of its long and short positions with the
market's one-day-forwardprice change. Inotherwords, the rule haspre
dictive power.
The Statistical Significance Level
A5percentlevelofsignificancewaschosenasathresholdforrejectionof
the null hypothesis. This means there was a 0.05 probability ofrejecting
the H hypothesiswhenthe H was, infact, true.
o o
Practical Significance
The practicalsignificance ofa resultis differentfrom itsstatisticalsignifi
cance. Thelatteristheprobabilitythatarulethathasnopredictivepower
(i.e., H is true) would earn a return as high as or higher than the return
o
produced by the rule in a back test by luck. In contrast, practical signifi
cance relates to the economic value of the observed rule return. When
samplesizes are large, as theyare in the casestudy (i.e., over6,000days),
the H can be rejected even ifa rule's return is positive by a very minor
o
amount. In otherwords, a rule can be statisticallysignificanteventhough
its practical value is negligible. Practical significance depends upon a
trader's objectives. One trader may be satisfied with a rule for which ex
pected return is 5percentwhereas another may rejectany rule for which
expected return isless than 20 percent.
RULES: TRANSFORMI G DATA SERIES
INTO MARKET POSITIONS
A rule is an inputJoutput process. That is to say, it transforms input(s),
consisting of one or more time series, into an output, a new time series



==================================================
                     PAGE 411                     
==================================================

Case StudyofRule Data Mining for the S&P 500 395
consisting of +1's and -1's that indicate long and short positions in the
market beingtraded (i.e., S&P 500). Thistransformation is defined bythe
one ormore mathematical,710gical,8ortime series9operatorsthatare ap
plied to the inputtime series. In otherwords, a rule is defined by a setof
operations. See Figure8.l.
Some rules utilize inputs that are raw time series. That is to say, the
one ormore time series used as inputs to the rule are not transformed in
any way prior to the rule's position logic being applied, for example the
S&P500closingprice. ThisisillustratedinFigure8.2.
Other rules utilize input series that have been derived from one or
more raw marketseries by applying various transformations to the mar
ket data. These preprocessed inputs are referred to as constructed data
series or indicators. An example ofan indicator is the negative volume
index. Itisderivedfrom transformationsoftwo raw dataseries; S&P500
closing price and total NYSE daily volume. This is illustrated in Figure
8.3. The transformations used in the creation ofthe negative volume in
dex and other indicators used in the case study are described in the fol
lowingsection.
INPUT OUTPUT
Market Time Series
Time of
Series Market Position
~
Rule
Time
~
Mathematical, +1
Logical,
,
,
,.-- - -1'
'
'
&
Time-Series Time
Time Operators -1
•
•
•
Etc.
FIGURE 8.1 TA rule transforms input into output.



==================================================
                     PAGE 412                     
==================================================

396 CASESTUDY: SIGNAL RULES FORTHES&P SOO INDEX
Input Output
+1
I~. =>i_Ru'e_l~
,
,,1 ,--- - '' 'I
-1
Raw
S&P 500
Series
FIGURE 8.2 Raw markettime series as rule input.
~ ~
TimeRaSweries TimeRaSweries
S&P 500 NYSEPVolume
Negative Volume Index
Formula
Indicator
(Negative Volume Index)
Rule
V
FtPf--::-----::-
Rule Output +1 ::, ::'
-1 ---. --'
FIGURE 8.3 Transformed markettime series as rule input.
TIME-SERIES OPERATORS
This section describes the time-series operators used in the case study to
transform raw dataseriesintoindicators. Time-seriesoperatorsare mathe
matical functions that transform a time series into a new time series. The
case study utilized several corrunon time-series operators: channel break
out, movingaverage, and channelnormalization. The channelbreakoutop
eratoris used to identifytrends in a time series for rules thatare based on



==================================================
                     PAGE 413                     
==================================================

Case StudyofRule Data Mining for the S&P 500 397
trends. Themoving-averageoperator,whichcanalsoidentifytrends, isused
in the case study for smoothing. The channel-normalization operator, or
stochastics, is used to eliminatethe trend ofa timeseries (Le., detrending).
Channel Breakout Operator (CBO)
"Thepurposeofall trend identificationmethodsis toseepastthe underly
ing noise in a time series, those erratic moves that seem to be meaning
less, andfind the currentdirectionoftheseries."IOOnesuchmethodisthe
n-period channel-breakout operator.II Here, n refers to the number of
time periods (days, weeks, and such) into the past that are examined to
define the upper and lower channel boundaries that enclose the time se
ries. The lowerboundary is defined by the minimum value ofthe time se
ries over the pastn-periods, not including the current period. The upper
boundary is defined by the time series' maximum value over the past
n-periods, notincludingthecurrentperiod. Despiteitsextremesimplicity,
the channel-breakoutoperatorhasprovento beas effective as more com
plex trend-following methods.
12
As conventionally interpreted, the channel breakout operator signals
long positions if the analyzed series exceeds its maximum value estab
lished over the past n-periods. Conversely, short positions are signaled
when the time series falls below its n-period minimum. This is illustrated
in Figure 8.4. In practice, the channel is redrawn as each new datapoint
Time
Series
Time
~~~~~;
:: t---.n.·..·..nnnn.n:······· :.............. •
I~IGURE 8.4 Channel-breakout operator.



==================================================
                     PAGE 414                     
==================================================

398 CASESTUDY: SIGNALRULES FOR THES&PSOD INDEX
becomesavailable, butinFigure8.4onlyseveral channelsareshownto il
lustrate itsfunction. Although the channel has a constantlook-backspan
with respect to the time axis, its vertical width with respect to the axis
that represents the value of the time series adjusts dynamically to the
range ofthe series over the pastn-periods. This feature may explain the
effectiveness of the method. This is to say, the channel breakout's dy
namic range may reduce the likelihood of false signals caused by an in
creaseintheseries'volatilityratherthananactualchangeinitstrend.
The n-period breakout operator has a single free parameter, its look
back span, the number oftime periods into the past used to establish the
channel's boundaries. In general, the larger the look-back span, the wider
the channel, and hence, the lesssensitive the indicator. Thus, largervalues
ofn are used to identify larger trends. The case study tested 11 different
look-back spans for which lengths were separated by a factor ofapproxi
mately 1.5. The specific spans used were: 3, 5, 8, 12, 18, 27, 41, 61, 91, 137,
and205days.
Moving-Average Operator (MA)
Themovingaverageisoneofthe mostwidelyused times-seriesoperators
inTA. Itclarifiestrendsbyfiltering outhighfrequency (short-period) fluc
tuations while passing through low frequency (long-period) components.
Thus, the moving-average operatoris said to function as a low-passfilter.
This is illustrated in Figure 8.5, where the moving average has been cen
tered13 (lagged by 0.5 x period -1). Note that the output ofthe moving
average operator is essentially the trend ofthe original time series with
thehighfrequency oscillationsaboutthe trendremoved.
The smoothing effect is accomplished by averaging the values of the
time series within a moving data window called the look-back span. This
eliminates orreduces the magnitude offluctuations with durations thatare
equaltoorlessthanthedurationofthelook-backspan.Thus, a 10-daymov-
Input: Output:
Time Series Time Series
Input _"
Series ..:\ " "
, !"\'~:
Moving \
Average
,I I" II
."' ",
Operator
MA
,'\ : I "
" ': 'I
','
Time Time
FIGURE 8.5 Moving-average operator: low-pass filter.



==================================================
                     PAGE 415                     
==================================================

Case StudyofRule Data Mining for the S&P 500 399
ingaveragewillreducetheamplitudeoffluctuationsforwhichpeak-to-peak
ortrough-to-troughduration islessthan 10days, anditwillcompletelyelim
inateafluctuationwithadurationthatisexactlyequalto 10days.
The benefitofsmoothingcomesatacost-lag. Inotherwords, fluctu
ations in the raw price that manifestin the smoothed version experience
some delay. Thus, a trough in the raw price does notproduce a trough in
the moving average until some later date. The amount of lag depends
upon the length ofthe look-backspan. Alongerlook-backspanproduces
moresmoothingwhileproducingmorelag.
The lag induced by a simple moving average, which gives equal
weightto all elementsinthe window, is equalto one-halfofthe look-back
span minus one. Thus, the outputofan 11-daysimple movingaverage has
a lag of(11 - 1)/2 or 5 days. This means that trend reversals in the input
time series, which are ofsufficientduration to manifestin the moving av
erage, will notshowup until 5days later.
Thereare manytypes ofmovingaverages, from thesimplemovingav
erage that gives equal weight to each element in the data window, to so
phisticated smoothing methods that use complex data-weighting
functions. The benefitconferredbyacomplexweightingisimprovedfilter
perfornlance-Iesslagfora givendegreeofsmoothing. Thefield ofdigital
signal processing is concerned with, among other things, the design of
weighting functions that maximize filter performance. Digital filters that
are relevant toTAare discussed in two books by Ehlers.
14
The calculation for a simple movingaverage isgiven bythefollowing
formula.
15
Moving Average Operator
Pt+ Pt-l + Pt-2 ... + Pt-n+1
MAt=
n
n
L
Pt-i+l
i=l
n
Where:
Pt = Price at time t
MAt = Moving average at time t
n = Number of days used to compute moving average



==================================================
                     PAGE 416                     
==================================================

400 CASESTUDY: SIGNAL RULES FORTHES&P 500 INDEX
The lag ofa simple moving average can be reduced while maintain
ing the same degree of smoothing by using a linearly weighted moving
average. Aspointed outbyEhlers16the lagofa linearlyweighted moving
en- en
average with a look-back span ofn days is 1)/3 compared with
1)/2for a simple movingaverage. Thus, a linearlyweighted movingaver
age with a span of 10 days will have a lag of 3 days whereas a simple
moving average with the same span will have a lag of4.5 days. The lin
ear-weighting scheme applies a weight of n to the most recent data
point, where n is the number of days in the moving average look-back
span, and then the weight is decreased by one for each prior day. The
sum ofthe weights becomes the divisor. This is illustrated in the follow
ingequation:
Linear Weighted Moving Average
Weights
r-"---..,...-"----, ~
(n) x Pt + (n-l) x Pt-l + ... + (n-(t-n+1» x Pt-n+1
WMAt
n + n-l + ... + n-(t-n-l)
----------_/
.....
Sum ofWeights
Where:
Pt = Price at time t
WMAt =Weighted moving average at time t
n = Number of days used to compute moving average
The case study used a four-day linearly weighted moving average to
smooth indicators used in rules that signal when the indicator crosses
above or below a critical threshold. Smoothing mitigates the problem of
excessive signals that an unsmoothed version of the indicator wiggling
back and forth across the threshold would produce. The calculation ofa



==================================================
                     PAGE 417                     
==================================================

Case StudyofRule Data Mining for the S&P500 401
four-day linearly weighted moving average is illustrated in the following
equation:
Four Day Linear Weighted Moving Average
Weights
,-..A--, ,-..A--, ,-..A--, ,-..A--,
(4)xPtO + (3)xPt-] + (2)xPt-2 + (1 )XPt-3
WMAt
10
'-v--J
Sum ofWeights
Where:
Pt = Price at time t
WMAt =Weighted moving average at time t
n = Number of days used to compute moving average
Channel- ormaJization Operator
(Stochastics): C
The channel-normalization operator (CN) removes the trend in a time
series, thus clarifyingshort-termfluctuations around the trend. Channel
normalization detrends the time series by measuring its current posi
tion within a moving channel. The channel is defined by the maximum
and minimumvalues ofthe time series over a specified look-backspan.
In this way it is similar to the channel breakout operator discussed
earlier.
The channelnormalizedversion ofa timeseriesisscaledtothe range
o
to 100. When the raw time series is at its maximum value within the
look-back span, CN assumes the value of 100. When the series is at the
minimumvalue withinthe look-backspanthe CN is equaltozero. Avalue
of50 indicates the series is currently midway between its maximum and
minimumvalues.



==================================================
                     PAGE 418                     
==================================================

402 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
ThecalculationofCNisillustratedin thefollowing equation:
Channel Normalization Operator
l
I
l St - Smin-n
CNt 100
= Smax-n - Smin-n
Where:
CNt = n day channel normalized value on day t
St =Value of time series at time t
Smin-n = Minimum value oftime series, last n days
Smax-n = Maximum value oftime series, last n days
n = Channel look-back span in days
An example ofthe calculationisillustratedinFigure8.6.
In terms of its filtering properties, the CN operator functions as a
high-passfilter. Incontrasttothe movingaverage, alow-passfilter, CN ex
presses the short-period (high-frequency) oscillations in its output while
filtering outthe long-periodfluctuations ortrend. Thisisillustrated inFig
ure8.7. Note thatthe inputserieshasa distinctupwardtrend, butthe out
putserieshasno trend.
Input
Look-BackSpan
Series
Current
7S
Channel Minimum: 60
Time
Channel Normalized Value =(75 - 60)/(83-60) =0.65
FIGURE 8.6 Channel normalization.



==================================================
                     PAGE 419                     
==================================================

Case StudyofRule Data Mining foy the S&P 500 403
Input: Output:
Time Series Time Series
eN
Operator
Time Time
FIGURE 8.7 Channel normalization operator: ahigh-pass filter.
The CN operator has been known in TA at least as far back as 1965
when Heiby used it to formulate divergence rules. He measured diver
l7
gences by comparing the channel normalized value ofthe S&P 500 with
the channel normalizedvalue ofa market breadth indicator. According to
Heiby, adivergenceoccurredwhen oneserieswasinthe upperquartileof
its50-dayrange (CN > 74) while the otherserieswasin the lowerquartile
ofitsrange (CN <26). Sometimein theearly 1970s,avirtuallyidenticalop
eratorwas described by George Lane,18 who dubbed it stochastic. Unfor
tunately, the name is a misnomer. In reality, stochastic is a mathematical
term thatrefers to the evolutionofa randomvariable through time. There
isnothingrandomabouttheC operator, whichisa completelydetermin
istic data transformation. This text uses the term channel normalization
(CN) ratherthanLane'sterm, becauseitaccuratelydescribes whatthe op
eratordoes. MostTAwritingscontinue use the tem1stochastics. The case
study utilizes the CN operator for two types of rules: (1) extremes and
transitions and (2) divergences. Theyare discussed infollowing sections.
Indicator Scripting
As discussed earlier, the tin1e series that are used as inputs to the rules
may either be in raw form, justas they comes from the market, orin the
form ofindicators-times series that have been created by applying one
or more mathematical, logical, or time-series operators to one or more
rawtime series. The specificationofan indicatormay require the applica
tion ofseveral operators. For example, the raw time series mightfirst be
smoothed with a moving average and then channel normalization is ap
plied to thesmoothedversion. When the creationofan indicatorinvolves
multiple steps, it is convenient to express it in terms of an indicator
scripting language (ISL). An ISL expression succinctly depicts the set of
transformations used to createthe indicatorfrom rawtimeseries.



==================================================
                     PAGE 420                     
==================================================

404 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
Myfirst exposure to an ISL was Gener, which was developed in 1983
for Raden Research Group by friend and colleague Professor John Wol
berg. Today, ISLs are common. All rule back-testing platforms,19 such as
TradeStation, Wealth-Lab, Neuro-Shell, Financial Data Calculator, and so
forth have an ISL. Though each ISL has features unique to it, they share
similar syntaxes. The most basic ISL statement is a time-series operator
followed bya setofparentheses thatcontain the arguments ofthe opera
tor. The arguments are the items that must be defined for the operatorto
do its thing. Operators differ in terms of the number of arguments re
quired. For example, the moving-average operator requires two argu
ments: a dataseriestowhichthe operatorwillbe applied and the number
ofperiods in the look-back span. Thus a moving-average indicatorwould
bespecifiedbya statementsuchas this:
Moving Average Indicator = MA (input series, N)
Where:
MA is the moving average operator
N is the number of days in the moving average.
The general form of an indicator expression in scripting lan
guage is:
Indicator = Operator (Argument l, Argument z' ... , Argument j)
Where:
i is the number ofarguments required by the operator.
Figure 8.8 shows the ISL syntax for the operators defined thus far:
channelbreakout, movingaverage, and channelnormalization.
AkeybenefitofISListheabilityto definecomplexindicators bynest
ing operators. In nesting, the output of one operator becomes an argu
ment for a second operator and so on. For example, if one wanted to
define an indicator that is the 60-day channel normalization of another
time series that is a lO-day moving average ofthe DowJones Transporta
tion Index, the expression for the indicatorin ISL would look something
likethis:
eN
Indicator = [MA(DJTA, 10), 60]



==================================================
                     PAGE 421                     
==================================================

Case StudyofRule Data Mining faY the S&P 500 405
Channel-Breakout Operator (CBO)
Number ofArguments = 2:
Indicator = CBO (Time Series, n)
Moving-Average Operator (MA) or Linear Weighted MA (LMA)
Number ofArguments = 2:
Indicator = MA or LMA (Time Series, n)
Channel-Normalization Operator (CN)
Number ofArguments = 2:
Indicator = CN (Time Series, n)
FIGUKI~ 8.8 Indicator scripting language forthree TAoperators.
I PUT SERIES TO RULES: RAW TIME SERIES
AND INDICA1'ORS
This section describes the 24 raw time series used in the case study as
well as the 39 time series that were ultimately used as rule inputs. Ten of
the 39 inputs were in the fOffil ofraw unprocessed time series: S&P 500
closing price, Dow Jones Transportations Index closing price, NASDAQ
CompositeIndex closingprice, andsoforth. The additional rule 29inputs
were indicatorsderivedfrom one ormore rawtimeseries.
Raw Time Series
The 24 raw time series used in the case study came from two sources:
Ultra Financial Systems20 and MarketTiming Reports.21 They are shown
inTable 8.1.
Indicators
Twenty-nine of the 39 rule inputs were indicators derived by transform
ing one or more raw time series with various mathematical, logical, and



==================================================
                     PAGE 422                     
==================================================

406 CASESTUDY: SIGNALRULES FORTHES&P sao INDEX
'rABLE 8.1 Raw Time Series
Number Raw Data Series Abbreviation Source
S&P SOO DailyClose SPX Ultra 8
2 S&P SOO Open SPO Ultra 8
3 DjlA High DjH Ultra 8
4 DjlA low Djl Ultra 8
5 DjlAClose OJ( Ultra 8
6 S&P Industrials SPIA MarketTiming Reports
7 Dowjones Transports DjTA MarketTiming Reports
8 Dowjones Utilities DjUA MarketTiming Reports
9 NYSEFinancial Index NYFA MarketTiming Reports
10 Dowjones 20 Bonds DjB Ultra 8
11 NASDAQComposite Close OTC Ultra 8
12 Value Line Geometric Close VL Ultra 8
13 NYSEAdvancing Issues ADV Ultra 8
14 NYSEDeclining Issues DEC Ultra 8
15 NYSEUnchanged Issues UNC Ultra 8
16 NYSE New S2-Week Highs NH Ultra 8
17 NYSE New 52-Week Lows NL Ultra 8
18 NYSETotal Volume TVOL Ultra 8
19 NYSEUpside Volume UVOl Ultra 8
20 NYSEDownside Volume DVOl Ultra 8
21 3-Month T-Bill Yields T3M MarketTiming Reports
22 1O-YearT-Bond Yields Tl0Y MarketTiming Reports
23 AAA Bond Yields AAA MarketTiming Reports
24 BAA Bond Yields BAA MarketTiming Reports
time-series operators. This section describes the transformations used to
producethese indicators.
The indicators are presented in four categories: (1) price and volume
functions, (2) market-breadth indicators, (3) prices-of-debt instruments,
and(4) interest-rate-spreadindicators.
Price & Volume Functions. Fifteen (15) indicators were functions
that combine price and volume information. A number ofstudies have



==================================================
                     PAGE 423                     
==================================================

Case StudyofRule Data Mining for the S&P 500 407
shown that trading volume contains useful infomlation both on its
own and when used in conjunction with price.22 Technical analysis
practitioners have suggested a number of price and volume functions:
on-balance volume, accumulation distribution volume, money flow,
negative volume, and positive volume. Each of these functions is de
scribed below.
The price-volume functions were used to create two types ofindica
tors: (1) cumulative sums and (2) moving averages. An indicator defined
as a cumulative sum is the algebraic sum of all prior daily values ofthe
price-volume function. The daily value ofa price-volume function can ei
ther be a positive or negative quantity. Thus, an indicator defined as the
cumulativesum ofthe on-balancevolume, atagivenpointintime, isequal
to the sum ofall priorvalues ofthe daily on-balance-volume quantity. An
indicator defined as a cumulative sum will display long-term trends simi
lar to those observed in the levels of asset prices and interest rates. In
otherwords, these indicatorsare nonstationarytimeseries.
In contrast, a movingaverage ofaprice-volumefunctionwillbe asta
tionary time series. In other words, it will not display trends. This is ex
plained bythefact thata movingaverage onlyconsidersthe observations
within the look-back span. Since price and volume functions can assume
both positive or negative values, a moving average will tend to remain
withina relatively confined range nearzero. See Figure8.9.
Cumulative Sum
Price-Volume Function
Time
Moving Average
Price-Volume Function
b
1\
f"v\
f\ Vrv \\ JJ[\\Tf\VJ.
I~V V
o
Time
FIGURE 8.9 Cumulative sum versus moving average ofprice-volume functions.



==================================================
                     PAGE 424                     
==================================================

408 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
Cumulative On-Balance Volume. Perhaps the first invented price &
volume function was the cumulative on-balance-volume indicator,
which is attributed toJoseph Granville and Woods andVignolia.23Itis a
cumulative sum of signed (+ or -) total market volume. The algebraic
sign of the volume for any given day is determined by the sign of the
change in a market index for the current day, week, or whatever time
interval is being used. The case study used daily market changes in the
S&P 500 to determine the appropriate sign for the volume. Ifthe price
ofthe index rose on a given day, that day's entire NYSE volume was as
signed a positive value and the number ofshares traded was added to
the prior cumulative sum. On days when the price declined, the NYSE
volume was assigned a negative value and added to the prior cumula
tive sum (Le., subtracted).
The computation for the cumulative on-balance volume is shown in
thefollowing equation:24
Cumulative On-Balance Volume (COBV)
COBVt = COBVt-l + (ftVt)
ft = 1.0 if PCt> PCt-l
ft =-1.0 if PCt < PCt-l
ft = 0 if PCt = PCt-l
Where:
OBVt = (ftVt) = On-Balance Volume for day t
COBVt = Cumulative On-Balance Volume for day t
Vt= NYSE volume for day t
PCt= Closing price S&P 500 for day t
Moving Averages ofOn-Balance Volume. In additionto the cumula
tive on-balance volume, two moving averages of daily on-balance vol-



==================================================
                     PAGE 425                     
==================================================

Case StudyofRule Data Mining for the S&P 500 409
ume were considered: for 10 and 30 days (OBVlO, OBV30). In ISL they
would be:
OBVl O=MA(OBV,10)
OBV30=MA(OBV,30)
where OBVrepresents the on-balancevolumefora givenday.
Cumulative Accumulation-Distribution Volume (CADV). Another
price-volume function, accumulation distribution volume, is
attributed to Marc Chaiken.25 However, there are several similar func
tions including: Intra-day Intensity developed by David Bostian and
Variable Accumulation Distribution developed by Larry Williams.
These transformations attempt to overcome a perceived limitation in
on-balance volume, which treats the entire day's volume as positive
(negative) even ifthe market index had a miniscule positive (negative)
price change. Itwasfelt that minorprice changes should not be treated
the same as large price changes. The proposed modifications to on
balance volume sought to ameliorate on-balance volume's all-or-none
approach.
Chaiken proposed that a day's price activity could be more accu
rately characterized by quantifying the position of the closing price
within the high-low range. Specifically, his calculation takes the differ
ence betweenthe closing price and the midpoint ofthe daily range and
divides the difference by the size ofthe range. The assumption is that,
when the marketcloses above the midpointofits daily range, bullish or
accumulation activity dominated the day's trading activity (accumula
tion) and should predict higher prices. Conversely when the market
closes below the midpoint ofits daily range, the day is said to be domi
nated by bearish or distribution activity and should predict lower
prices. Only if the index closed at the low ofthe day or the high ofthe
day isthe entire day'svolume assigned a negative (distribution) orposi
tive (accumulation) value. More typically, the close is somewhere
withinthe day's range resultinginsomefraction ofthevolume beingas
signed to accumulation (positive volume) or to distribution (negative
volume). The range factor, which is defined below, quantifies the posi
tion of the closing price within the day's range. The range factor is
multiplied by the daily volume, and this figure is accumulated to pro
duce the cumulative sum. The calculation is shown in the following
equation.
26



==================================================
                     PAGE 426                     
==================================================

410 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
Cumulative Accumulation/Distribution Volume
CADVt = CADVt-l + (Vt x Rft)
(PCt - Pit) - (Pht- PCt)
Range Factor (Rft) =
(Pht- Pit)
Where:
ADVt=VtxRft=Accumulation/Distribution Volume for day t
CADVt= Cumulative Accumulation/Distribution Volume for day t
Vt = NYSE volume for day t
Rft= Range factor for day t
PCt= Closing price DowJones Industrials for day t
Pit= Low price DowJones Industrials for day t
Pht = High price DowJones Industrials for day t
The case study used data for Dow Jones Industrials for the calcula
tion oftheaccumulation-distributionfunction ratherthan theS&P500be
cause daily high and low prices were not available for it. I assumed that
the closingpositionofthe DowIndustrialswithinits range would besimi
larto the closingposition ofthe S&P500withinitsrange.
Moving Averages of Accumulation Distribution Volume. Moving
averages of daily accumulation/distribution values were constructed for
10-and 30-dayperiods. InISLtheyare:
ADVI O=MA(ADV,10)
ADV30=MA(ADV,30)
Cumulative Money Flow (CMF). Cumulative moneyflow (CMF) is
similar to CADY with the exception that the product of volume and
range factor isfurther multiplied bythe price levelin dollars ofthe S&P
500 Index. This is intended to measure the monetary value of the vol
ume. The price level used is an average ofthe daily high, low, and clos
ing prices. The computationis shown in the following equation:27



==================================================
                     PAGE 427                     
==================================================

Case StudyofRule Data Mining for the S&P 500 411
Cumulative Money Flow
CMFt = CMFt-l + (Vt x APt x Rft)
(PCt - Pit) - (Pht - PCt)
Range Factor (Rft) =
(Pht- Pit)
Where:
MFt = (Vt x APt x Rft) = Money Flow for day t
CMFt = Cumulative Money Flow for day t
APt= Average price S&P 500 for day t: (Hi + La + Close)/3
Vt= NYSE volume for day t
Rft= Range Factor for day t
PCt= Closing price DowJones Industrials for day t
Pit= Low price DowJones Industrials for day t
Pht= High price DowJones Industrials for day t
Moving Averages of Money Flow. Moving averages of money flow
wereconstructedfor 10-and30-dayperiods. InISLtheyare:
MFIO=MA(MF,10)
MF30=MA(MF,30)
Cumulative Negative Volume Index (CNV). Cumulative negative
volume is an accumulation ofprice changes on days oflowertrading vol
ume. In otherwords, itis a continuous algebraic sum ofdailypercentage
changes in a stock market index on days when volume is less than the
priorday. Whenvolumeisequalto orgreaterthanapriorday, the CNVre
mains unchanged.
The inventor ofCNV is unclear. Several sources attribute it to Nor
man Fosback28 but several sources, including Fosback, attribute the
ideato Paul Dysart.29 However, it seems clearthat it was Fosback who
developed objective signal rules for CNV and evaluated their predictive
power.30
The notionofnegative volume is basedonthe conjecturethat"trad
ing by unsophisticated investors occurs predominately on days ofexu
berantly rising volume, whereas informed buying and selling (Le.,



==================================================
                     PAGE 428                     
==================================================

412 CASESTUDY: SIGNALRULES FORTHES&P sao INDEX
smart-money activity) usually occurs during quieter periods of declin
ingvolume. Therefore, the directionthat the marketassumesondays of
negative volumepurportedlyreflects accumulation (buying) ordistribu
tion (selling) ofstockby those who are in the know."31
The computationofthe indexisasshowninthefollowing equation:
Cumulative Negative Volume Index (CNV)
CNVt = CNVt-l + ft
ft = [(PctlPCt-l)-1] x 100 if Vt < Vt-l
a
ft = if Vt> or = Vt-l
Where:
NVt = ft = Negative Volume Index for day t
CNVt = Cumulative Negative Volume Index for day t
Vt= NYSE volume for day t
PCt = Closing price S&P 500 for day t
MovingAverages ofNegativeVolume Index. Movingaveragesofthe
daily value ofnegative volume index (i.e., index's percentage change on
lowervolume days) were constructedfor 10-and3D-dayperiods. InISL:
NV1O=MA(NV,10)
NV30=MA(NV,30)
Cumulative Positive Volume Index (CPV). Cumulative positive vol
ume (CPV) isthe opposite ofCNV. Itis analgebraic accumulation ofmar
ket index changes for days when volume is greater than the prior day.
Positive volume index is variously attributed to Fosback32 and to Dysart.
The calculation is presented in the equation that follows. It seems as if
Dysart proposed the positive volume index and suggested ways ofinter-



==================================================
                     PAGE 429                     
==================================================

CaseStudyofRule Data Mining for the S&P 500 413
pretingitinasubjectivemanner, whereas Fosbackdefinedobjectiverules
andformally testedthem:
Cumulative Positive Volume Index (CPV)
CPVt = CPVt-l + ft
ft = [(PctlPCt-l)-1] x 100 if Vt> Vt-l
ft = 0 if Vt < or = Vt-l
Where:
PVt = ft = Positive Volume Index for day t
CPVt = Cumulative Positive Volume Index for day t
Vt= NYSE volume for day t
PCt= Closing price S&P 500 for day t
Moving Averages of Positive Volume. Moving averages of daily val
ues ofthepositivevolumeindexwereconstructedfor 10-and30-dayperi
ods (PVIOand PV30). InISL:
PV1 O=MA(PV,10)
PV30=MA(PV,30)
Market Breadth Indicators. Market breadth refers to the spread or
difference between the number ofstocks advancing and the number de
clining on a given day, week, or other defined time interval. Breadth has
been measured in a variety ofways, and they are reviewed in a scholarly
study by Harlow.33 For purposes ofthe case study, breadth is defined as
the dailyadvance-decline ratio; thatis, itis the difference betweena day's
advancing and declining issues divided by the total number of issues
traded. The datais based onall issuestraded onthe NYSE ratherthan an
alternative version that is restricted to NYSE common stocks. The series
restricted to common stocks, which some have claimed is superior be
cause it excludes closed-end stock and bond funds and preferred stocks,
was notavailableincomputer-readable form forthe time periodrequired.



==================================================
                     PAGE 430                     
==================================================

414 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
The case study's breadth indicators are of two forms: cumulative
sums of daily figures and moving averages of daily figures. For reasons
previouslyexplained, thebreadthindicatorsthatarecumulativesumsdis
playlong-termtrends, whereasmoving-average breadthindicatorstendto
have reasonablystablemeanvaluesandfluctuation ranges.
Cumulative Advance-Decline Ratio (CADR). The cumulative ad
vance decline ratio or CADR is a cumulative sum ofthe daily advance
decline ratio:
CADRt = CADRt-l + ADRt
ADRt = advt- dect
advt+ dect+ uncht
Where:
CADRt = Cumulative Advance/Decline Ratio for dayt
ADRt =Advance/Decline Ratio for day t
advt= NYSE advancing issues for day t
dect = NYSE declining issues for day t
uncht= NYSE unchanged issues for day t
Moving Averages of Advance-Decline Ratio. Moving averages of
dailyadvance-decline ratio (ADR) wereconstructedfor 10-and30-daype
riods (ADRIOandADR30). InISLtheyare:
ADR1O=MA(ADR,10)
ADR30=MA(ADR,30)
Cumulative Net Volume Ratio (CNVR). Another measure ofmarket
breadthisthenetvolumeratio. Itis basedonthedifferencebetweendaily
upsideanddownsidevolume. Upside (downside)volumeisthetotalnum
berofsharestradedinstocksthatclosedup (down)fortheday. Theinno
vation of calculating and analyzing upside and downside volume
separatelyisattributedto LymanM. Lowryin 1938.34Thedaily netvolume
ratio, a quantity thatcan assume eitherpositive ornegative values, is the
difference between upside and downside volume divided by total trading
volume. The case study used statistics released daily by the New York



==================================================
                     PAGE 431                     
==================================================

Case StudyofRule Data Mining for the S&P 500 415
Stock Exchange. The cumulative net volume ratio (CNVR) is the cumula
tivesumofdailyratios:
Cumulative Net Volume Ratio (CNVR)
CNVRt = CNVRt-l + NVRt
NVRt upvolt- dnvolt
upvolt+ dnvolt + unchvolt
Where:
CNVRt = Cumulative Net Volume Ratio for day t
NVRt = Net Volume Ratio for day t
upvolt= NYSE advancing volume for dayt
dnvolt= NYSE declining volume for day t
unchvolt= NYSE unchanged volume for day t
Moving Averages ofNet Volume Ratio. Moving averages ofthe daily
netvolume ratio were constructedfor 10-and3D-dayperiods (NVRlOand
NVR30). InISL:
NVR1O=MA(NVR,10)
NVR30=MA(NVR,30)
Cumulative New Highs-Lows Ratio (CHLR). A third measure of
breadth, the cumulative new highs and new lows ratio (CHLR), takes a
longer-term view. In contrast to CADR and CNVR, which are based on
dailypricechangesand dailyvolumes, CHLRisbasedonastock'scurrent
pricerelativetoitsmaximumandminimumpriceovertheprecedingyear.
Specifically, CHLRisthe cumulative sumofthe daily new highs-new lows
ratio (HLR), a quantity that can assume positive or negative values. The
high-low ratio is defined as the difference between the number ofstocks
making new 52-week highs on the day and the number ofstocks making
new 52-week lows on the same day divided by the total numberofissues
traded thatday. Thisfigure isaccumulatedalgebraicallyto obtainCHLR.
Thereisnothingparticularlysignificantaboutusing alook-backinter
val of52 weeks. It is simply the mostly widely available statistic for new
high and new low data. Priorto 1978, the number ofissues making highs
and lows was not based on a consistent 52-week look-back span. From
the middle ofMarch in any given year, the data were based only on the



==================================================
                     PAGE 432                     
==================================================

416 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
current calendar year. Prior to the middle of March, the figures were
based on the current and prior years. Thus, over the course of a given
year, the look-back span used to determine ifa stock was making a new
low ora new highvariedfrom 2.5months in mid-March to as longas 14.5
months just before mid-March. Most of the data used in this study was
post-1978.
Despite the distortions created by the variable look-back span, rule
tests based on pre-1978high-low datasuggestedthatthe indicator'sinfor
mationcontentwasrobusttothis distortion. Forexample, Fosbackdevel
oped and tested an indicator called the high-low logic index over the
period 1944to 1980. His results showed that indicator demonstrated pre
dictivepoweroverthe entireperiodeventhoughthe bulkofthe datawas
infected with the variable look-back span distortion.35 The formula for
constructingthe CHLRseriesisshownhere:36
Cumulative New Highs New Lows (CHLR)
CHLRt = CHLRt-l + HLRt
HLRt = nuhit- nulot
advt+ dect+ uncht
Where:
CNHLt = Cumulative New Highs New Lows on day t
HLRt = New Highs New Lows on day t
nuhit= NYSE new 52-week highs day t
nulot= NYSE new 52-week lows day t
advt= NYSE advancing issues for day t
dect = NYSE declining issues for day t
uncht = NYSE unchanged issues for day t
Moving Averages of New HighslNew Lows Ratio (HLRI and
HLR30). Moving averages ofthe daily higMow ratio were constructed
for 10-and30-dayperiods (HLRlOandHLR30). InISLtheyare:
HLR1O=MA(HLR,10)
HLR30=MA(HLR,30)
Pl'ices-of-Debt Instruments from Interest Rates. Typically, inter
estratesandstockpricelevelsmoveinversely.However, bytakingtherecip-



==================================================
                     PAGE 433                     
==================================================

Case StudyofRule Data Mining for the S&P 500 417
rocal (1/interest rate) interest rates can be transformed into price-like
time series that are, in general, positively correlated with stock prices.
This reciprocal series can be multiplied by a scaling factor such as 100.
Thus, a rate of6.05percentwould be equivalentto a price of15.38(1/6.05
x 100).
This transformation was used in the case study and was performed
on four interest rate series: three-month treasury bills, lO-yeartreasury
bonds, Moody's AAA corporate bonds, and Moody's BAA corporate
bonds.
Interest Rate Spreads. An interest-rate spread is the difference
between two comparable interest rates. 1\vo types of interest-rate
spreads were constructed for the case study; the duration spread and
the quality spread. The duration spread, also known as the slope ofthe
yield curve, is the difference betweenyields on debt instruments having
the same creditqualitybuthavingdifferentdurations (Le., time to matu
rity). Thedurationspreadused inthecasestudywasdefinedastheyield
onthe 10-yeartreasurynote minustheyieldonthe three-monthtreasury
bills (IO-year yield minus 3-monthyield). The spread was defined in this
way rather than 3-month minus 10-year so that an upward trend in the
spreadwouldpresumablyhave bullishimplicationsforthe stockmarket
(S&P500).37
A quality spread measures the difference in yield between instru
ments with similar durations butwith different credit qualities (default
risk). The quality spread for the case study was based on two of
Moody's38 long-term corporate bond series: AAA,39 which are the high
est rated corporate debt, and BAA,40 a lower rated grade of corporate
debt. The qualityspread is defined here as AAAyield-BAAyield. When
rates on lower quality debt (higher default risk such as BAA rated
bonds) are falling faster than rates on higher quality debt such as AAA
rated bonds, it is interpreted as indication that investors are more will
ing to assume additional risk to earn the higher yields on lower quality
debt. In otherwords, a rising trend in the quality spread is a signal that
investors are willing to take higher risks to earn higher returns. Bythis
reasoning, the quality spread should trend in the same direction as
stocks.
1f\BLE OF 40 INPUT SERIES USED IN CASE STUDY
Table8.2listsallofthe dataseriesusedasrule inputsinthe casestudy.



==================================================
                     PAGE 434                     
==================================================

418 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
T.!\BLE 8.2 InputSeries Used in the Case Study
Number Description Abbreviation Form
1 S&P 500 Close SPX Raw
2 S&P 500 Open SPO Raw
3 S&P Industrials Close SPIA Raw
4 DowjonesTransportation Index DjTA Raw
5 Dowjones UtilityIndex DjUA Raw
6 NYSEFinancial Index NYFA Raw
7 NASDAQComposite OTC Raw
8 Value Line Geometric Close VL Raw
9 Cumulative On-Balance Volume COBV Indicator
lOOn-Balance Volume 1O-Day MA OBV10 Indicator
11 On-Balance Volume 30-Day MA OBV30 Indicator
12 Cumulative Accum. Distr. Volume CADV Indicator
13 Accum. Distr. Volume 1O-Day MA ADV10 Indicator
14 Accum. Distr. Volume 30-Day MA ADV30 Indicator
15 Cumulative MoneyFlow CMF Indicator
16 Money Flow 1O-Day MA MF10 Indicator
17 Money Flow 30-Day MA MF30 Indicator
18 Cumulative Negative Volume Index CNV Indicator
19 Negative Volume Index 1O-Day MA NV10 Indicator
20 Negative Volume Index 30-Day MA NV30 Indicator
21 Cumulative Positive Volume Index CPV Indicator
22 Positive Volume Index 1O-Day MA PV10 Indicator
23 Positive Volume Index 30-Day MA PV30 Indicator
24 Cum. Advance Decline Ratio CADR Indicator
25 Advance Decline Ratio 1O-Day MA ADR10 Indicator
26 Advance Decline Ratio 30-Day MA ADR30 Indicator
27 Cum. Up Down Volume Ratio CUDR Indicator
28 Up Down Volume Ratio 1O-Day MA UDR10 Indicator
29 Up Down Volume Ratio 30-Day MA UDR30 Indicator
30 Cum. New Highs/New Lows Ratio CHLR Indicator
31 New Highs/New Lows Ratio 1O-Day MA HLRIO Indicator
32 New Highs/New Lows Ratio 30-Day MA HLR30 Indicator
33 NYSEVolume TVOL Raw
34 Dowjones 20 Bond Index DjB Raw
35 Price 3-Month T-Bill PT3M Indicator
36 Price 1O-YearTreasury Bond PT10Y Indicator
37 Price AAA Corporate Bonds PAAA Indicator
38 Price BAA Corporate Bonds PBAA Indicator
39 Duration Spread (10 Year-3 Month) DURSPD Indicator
40 Quality Spread (BAA-AAA) QUALSPD Indicator



==================================================
                     PAGE 435                     
==================================================

Case StudyofRule Data Mining for the S&P 500 419
THE RULES
The rules tested in the case study can be grouped into three categories,
each representing a different theme oftechnical analysis: (1) trends, (2)
extremes and transitions, and (3) divergence. The following sections de
scribethe rules in eachcategory.
Trend Rules
The first category ofrules is based on trends. Afoundational principle of
TAis thatpricesandyields move in trendsthatcanbeidentifiedina suffi
ciently timely manner to generate profits. Practitioners have developed a
variety ofobjective indicators to define the direction ofthe currenttrend
and signaltrend reversals. Among the mostwidelyused are moving aver
ages, moving-average bands, channelbreakout, and Alexanderfilters also
known as zigzagfilters. These are described in Kaufman41 and there is no
need to coverthem allhere.
Thetrend rules in the case study used the channelbreakoutoperator
or CBO to define trends in the input time series. Thus, the CBO operator
transformed the inputtime seriesinto a binaryvalued timeseries consist
ingof+1and -1. When the trend ofthe inputseries was in an uptrend, as
determined by the CBO, the rule's output was +1. Conversely, when the
analyzedserieswasdetermined to beina downtrend, theoutputwas-1.
The identification oftrend reversals in the inputseries by CBO is sub
jectto lag. All trend indicators necessarily incurlag-adelay between the
time the inputseries experiences a trend reversal and the time the opera
tor is able to detect it. Lag can be reduced by making the indicator more
sensitive. Forexample, inthe case ofCBO, lagcanbereduced bydecreas
ing the numberofperiodsin the look-backspan. However, this fix creates
adifferentproblem-anincreasednumberoffalse signals. Inotherwords,
there is a tradeoff between a trend-following indicator's lag and its accu
racy. Therefore, all trendindicatorsattempttostrike a reasonable balance
between signal accuracy and signal timeliness. Finding the optimum is a
challenge in the design ofany signaling system, be it a household smoke
detectororaTAtrend rule. Inthe end, optimalis whatevermaximizesthe
indicator's performance, such as average return, risk-adjusted rate of re
turn, and so forth. Since the behavior of financial market time series
changesovertime, theoptimallook-backspanforthe CBOoperatorwould
changeaswell. AdaptiveversionsofCBOwere notused inthe case study.
Ofthe40inputseriesdepictedinTable8.2, 39were usedasinputsfor
the trend rules. The open price ofthe S&P 500, inputseries2inTable 8.2,
wasexcludedongroundsofbeingredundantoftheS&P500close.



==================================================
                     PAGE 436                     
==================================================

420 CASESTUDY: SIGNAL RULES FORTHES&P SOO INDEX
Each trend rule was defined by two parameters or arguments: the
CBO look-back span and a time series chosen from the set of39 candi
dates shown in Table 8.2. A set of 11 values were tested as look-back
spans. They were 3,5,8, 12, 18,27,41,61,91, 137, and 205 days. The val
ues were chosento be separated by a multiplierofapproximately 1.5. For
example, the look-back span of205 days is approximately 1.5 times 137
days. This resulted in a total of858 trend rules, 429 (39 x 11) based on a
traditionalTAinterpretation, andanadditional429 inverse-trendrules.
A traditional version ofa trend rule produced an output value of +1
(longpositionin the S&P500) when the inputtimeserieswasdetermined
to be in an upward trend by CBO operator, and an output value of -1
(shortpositionintheS&P500)when the timeserieswasindowntrendac
cording to CBO operator. Inverse trend rules simply produced the oppo
site signals (e.g., shortS&P500 when the analyzed serieswas determined
to beinanuptrend).
In Chapter9, where testresultsare reported, thefollowing shorthand
nan1ingconventionwill beusedforthe traditionaland inversetrend rules.
Anaming convention was necessitated by the large numberofrules. The
syntaxofthe namingconventionisasfollows (note thissyntaxisdifferent
from thesyntaxusedfor indicatorscripting):
Rule (TTorTI)- InputSeriesNumber- Look-backSpan
WhereTTstandsfora traditional trend rule
and
TI standsfor an inversetrend rule
Forexample, the rule namedTT-15-137, isa traditional trend rule ap
plied to input series 15 (Cumulative Money Flow) using a look-back
span of 137 says. Arule named TI-40-41 is an inverse trend rule applied
to inputseries40 (the quality spread BAA-AAA) using a look-backspan
of41 days.
Extreme Values and Transitions
The second category of rules considered in the case study is "Extreme
Values and Transitions" or E rules. This category is based on the notion
thata time series conveys information when it assumes an extreme high
or low value or as it makes the transition between extremes. High and
lowextremescan bedefined in termsoffixed value thresholds ifthe time
series has a relatively stable mean and fluctuation range (Le., is station
ary). All input series used for E rules were made stationary by applying
the CN operator.



==================================================
                     PAGE 437                     
==================================================

Case StudyofRule Data Mining for the S&P 500 421
Thirty-nine ofthe 40 input series were used for the E-type rules. The
open price ofthe S&P 500 was excluded on grounds ofredundancy. The
input series usedfor E rules were first smoothed with a four-day linearly
weighted moving average (LMA) before applying the CN operator. The
smoothingwas done to reduce the numberofsignals that would have re
sultedfrom an unsmoothedversionofthe inputserieswigglingaboveand
below thesignal threshold. Thoughsmoothingmay have beeninappropri
ate forseriesthatwere alreadysmoothed, for example the30-daymoving
average of negative volume, all series were treated with the LMA on
groundsofconsistency.
In ISL, the expression for the input time series used for E rules is
given bythe expression:
=CN (LMA(InputSeries, 4),N-days)
Where:
CNis the channelnormalization operator
LMAisalinearlyweighted moving-average operator
ThissequenceoftransformationsisillustratedinFigure8.10.
Input
Time Series
4-day LMASmoother
Smoothed
Time Series
CN Operator
{)
Smoothed fn(\nAu ~
Channel-Normalized ~.Q~
Series
."IGUKI': 8.10 Smoothed channel normalized series.



==================================================
                     PAGE 438                     
==================================================

422 CASESTUDY: SIGNALRULES FORTHES&P sao INDEX
E-rule signals were generated when the channel normalized smoothed
seriescrossedathreshold.Giventhattherearetwothresholds,anupperand
lower, andgiventhattherearetwo directionsin whichacrossingcanoccur
(upordown)therearefourpossiblethreshold-crossingevents:
1. Lowerthreshold iscrossedin the downward direction.
2. Lowerthreshold iscrossedinthe upward direction.
3. Upperthreshold iscrossedin the upward direction.
4. Upperthreshold iscrossedinthe downward direction.
These eventsare illustratedinFigure8.1l.
Each E rule was defined in terms of two threshold-crossing events:
onespecifyingthe longentry/shortexitand the otherspecifyingtheshort
entry/long exit. This scheme yields 12 possible E-rule types. They are
listed inTable 8.3. Because they coverall possibilities, there would be no
point in including inverse versions. Note that type 7 is an inversion of
Type 1,Type8isan inversionofType2, andsoon.
The 12E-ruletypesare illustratedinFigures8.12 to8.23.
Channel-Normalized
Smoothed
Series
Events
Upper
------th-r-eshold-------
T.im...e
50 I'------~------I------~:_----___,
2
Lower
--------- .------------------------
Threshold
o
Events
FIGURE 8.11 Threshold crossing events.



==================================================
                     PAGE 439                     
==================================================

Case StudyofRule Data Mining for the S&P 500 423
'Il\BLE 8.3 The 12 E-RuleTypes Defined in Terms ofThreshold Crossing Events
E-RuleTypes Long Entry/Short Exit Short Entry/Long Exit
1 Event 1 Event 2
2 Event 1 Event 3
3 Event 1 Event 4
4 Event 2 Event 3
5 Event 2 Event 4
6 Event 3 Event4
7 Event 2 Event 1
8 Event 3 Event 1
9 Event4 Event 1
10 Event 3 Event 2
11 Event4 Event 2
12 Event4 Event 3
100
Upper
-------Thres-holC"{-----
....
50 I'-------l_----~f__------'~----~
Lower
---------------------
Threshold
o
L S L S
I I I I
+1 ,----------
o Rule '
I----;: O~ u. t. p. u; t...--_+
,
,---.;..', ,--------+--
,
,--;.., ,~~
-1 ---------------------_. '-------------------------, '---_.
FIGURE 8.12 Extreme value and transition rule: Type 1.



==================================================
                     PAGE 440                     
==================================================

424
CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
100
Upper
-------Thres}lolc(-----
Time
.....
50 I'-.....j.---.-...-----....j.-+----~----__,
Lower
--Thre-s"hoicl"~-
a
S L S L
I I I I
+1 '-------"----"--"", ,,--------------
--;.-....R.."u..;l.e;.;:;;,,;.;;..---.+:------'------"",
"""----....
0 .....
: Output :
-1 ,------------------
f'IGURE 8.13 Extreme value and transition rule: Type 2.
100
Upper
------"Thres}iolc(---"-
Time
50 1'------1~~----__J.----_++-----.,....
Lower
--------------
Threshold
a
S L S L
I I I I
+1 --------------: :----------------------------; .,--------------
, Rule :
Ol------"---!---~.:.;.::.;~---~-_+_---_+
Output :
-1 --------- '-------.
FIGURE 8.14 Extreme value and transition rule: Type 3.



==================================================
                     PAGE 441                     
==================================================

Case StudyofRule Data Mining foy the S&P 500 425
100
-------fhrUepsp}eloricr----
....
501'-.......----~-----_I_-01_---~-----.....,
Lower
----------------
Threshold
a
S L S L
I I I I
+1 ,-------. 1----
a ~.......---------~: __.:..: ...",.:R.;.:u:.:,l.::,e _~
+-'
, :: Output
_1 : ~ I. •
FIGURE 8.85 Extreme value and transition rule: Type 4.
100
-------fhrUeps-pheoria------
Time
501'-----.j....;I,------I-----4-....-----.....
Lower
-.------------
Threshold
a
S L S L
I I I I
~ -.;.:R~~.:.::I~:..-_- --:~~-----+-:
+ 1---_-_--_-_--_-_--_-_-.-.:.. .i.:-_-_--_-.,,- -_-_- -_-_--.
Output
o
-1 I. •
FIGURE 8.86 Extreme value and transition rule: Type 5.



==================================================
                     PAGE 442                     
==================================================

426 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
100
Upper
-------Threslioi((-----
Time
50 I'--+----+~~----_+-+_--_++----___:r+
a
L S L S
I I I I
+1 ,---------,
a t----t:---+-:__---:~R.;.::u;.;.le~--...;..---..;....------_+
Output , ,
-1 • 1 ----------------------
FIGURE 8.17 Extreme value and transition rule: Type 6.
100
Upper
-------Thres-hola------
....
50 I'------~-----_+-------l,~----_.,
Lower
---------------------
Threshold
a
S L S L
I I.- I I
+1 , ------------------------,,
Rule , ,
Ot-----:~----:---t---~-------_+---+_ ....
Output ,
-1 "----------
FIGURE 8.18 Extreme value and transition rule: Type 7.



==================================================
                     PAGE 443                     
==================================================

Case StudyofRule Data Mining foy the S&P 500 427
100
Upper
-------Thres"tibia------
50 I'--+---~~-----I--+----~-----r+
Lower
----------------
lhreshold
a
L S L S
I I I I
+1 ,-----------------. .------------------.
a 1----!_~~R,;,;u..;.le,;,...-......-----_, +- •
Output , , ' ' , ,
-1 I. .1 1 --------
FIGURE 8.19 Extreme value and transition rule: Type 8.
100
Upper
-------Thres-tibia------
50 I'-----+~-----_f_----_+_~----~r+
Lower
--------------
Threshold
a
L S L S
I I I I
+1 --------. ,-------.
Rule ::
a 1------.;.,---+,---~O~u;,;,t;,;.p;..u--t --~:---+-----+
-1 --------------1 :---------------------------~ :--------------
FIGURE 8.20 Extremevalue and transition rule: Type 9.



==================================================
                     PAGE 444                     
==================================================

428 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
100
Upper
-------fhreslioia------
Time
...
50 I'--+-----'~-----.;__+_----~----__,
Lower
----------------
Threshold
o
s s
l l
I I
+1 :---------------------------~ .----------------------------.
o t----!'---------.....:. ' -:::'R_u~l..;.e~---T_ ...
, Output ,
-1 I. _
FIGURE 8.21 Extreme value and transition rule: Type 10.
100
Upper
-------fhreslioia------
Time
...
50 I'-----+-~-----....,.------.,H~------,
---.L_o--w-e--r----
Threshold
o
l S l S
I I I I
+1 ,.----------------~,
o Rule ,
,
: Output :
-1 '-----.------------ 1 _
FIGURE 8.22 Extreme value and transition rule: Type 11.



==================================================
                     PAGE 445                     
==================================================

Case StudyofRule Data Mining for the S&P 500 429
100
-------fhrUepsrpieoria------
Time
50 I'---+---+-~------I--+-----+~--------,""
a
S L S L
I I I I
.
,
1, 10 ,-----------o 0
FIGURE 8.23 Extreme value and transition rule: Type 12.
Parameter Sets and Total Number ofE Rules. An E rule is defined
by four parameters: type (I through 12), input series, displacement ofthe
thresholdsform 50, and look-backspanfor the channelnormalization. Be
cause boththe upperand lower thresholds are displaced equallyfrom the
midvalue of 50, it is possible to specify them with a single number, their
displacement from 50. For example, a threshold-displacement 10 places
theupperthresholdat60 (50 +10)andthe lowerthresholdat40 (50- 10).
1\vo different values for the threshold displacement parameter were
tested: 10and20.Thedisplacementvalueof20gavetheupperthresholdas
70 and the lower threshold as 30. Three different values were considered
forthe channelnormalizationlook-backspan: 15,30, and60.Allparameter
valueswerechosenwithoutoptimizationonthe basisofintuition.
Given12possibleruletypes, 39possiblecandidateinputseries, 2pos
siblethresholddisplacements, and3possible channelnormalizationlook
backspans, there were 2,808E-type rules (12 x39x 2x3).
Naming Convention for Extreme Value and Transition Rules. In
Chapter9, the following naming conventionwill be used to reportresults
forE rules:
(E)-(type)-(lnput Series)-(Threshold Displacement)-(Channel Nor
malizationLook-BackSpan)



==================================================
                     PAGE 446                     
==================================================

430 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
For example, E-4-30-20-60 would be: E rule, type 4, input series 30
(cumulative new highs/new lows ratio), a threshold displacement of 20
(upper = 70, lower = 30), and a channelnormalization look-back span of
60 days.
Divergence Rules
Divergence analysisis a foundational concept ofTA thatconcernsthe re
lationship between apairoftime series. Itis premised onthe notion that,
under normal circumstances, certain pairs ofmarket time series tend to
move up and down together, and when theyfail to do so itconveys infor
mation. A divergence is said to occur when one member ofthe pair de
parts from their shared trend. Typically, a divergence manifests itself as
follows: both series have been trending in the same direction, but then
one seriesreverses its priortrend whileits companion continuesits prior
trend. This event, according to divergence analysis, is a potential signal
that the prior shared trend has weakened and may be about to reverse.
This is illustrated in Figure 8.24. Hussman contends that divergence sig
nals are most informative when numerous time series are analyzed and a
significantnumberbeginto diverge.
42
The Dow theory is based on divergence analysis. It asserts that
when an index of industrial stocks and an index of transportation
Time Series
2
Time Series
1
Time
FIGURE 8.24 Divergence analysis.



==================================================
                     PAGE 447                     
==================================================

Case StudyofRule Data Mining for the S&P 500 431
stocks are trending in the same direction, the trend they share is
healthy and likely to persist. However, ifone series begins to diverge, it
is taken as preliminary evidence that the trend is weakening and may
reverse. Another application ofdivergence analysis considers the price
ofan instrumentas one time series and its rate ofchange ormomentum
as the second series. Price/momentum divergence analysis is discussed
byPring.43
A divergence leads to one of two outcomes: either the series, for
which the trend had remained undisturbed, will experience a reversal to
join its diverging companion or the diverging series will end its errant
ways and rejoin its companion. Acompleted signal ofreversal is notpre
sumeduntilbothserieshave convincinglyreversedandare again moving
in the same direction. Thus, the fundamental idea behind divergence
analysis is coherence, that is, the state of affairs when two waveforms
are in phase with each other (see Figure 8.25). When two series are co
herent, their common trend is considered strong and expected to con
tinue. However, when their trends become incoherent, or out of phase,
the future ofthe once shared trend is in question. Therefore, candidates
for divergence analysis are pairs oftime series that are generally coher
ent, butwhentheydo diverge, oneparticularmemberofthepairtendsto
have leading information about the other. In other words, there is a
ratherstable lead-lagrelationship.
Coherent
Uptrends
Trend Should
Persist
Coherent
Downtrends
Trend Should
Persist
Time
FIGURE 8.25 Trend coherence and divergence.



==================================================
                     PAGE 448                     
==================================================

432 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
Subjective Divel'gence Analysis. Subjective divergence analysis
typically involves comparing the peaks and troughs of the two time
series under consideration. A negative or bearish divergence is said to
occur if one series continues to register peaks at successively higher
levels while the other series begins forming peaks at lower levels.
The failure by the second series to form peaks at successively higher
levels is also termed a bearish nonconfirmation. This is illustrated in
Figure 8.26.
Apositiveorbullishdivergenceissaidto occurwhenoneseriescon
tinues to registersuccessivelylowertroughs inan established downtrend
while the second series begins to form troughs at higher levels. This is
alsotermeda bullishnonconjirmation. SeeFigure8.27.
The problem with subjective divergence analysis, or any subjective
method for that matter, is that it the procedure is neither repeatable nor
testable. One issue requiring the analyst's subjectivejudgmentis the tim
ing ofpeaks and troughs. Because the two time series may not peak and
trough at precisely the same time, it may be unclear which peaks and
troughs should be compared. Another issue is related to duration: How
long must the divergence be inexistence before deciding thatone hasoc
curred? Still a third unresolved issue is how much upward (downward)
Series 1
+
+ ,
,
,
,
,
,
,
Diverging Peaks
i r
Confirmed Peaks
i
~
j
Series 2
Time
FIGURE 8.26 Negative divergence (peaks compared).



==================================================
                     PAGE 449                     
==================================================

Case StudyofRule Data Mining for the S&P 500 433
Series 2
Confirmed Troughs
Series 1
Time
FIGURE 8.27 Positive (bullish) divergence: troughs compared.
movement in a series is required to establish that a new trough (peak)
hasformed.
An Objective Measure of Divergence. These problems can be ad
dressed with objective divergence analysis. A method that objectifies
peak and trough comparisons is discussed by Kaufman44 and he pro
vides computer code. Kaufman also discusses a second method that
uses linear regression to estimate the slopes in the two time series
being compared. A divergence is defined as the differences in their
slopes. Bothmethodslookinterestingbutno performancestatistics are
offered.
The divergence rules considered in this case study were inspired by
the objective divergence method discussed by Heiby45 in his book, Stock
Market Profits through Dynamic Synthesis. The two series being exam
ined by Heiby were first detrended with channel normalization. He de
fined a divergence to be in effect ifone series had a channel-normalized
value of 75 or greater while the other series had a value of 25 or less.
Thus, divergence was quantified in terms of the difference in channel
normalizedvalues.
Thissuggestedan initialformulation for a divergenceindicatorthatis
defined in the equation that follows. Note that one series is always the
S&P 500, the target market ofthe case study. The companion series, for



==================================================
                     PAGE 450                     
==================================================

434 CASESTUDY: SIGNAL RULES FORTHES&P SOO INDEX
example the DowJones Transports, is the serieswith which the S&P 500
is compared. The case study compared the S&P 500 to all the time series
in Table 8.2, except the S&P 500 open price, which was excluded on
groundsofredundancywiththeS&P500close.
Divergence Indicator
(Initial Formulation)
=CN (Companion Series, n) - CN (S&P 500, n)
Where:
CN = Channel normalization operator
n = Look-back span ofthe channel normalization
Because the channel normalized value of each series can vary be
tween0and 100,this divergenceindicatorhasapotentialrange of-100to
+100. Forexample, ifthe companionseriesis atthe bottom ofits channel
range (CN= 0)andtheS&P500isatthetopofitsrange(CN = 100),thedi
vergenceindicatorwouldhaveavalueof-100. Itshouldbenotedthatthis
quantification ofdivergence is merelyone approach among severalpossi
bilitiesthatmayprovesuperior.
Limitations of the Proposed Divergence Indicator. The proposed
divergence indicatormeasures the degree to which two timesseries have
similarpositionswithintheirrespectivechannels. Ingeneral,thiswillgive
a reasonable quantification ofthe degree to which the series are in phase
with one another. For example, when the indicator registers a value of
zero, it indicates that there is no divergence; both series have the same
channel normalized values and can be presumed to be trending together.
However, there can be cases for which a value ofzero does not indicate
that the two series are inphase. Forexample, iftwo time series are nega
tivelycorrelated,thatistosay,theytendtomoveinverselywithrespectto
each other within their channels, the divergence indicator will assume a
value ofzeroastheseries-normalizedvalues crosspaths. In this instance,
a value ofzero would be an erroneous indication that the two series are
trending together. This is illustrated in Figure 8.28 and is clearly a limita
tionoftheproposeddivergenceindicator.
This problem couldhave been avoided with a more complex formu
lation ofdivergence indicatorbased on cointegration,46 a concept devel
oped in the field of econometrics. Cointegration was proposed in 1987
47
by Engle and Granger. This more sophisticated and potentially more



==================================================
                     PAGE 451                     
==================================================

Case StudyofRule Data Mining for the S&P 500 435
Channel-Normalized Value Companion Series
a
Channel-Normalized Value S&P 500
100
Divergence Indicator
+100
OCB-----~~----~+------A_----~~
~ \----/i ;""'/
Erroneous Indications ofIn-Phase Behavior
FIGURE 8.28 Erroneous indications ofin-phase behavior.
accurate way of measuring divergences uses regression analysis to de
termine ifa linearrelationship exists between two time series. Because
this method can easilyaccommodate the casein which thepairofseries
are strongly negatively correlated (180 degrees out of phase), it solves
theproblem discussedearlierthatcanoccurwitha divergenceindicator
based on channel normalization. An additional advantage of cointegra
tion analysis is that it first applies a statistical test48 to determine ifthe
two series have related trends (i.e., that they are cointegrated). If the
testshowsthattheyare cointegrated, astationarydivergence indicator49
naturallyfalls outofthe analysis. In cointegration terminology, this indi
catorisreferredto asthe error-correctionmodel. Itmeasuresthe degree
to which one series has diverged from the typical linearrelationship be
tween the twoseries.
In most applications of cointegration, the error-correction model is
used to predict the behavior ofthe spreadbetweenthe cointegratedtime
series. In these applications, the divergence is predictedto revert backto
the nOffilal state of a shared trend when the error-correction model be
comes extremely positive or negative. In other words, when the error,
which isa departure from the normal linearrelationship betweenthe two



==================================================
                     PAGE 452                     
==================================================

436 CASESTUDY: SIGNALRULES FORTHES&P sao INDEX
series, becomes extreme, it is predicted to correct back to its normal
value ofzero.
Atypical application ofthis analysis ispairs tmding. Forexample, if
ithas been determined thatFordand GMstocksare cointegrated timese
ries and Ford has gotten high relative to GM, a short position would be
taken in Ford against a long position in GM. Thus, no matter how the di
vergence is corrected (Ford falls, GM rises, or some combination) the
pairstrade will generatea profit.
FosbacJ<50madeaninnovativeuseofcointegrationanalysistodevelopa
stockmarketpredictor. Hisinnovationwastouse the measurementderived
fromtheerrorcorrectionmodelasanindicatortopredictthemovementofa
thirdvariable, theS&P500,ratherthanpredictthebehaviorofthespreadbe
tween the cointegrated variables. The indicator, called the Fosback Index,
exploits the fact that interest rates and mutual fund cash levels are cointe
gratedtimeseries. Fosback'sworkisparticularlynoteworthybecauseitpre
cededtheEngleandGrangerpublicationoncointegrationbyoveradecade.
The FosbackIndexsignalswhenthemutualfund cashleveldivergessignifi
cantlyfrom alevelpredicted byitslinearrelationship withshort-terminter
est rates. The premise is that when mutual fund managers are excessively
pessin1istic aboutstock marketprospects, they hold cash reserves thatare
substantiallyhigherthanthethatpredictedbyinterestrates. Excessiveopti
mism is the opposite situation. Fosback found that both excessive pes
simism and excessive optimism, measured in this fashion, correlated with
future returnsonthestockmarket.Ineffect,theFosbackIndexremovesthe
influenceofshort-terminterestratesfrom mutualfund cashlevels,thuspro
vidingapurermeasureoffundmanagersentimentthanthatprovided bythe
rawcashlevel,whichiscontaminatedbyinterestrates.
Intheinterestsofsimplicity, thecointegrationtechniquewasnotused
for the divergence indicators in the case study. It was assumed that ifa
companionserieswas notrelated to the S&P500, it would berevealed by
the poor financial performance of the divergence rule. However, 151 be
lieve thatthe cointegration methodology warrantsfurther investigation in
developmentofindicators.
Need for Double Channel Normalization. A second problem with
the initially proposed version ofthe divergence indicator was more seri
ous and had to be remedied. The divergence rules in the case study in
volved pairing the S&P 500 with 38 other time series. Given that these
serieshadvaryingdegrees ofco-movementwith the S&P 500, the fluctua
tion range ofthe divergence indicator would vary considerably from one
pairto the next. Thiswould make itimpracticalto use thesamethreshold
for all pairings. This problem is illustrated in Figure 8.29. Note that the
high threshold displacement that would be suitable for a companion se-



==================================================
                     PAGE 453                     
==================================================

Case StudyofRule Data Mining for the S&P 500 437
+100
Companion Series Has High Degree ofCo-Movement with S&P 500
o
Time
-100
Companion Series Has Low Degree of
+100
Co-Movement with S&P 500
o
-100
I?IG HE 8.29 Divergence indicator: inconsistent volatility.
rieswithalowdegreeofco-movementwith theS&P500wouldneverpro
duce a signal for a companion series with a high degree ofco-movement
to the S&P 500. Forthis reason, the initial formulation ofthe divergence
indicatorwas deemed impractical.
Thisproblemwasaddressed witha modifiedformulation ofthe diver
gence indicator illustrated in the equation that follows. It employs the
channel normalization operator twice. That is to say, the indicator is a
channel-normalizedversionofthe initial divergenceindicator.
Divergence Indicator
(Double Channel Normalization)
=CN{CN (Series 1, n) - CN (S&P 500, n), 10n}
Where:
CN = Channel normalization operator
Series 1 = Companion series
n = Look-back span of the first channel normalization



==================================================
                     PAGE 454                     
==================================================

438 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
The second layer of channel nom1alization takes into account the
fluctuation range of the initial formulation of the divergence indicator.
This solves the problem ofinconsistent fluctuation ranges across the 38
pairsoftimeseries. Asa result, the modifiedversion ofthe divergencein
dicator will have roughly the same fluctuation range irrespective of the
particular pair oftime series being used, making it practical to use uni
form thresholds.
Thelook-backspanforthesecondlevelofchannelnormalizationwas
setat10timesthe look-backintervalusedforfirst level. Thus, ifthe chan
nel normalization used a look-back span of 60 days, the second layer of
channelnormalization used a look-backspan of600days. Itwasassumed
that a lO-fold look-back span would be sufficientto establish the fluctua
tion range of the basic divergence indicator. Note that a modified diver
genceindicatorhasapotentialfluctuation range of0to 100, similarto any
channel-normalizedvariable. SeeFigure8.30.
Divergence Rule Types. Upperand lowerthreshold wereapplied to
the modified divergence indicatorto generatesignals. Apositive orbull
ish divergence was in effect when the divergence indicator was above
its upper threshold. This occurred when the companion series was
moving up more or moving down less than the S&P 500. This was evi
denced bythe companionserieshavinga higher relativepositionwithin
its channel than the S&P 500. Conversely, a negative or bearish diver
gence existed when the divergence indicator was below the lower
threshold. This occurredwhen the companion series was moving down
more ormoving up less than the S&P500, resulting inits havinga lower
relative position within its channel than the S&P 500. This is illustrated
inFigure 8.30.
Positive Divergences
/
100 ~ ~
50F---------~---~f_----'~---.....,.'----_+
--------'(hrLeoswhoe-r'c{---------
o
Negative Divergences
FIGURE 8.30 Modified divergence indicator.



==================================================
                     PAGE 455                     
==================================================

Case StudyofRule Data Mining for the S&P 500 439
The question was how to create signaling rules from the modified di
vergence indicator. Two rules seemed obvious: a bullish divergence rule,
which would call for long positions in the S&P 500 when the divergence
indicator was above the upper threshold, and a bearish divergence rule,
which would call for short positions in the S&P 500 when the divergence
indicatorwasbelowitslowerthreshold.
Both rules assumed that the companion series had leading informa
tion about the S&P 500. If the companion series was stronger than the
S&P 500 (positive divergence), then a long position would be justified,
whereas ifthe companion series was weak (negative divergence) a short
positionwouldbejustified. However, this assumptionmaybeincorrect. It
may be the case that negative divergences are predictive ofhigher prices
for the S&P 500, whereas positive divergences are predictive of lower
prices. Inotherwords, perhapsitis theS&P500thathas the leadinginfor
mation. Thissuggeststhatinverseversionsofeachruleshould betried. In
the end, it was determined that there are 12 possible divergence rule
types, and all were tested.
These 12 rule types are exactly the same set used for the extreme
value and transition rules. This makes sense because the modified diver
gence indicator is similar to the indicator used for the E rules because it
hasafluctuation range of0to 100and hastwothresholds.
The 12divergence rule types, presentedinTable8.4, includethe basic
bullish divergence (type 6), the bearish divergence (type 7) and their in
versions (types 12 and 1). The 12 types are not illustrated because the il
lustrations would be redundant of those presented in the section on
Extremeand Transition Rules.
I-ar'ameler Combinations and Naming Convention fOl' Diver'
gence Ilnles. Each divergence rule is defined by four parameters:
type, companion series, threshold displacement, and channel normaliza
tion look-backspan. Thereare 12 types ofthe divergence rules (seeTable
8.4),38 companion dataseries, 2threshold displacement values-lO and
20, and 3 look-backspans-15, 30, and 60 days. This gives a total of2,736
divergence rules (12 x38x2x3)
The naming convention used for reporting results of divergence or
Drules in Chapter 9 is as follows: (D)-type-companion series-threshold
displacement-channel normalization look back span. Thus, a rule is
named
D-3-23-10-30
Divergence rule, type 3, companion series 23 (positive volume in
dex 3D-day moving average), threshold displacement = 10 (upper



==================================================
                     PAGE 456                     
==================================================

440 CASESTUDY: SIGNALRULES FORTHES&P SOD INDEX
'D\BLE 8.4 Divergence Rules and Associated Threshold Events
Divergence Rule Types Long Entry/Short Exit Short Entry/Long Exit
Down Cross UpperCross
LowerThreshold LowerThreshold
Down Cross UpperCross
2 LowerThreshold UpperThreshold
Down Cross Down Cross
3 LowerThreshold UpperThreshold
Up Cross Up Cross
4 LowerThreshold UpperThreshold
Up Cross Down Cross
5 LowerThreshold UpperThreshold
Up Cross Down Cross
6 UpperThreshold UpperThreshold
Up Cross Down Cross
7 LowerThreshold LowerThreshold
Up Cross Down Cross
8 UpperThreshold LowerThreshold
Down Cross Down Cross
9 UpperThreshold LowerThreshold
Up Cross Up Cross
10 UpperThreshold LowerThreshold
Down Cross Up Cross
11 UpperThreshold LowerThreshold
Down Cross Up Cross
12 UpperThreshold UpperThreshold
threshold = 60, lower threshold = 40), 30-day channel normalization
look-backspan.
This completes the description ofthe rules tested in the case study.
Results arepresentedinChapter9.



==================================================
                     PAGE 457                     
==================================================

Case Study
Results and the
Future of TA
PRESEN1f\TlON OF RESULTS
The primary objective ofthe case study was to demonstrate the applica
tion of two statistical inference methods suitable for the evaluation of
rulesdiscovered bydatamining. AsexplainedinChapter6, traditionalsig
nificance tests are notsuitable because they do nottake into account the
biasing effect ofdatamining. The two methods used were White's reality
check(WRC)and the Monte Carlopermutation (MCP).
Asecondaryobjective ofthe casestudywas thepossible discoveryof
rules withstatisticallysignificantreturns when applied to the S&P500 In
dex. Toward this end, a set of 6,402 rules described in Chapter 8 were
backtestedandevaluated.
With respect to the primary objective, the case study resoundingly
demonstrated the importance ofusingsignificance tests designed to cope
with data-mining bias. With respectto thesecond objective, no rules with
statistically significantreturns were found. Specifically, none ofthe 6,402
ruleshadaback-testedmeanreturnthatwas highenoughtowarranta re
jection of the null hypothesis, at a significance level of 0.05. In other
words, the evidence was insufficientto rejecta presumptionthat none of
the ruleshadpredictivepower.
The rule with the bestperformance, E-12-28-1O-30,1 generated a mean
annualized return of 10.25 percent, on detrended market data. In Figure
9.1,therule'sreturniscomparedto thesamplingdistributionproducedby
WRC. Thep-valueofthereturnis0.8164, farabovethe0.05levelsetasthe
significance threshold. Figure 9.1 makes it clear that the performance of
441



==================================================
                     PAGE 458                     
==================================================

442 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
0.10
0.09
R
E 0.08
L
0.07
0.06
F
0.05
R
E
0.04
Q
0.03
0.02
0.01
0
8% 10% 12% 14% 16%
Annualized Return
I<'IGURE 9.1 Sampling distribution for best of 6,402 worthless rules using
White's reality check.
the bestrule outofthe 6,402 examined falls well within the range ofordi
narysamplingvariability. Infact, itevenfalls belowthesamplingdistribu
tion's centralvalue of11 percent. Quite disappointing!2
The sampling distribution was generated by 1,999 replications ofthe
bootstrap procedure. Had a larger number ofreplications been used, the
distribution'sshape would have beensmoother, butthe conclusion would
have beenthesame: the rule'sperformancewas not high enough to reject
the null hypothesis.
Figure 9.2 shows E-12-28-10-30 plotted on the sampling distribution
producedbyMCP.Itproducedasimilarp-valueof0.8194.
Note that both sampling distributions are centered at approximately
an 11 percentmeanreturn. Thisparticularvaluewasaconsequenceofthe
specifics of the case study; the particular set of 6,402 rules, their return
correlations, the number ofobservations used to compute their mean re
turns, and the particularsetofS&P data. In otherwords, the expected re
turn for the best rule of 6,402 competing rules with no predictive power
under these specific conditions is approxinlately 11 percent. It is not 0
percent.
To recap a pointmade in Chapter6, the sampling distribution for the
mean return ofthe best-perfomling rule outofN rules (e.g., N = 6,402) is



==================================================
                     PAGE 459                     
==================================================

Case Study Results and the Future ofTA 443
0.125
0.10
R
E
L
0.075
F
R
0.05
E
Q
0.025
8% 10% 12% 14% 16% 18%
Annualized Return
FIGURE 9.2 Sampling distribution for best of 6,402 worthless rules using
Monte Carlo permutation.
notcenteredatzero. Rather, itissituatedatamore positivevalue thatre
flects the sampling distribution for a statistic defined as the maximum
meanofN means.
Figures 9.1 and 9.2 give an indication of what rate of return would
have been required to be statistically significant. Returns in excess of 15
percent would have been significant at the .05. Returns in excess of 17
percentwouldhavebeenhighlysignificant(p-value < .001).
Ironically, the failure ofany rule to generate statistically significant
returns, afteradjustmentfor data-miningbias, underscoresthe huge im
portance ofusing statistical inference methods that take the biasing ef
fects of data mining into consideration. Had I used an ordinary
significance test, whichpays no attention to data-mining bias, the mean
return of the best rule would have appeared to be highly significant
(a p-value of 0.0005). This is dramatically illustrated in Figure 9.3. It
shows the bootstrapped sampling distribution appropriate for a single
rule back test (data-mining bias ignored). In contrast to the previous
two Figures 9.1 and 9.2, the sampling distribution in Figure 9.3 is
centered at zero. The arrow represents the mean return ofthe bestrule
E-12-28-10-30.



==================================================
                     PAGE 460                     
==================================================

444
CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
0.09
0.08 \----Best Rule
Return: 10.25%
R 0.07 P-value: 0.0005
E
L
0.06
0.05
F
R 0.04
E
Q 0.03
0.02
0.01
0
-15% -10% -5% 0% 5% 10% 15%
FIGURE 9.3 Sampling distribution appropriate for asingle rule back test.
Had a conventional test of significance been used, about 320 of the
6,402 rules would have appeared to be significantat the 0.05level. This is
exactly what would be predicted to occur by chance. The naive data
minerusinga conventionaltestofsignificancewould haveconcluded that
many rules with predictive powerhad been discovered. In reality, mining
operations conducted in this fashion would have produced nothing but
fool's gold.
The full set of rule test results can be found on the web site
www.evidencebasedta.com. The results for the 100 rules with the high
est mean returns are presented in Table 9.1. The columns are: column
I-the rule's code according to the naming convention established in
Chapter 8, column 2-the rule's mean return on detrended S&P data
overthe back-testperiod, columns 3and 4-the single-rule p-values via
bootstrap and Monte Carlo (note these p-values do not take account of
the data-mining bias), columns 5, 6, and 7-significance at the 0.05
level, using three differentinference methodsinwhich data-miningbias
is taken into account. Had any of the rules been significant, it would
have been indicated by an asterisk. None were significant, hence there
are no asterisks. Columns 8 and 9 are the lower and upper 80 percent
confidence intervals.



==================================================
                     PAGE 461                     
==================================================

'li\IU,E 9.1 The Best Performing 100 Rules
Rule 10 Return SR Boot SR MC B RC MC Lower80% Upper80%
p-value p-value Sig Sig Sig CI CI
E-12-28-10-30 0.102501 0.0015 0.002 -0.02932 0.23527
0-8-4-I0-60 0.102395 0.001 0.00\5 -0.02943 0.23516
E-I-28-10-15 0.100562 0.0025 0.0005 -0.03126 0.23333
E-12-21-20-15 0.098838 0.0005 0.0015 -0.03299 0.23161
E-12-28-10-15 0.097502 0.001 0.0025 -0.03432 0.23027
E-l2-24-20-15 0.095904 0.0005 0.001 -0.03592 0.22867
E-II-28-10-15 0.094635 0.003 0.002 -0.03719 0.2274
E-I 1-39-10-60 0.092282 0.001 0.0015 -0.03954 0.22505
0-7-37-10-30 0.092078 0.0025 0.003 -0.03975 0.22485
0-7-37-10-15 0.090196 0.0045 0.004 -0.04163 0.22296
TI-37-5 0.089024 0.0045 0.003 -0.0428 0.22179
TI-20-41 0.08862 0.0045 0.0045 -0.0432 0.22139
E-I-22-10-60 0.088591 0.0055 0.007 -0.04323 0.22136
TI-21-8 0.086963 0.0035 0.0075 -0.04486 0.21973
0-10-28-10-30 0.086939 0.004 0.0025 -0.04489 0.21971
0-11-28-10-15 0.085266 0.0055 0.0055 -0.04656 0.21804
0-7-38-10-30 0.085206 0.005 0.006 -0.04662 0.21798
TI-28-12 0.084912 0.0035 0.0035 -0.04691 0.21768
TI-24-8 0.084021 0.007 0.005 -0.0478 0.21679
E-2-28-10-15 0.083267 0.0075 0.0025 -0.04856 0.21604
E-5-36-10-30 0.083087 0.0055 0.0065 -0.04874 0.21586
0-9-34-10-30 0.081577 0.008 0.009 -0.05025 0.21435
E-l 1-15-20-60 0.08058 0.007 0.0075 -0.05124 0.21335
0-1-21-10-30 0.080499 0.008 0.008 -0.05132 0.21327
0-8-32-10-15 0.08048 0.006 0.0085 -0.05134 0.21325
0-7-36-10-30 0.079943 0.0065 0.0055 -0.05188 0.21271
0-8-27-10-30 0.079601 0.007 0.0085 -0.05222 0.21237
E-I2-22-10-30 0.077796 0.009 0.008 -0.05403 0.21057
0-1-24-10-30 0.077582 0.01 0.0125 -0.05424 0.21035
0-7-36-20-30 0.077312 0.0095 0.0135 -0.05451 0.21008
0-8-33-20-15 0.077101 0.0095 0.0085 -0.05472 0.20987
E-I-I9-I0-60 0.077092 0.0075 0.0125 -0.05473 0.20986
0-8-32-20-15 0.076434 0.0105 0.009 -0.05539 0.2092
TI-8-18 0.076144 0.009 0.01 -0.05568 0.20891
TI-26-137 0.075767 0.009 0.0105 -0.05606 0.20854
TI-24-5 0.075732 0.008 0.009 -0.05609 0.2085
0-7-36-10-60 0.0755 0.0105 0.0105 -0.05632 0.20827
E-I-28-20-15 0.07541 0.013 0.0135 -0.05641 0.20818
E-6-38-20-30 0.074831 0.0085 0.0125 -0.05699 0.2076
E-12-24-10-15 0.074809 0.0115 0.0135 -0.05701 0.20758
TI-37-3 0.074756 0.0125 0.01 -0.05707 0.20753
TI-38-5 0.074579 0.0135 0.012 -0.05724 0.20735
(Continued)
445



==================================================
                     PAGE 462                     
==================================================

446 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
'mBLE 9.1 (Continued)
E-11-21-10-15 0.074168 0.0145 0.011 -0.05766 0.20694
0-9-4-10-60 0.07409 0.0125 0.0125 -0.05773 0.20686
E-1-25-10-60 0.073966 0.0155 0.01 -0.05786 0.20674
E-1-18-20-60 0.073439 0.015 0.0155 -0.05838 0.20621
E-II-25-10-15 0.073131 0.01 0.018 -0.05869 0.2059
0-10-32-20-15 0.072548 0.0205 0.0145 -0.05928 0.20532
E-5-38-10-30 0.07245 0.014 0.014 -0.05937 0.20522
0-11-28-10-30 0.07245 0.0185 0.013 -0.05937 0.20522
E-ll-15-20-15 0.072363 0.012 0.018 -0.05946 0.20513
E-2-24-20-15 0.072286 0.018 0.011 -0.05954 0.20505
0-9-34-10-60 0.072065 0.012 0.016 -0.05976 0.20483
0-7-11-10-15 0.071859 0.0135 0.0115 -0.05996 0.20463
0-8-37-20-60 0.071562 0.0205 0.022 -0.06026 0.20433
E-2-21-10-15 0.071546 0.0175 0.015 -0.06028 0.20432
E-6-37-20-15 0.071509 0.016 0.0135 -0.06031 0.20428
TI-12-12 0.071403 0.0135 0.0095 -0.06042 0.20417
E-12-21-10-15 0.07114 0.016 0.0195 -0.06068 0.20391
TI-23-5 0.071013 0.0155 0.0115 -0.06081 0.20378
E-2-1-20-30 0.070897 0.016 0.013 -0.06093 0.20367
TT-36-5 0.070709 0.0235 0.017 -0.06112 0.20348
E-6-38-20-15 0.070681 0.017 0.021 -0.06114 0.20345
TI-21-12 0.070672 0.017 0.0105 -0.06115 0.20344
E-1-21-10-15 0.070672 0.02 0.016 -0.06115 0.20344
0-8-38-20-30 0.070308 0.02 0.0225 -0.06152 0.20308
0-10-4-10-60 0.070298 0.0145 0.016 -0.06153 0.20307
TI-32-61 0.070203 0.0215 0.0135 -0.06162 0.20297
E-2-19-10-60 0.070192 0.015 0.016 -0.06163 0.20296
TI-18-8 0.069985 0.019 0.0135 -0.06184 0.20275
TT-36-3 0.069962 0.0185 0.0195 -0.06186 0.20273
0-8-23-20-60 0.069884 0.0175 0.019 -0.06194 0.20265
0-7-16-10-30 0.069749 0.013 0.0145 -0.06207 0.20252
0-7-34-10-60 0.069721 0.016 0.0235 -0.0621 0.20249
E-2-28-20-5 0.069572 0.017 0.02 -0.06225 0.20234
0-9-37-10-30 0.069551 0.0235 0.0195 -0.06227 0.20232
0-9-29-10-60 0.069538 0.013 0.0185 -0.06229 0.20231
0-9-36-20-30 0.069506 0.0225 0.018 -0.06232 0.20228
0-8-36-10-60 0.068993 0.018 0.022 -0.06283 0.20176
0-8-33-10-15 0.068802 0.025 0.023 -0.06302 0.20157
0-10-23-20-60 0.068752 0.0145 0.02 -0.06307 0.20152
0-7-37-20-30 0.068628 0.02 0.021 -0.0632 0.2014
E-2-1-10-30 0.068546 0.016 0.018 -0.06328 0.20132
0-8-31-10-30 0.06831 0.019 0.023 -0.06351 0.20108
0-9-32-10-15 0.068112 0.0175 0.022 -0.06371 0.20088
E-5-20-10-30 0.068035 0.0185 0.011 -0.06379 0.2008



==================================================
                     PAGE 463                     
==================================================

Case Study Results and the Future ofTA 447
1l\BLE9.. (Continued)
E-3-36-10-30 0.067922 0.0205 0.019 -0.0639 0.20069
TI-23-41 0.067856 0.0225 0.019 -0.06397 0.20063
E-2-9-10-15 0.067833 0.0185 0.0215 -0.06399 0.2006
E-7-36-20-60 0.067749 0.0255 0.0205 -0.06408 0.20052
TI-21-5 0.067723 0.022 0.0205 -0.0641 0.20049
E-6-38-10-30 0.067453 0.0225 0.024 -0.06437 0.20022
0-6-33-10-15 0.067088 0.023 0.022 -0.06474 0.19986
0-8-34-20-60 0.066988 0.025 0.023 -0.06484 0.19976
E-12-21-20-60 0.066859 0.0175 0.025 -0.06496 0.19963
E-1-18-I0-60 0.066816 0.027 0.0235 -0.0650I 0.19959
0-5-15-10-30 0.066604 0.024 0.017 -0.06522 0.19937
The three inference methods that take data-mining bias into account
wereused:
t. B (boot) is an improved version ofWRC, which incorporates the en
hancement suggested by Wolfand Roman03 designed to improve the
powerofthetest(reducethe chanceofatype-IIerror) asdiscussedin
Chapter6.
2. RC (reality check) is the version of WRC that is currently available
from Quantmetrics,whichdoesnotincorporatethe WolfandRomano
enhancement.
3. MC is the Monte Carlo permutation method, developed by Masters,
whichincorporatesthe Wolfand Romano enhancement.
The upper and lower bounds ofthe 80 percent confidence intervals
were derived on the basis of the mathematical theory first proposed by
White,4 using an algorithm that is described inWolfand Romano. Dr. Tim
Masters implemented the computer code for this. The confidence inter
vals presented take full account of the data-mining bias. That is to say,
theyarejointconfidenceintervalsforwhichupperandlowerboundscon
tainthe true return ofall rules, witha probabilityof0.80. This means that
ifwe wereto redothe casestudyon 1,000independentsetsofdata, an ob
vious impracticalitygiven the existence ofonly one set ofhistorical mar
ket data, in 800 ofthe tests the 6,402 confidence intervals would contain
theexpectedreturnsfor allrules.
Itcanalso be saidthat, in950 ofthese cases, we would notbe fooled
into an erroneous rejection ofthe null hypothesis, but in 50 we would. It
so happens that in the particular experiment run for the case study, no



==================================================
                     PAGE 464                     
==================================================

448 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
ruleshowedup assignificant. However, bysettingthesignificancelevelto
.05, the case studydid infacthave a 1in20 chance ofatype-1 error-that
a rulewithno actualpredictivepowerwouldhavedisplayedap-valueless
than0.05, therebyresulting inafalse rejectionofthe nullhypothesis.
CRITIOUE OF CASE STUDY
Positive Attributes
Use ofBenchmarks in Rule Evaluation. One thing the case study
did right was using a benchmark to evaluate rule performance. As
pointed out in Chapter 1, the back-tested performance of a rule only
makes sense in relation to a benchmark. Absolute levels of perfor
mance are uninformative.
The case study used the lowest reasonable benchmark, the perfor
mance ofa rule with no predictive power. Under this standard, a rule is
deemed effectiveifandonlyifitbeatstheperformance ofa nonpredictive
rule by a statistically significant margin. Clearly, there are higher bench
marks that could make sense in specific cases. For example, suppose a
claim is made that a superior version of the double moving average
crossover rule has been developed. Asensible benchmark would be the
performance ofa conventionalversionofthe rule.
Controlled for the Effect of Market 'I'rends. As described in
Chapter 1,the case study used detrended marketdatato computerule re
turns to eliminate performance distortions. As explained, distortions re
sult when a rule with a long- or short-position bias has its returns
computed onmarketdatathathas a net upward ordownward trend over
the back-testperiod.
Controlled for Look-Ahead Bias. Rule studies that assume the
availabilityofinformationthatis notyettrulyavailableatthetime market
positions are entered or exited are infected with look-ahead bias, also
known as future information leakage. Forexample, ifclosing price infor
mationis neededto compute a rule's signals, itwould notbe legitimateto
assume an entry or exit at the closing. The first legitimate price at which
anentryorexitcouldlegitimatelybeassumedisthenextprice.Ifthe data
is ofa daily frequency, as was true for the case study, the first legitimate
execution price does not occur until the market opens the day following
thesignal.
Look-ahead bias can also occur ifa rule uses data series that are re
ported with a lag, such as mutual fund cash statistics, or that are subject



==================================================
                     PAGE 465                     
==================================================

Case Study Resultsand the Future ofTA 449
to revisions, such as government economic statistics. When this is the
case, lagged values must be used that take into account reporting delays
orrevisions.
Thecasestudyavoidedlook-aheadbiasbyassumingentriesand exits
on the open price of the day following a position-reversal signal. More
over, noneofdataseriesusedweresubjectto reportinglagsorrevisions.
Contl'olled fol' Data-Mining Bias. Few rule studies in popular TA
applysignificance tests ofany sort. Thus, they do not address the possi
bility that rule profits may be due to ordinary sampling error. This is a
seriousomission, whichis easily corrected byapplyingordinaryhypoth
esistests.
However, ordinary tests of significance are only appropriate when
only one rule has been back tested. When many rules have been tested
and a bestisselected, the ordinaryhypothesistestwillmakethe bestrule
appearmore statisticallysignificantthan itreally is (false rejection ofthe
null hypothesis). Avoiding such type-I errors is the motivation for using
advanced hypothesis tests such as WRC and Monte Carlo permutation
method, as described in Chapter 6. The case study used such tests. As
pointedoutearlier, bothWRCand MCPMrevealedthatthebest-performing
rule ofthe6,402 tested wasnotstatisticallysignificant.
Contl'olled fol' nata-Snooping Bias. Data-snooping bias, which
might be more properly named prior-research-snooping bias, occurs
when dataminers use the results ofpriorresearch to choose which rules
to test. In otherwords, they use rules that have been previouslyfound to
be successful. This is an insidious problem because it is unknown how
many rules were tested to find the successful rules. Because the number
ofrules testedis an importantfactor contributingto the magnitude ofthe
data-mining bias it is impossible to evaluate the actual statistical signifi
cance ofa rule thatwas includedina new data-miningventure because it
had beensuccessfulinpriorresearch efforts.
The case study mitigated the prior-research-snooping bias by not ex
plicitly including any rules that were discovered by other researchers.
Though many of the rules examined with the study's 6,402 rules were
probably similar to those examined by other researchers, the similarity
was coincidental. As explained in Chapter 8, the set ofrules tested were
arrived at by combinatorial enumeration ofall rules possible within a set
ofspecifiedparametervalues. Forexample, trend rules were based on 11
possiblevaluesforthechannelbreakoutlook-backparameterand39pos
sibletimeseries.All429possiblerules(11 x39) wereincludedinthe case
study. Ifone ofthese happened to bea rule discoveredbysome priorrule
research, itsinclusionwasbyhappenstance andnota perusaloftheprior



==================================================
                     PAGE 466                     
==================================================

450 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
research. Thus, the case study accounted for the full number ofrules ex
aminedleadingto the discoveryofthe best-perfornlingrule.
Negative Attributes
Complex Rules Were Not Considel'cd. Perhaps the biggest defi
ciency ofthe case studywas the failure to consider complex rules. Com
plex rules result from combining and condensing, in some fashion, the
informationcontained ina multiplicityofsimplerules. Thisisaseverede
fect for several reasons. First, few TA practitioners, subjective or objec
tive, restrict themselves to decisions based on one rule. In this sense, the
casestudydid notreplicate whatsubjective orobjective analysts actually
do. However, as commented on later in this chapter, subjective analysts
are severely constrained in theirability to properly interpretthe informa
tionpatternembodiedina multiple rule configuration.
Second, complexrulesshouldproduce superiorperformanceondiffi
cultpredictionproblems, suchasfinancial markets. Anonlinearcombina
tion ofsimple rules allows the complex rule to be more informative than
the summed information contained in its individual constituents. This al
lowstheruleto complywithAshby's LawofRequisiteVariety,5whichstip
ulates that a problem and its solution must have similar degrees of
complexity. In the context ofprediction, this in1plies that a model (rule)
intended to predict the behavior of a complex system (e.g., a financial
market) must also be complex. The general superiority ofcomplex rules
even of a linear form, was demonstrated in an analysisGof 39,832 rules,
both simple and complex. This rule set was tested on four stock market
indexes: DowJones, S&P 500, NASDAQ, and Russell 2000overthe period
1990through 2002. Oftheentire ruleset, 3,180or8percentwere complex
rules, but of the 229 rules that generated statistically significant profits,
188, or 82 percent, were complex rules. This study took account of the
data-nUning bias by using WRC to compute significance levels. Inciden
tally, the studyshowedthatnone ofthe rulesproducedstatisticallysignif
icantgains oneitherthe DowJonesIndustrialsorthe S&P500Index.
The case study was restricted to simple rules to keep its scope man
ageable. However,Ididsointhebeliefthatatleastafew ofthe6,402 rules
wouldprovesignificant. Clearly, Iwas overconfident.
Only Long/Short Rcvel'sal Rules Considcred. Also in the interest
ofkeeping the case study manageable, the rule set was restricted to one
type ofbinary rule; long/short reversal. As discussed in Chapter 1, this re
striction can distort the TA concept a rule is intended to express. This is
anacknowledgedlimitationofbinaryrulesingeneraland oflong/shortre
versal rules in particular. The requirement thata rule always hold a mar-



==================================================
                     PAGE 467                     
==================================================

Case Study Results and the Future ofTA 451
ketposition, which istrue ofreversal rules, rests onthe unlikely assump
tion thatthe marketis in a perpetual state ofinefficiency and continually
presentsprofit opportunities. In contrast, rules that are more selective in
identifying when market exposure is warranted are consistent with the
more reasonable assumption that markets are occasionally inefficient.
Therefore, tri-state rules, which would allow for long/shorUneutral posi
tions, or binary rules, which are long/neutral or shorUneutral, may be su
perior. Nosuchrules were examinedbythe casestudy.
Rules Limited to S&P 500 'Irading. Alsointhe interestoflimiting
the scope of the case study, rules were applied to only one market, the
S&P 500 Index. The large rule study referred to earlier did not find any
7
rules, either simple or complex, which were effective on the S&P 500.
When I selected S&P 500 as the market to trade, I was unaware of the
largerrule study, which didfind useful rules for the NASDAQ and Russell
2000. HadIknownofthestudyand decidedtouseeitherNASDAQorRus
sell2000 as the target market, Iwould have been guilty ofsnooping prior
research, therebycompromisingthe validity ofthe p-values that were ob
tained inthe case study. On the otherhand, had Ichosenthe NASDAQ or
Russell 2000without knowledge ofthatstudyand found successful rules,
they would have been discovered legitimately and the reported p-values
wouldhave beenaccurate.
POSSIBLE CASE STUDYEXTENSIONS
Theaforementionedlimitationssuggestseveralpossible extensionstothe
casestudy.
Application to Less Seasoned Stock Indexes
The Hsu and Kuan study referred to earlier that found statistically sig
nificant rulesfor the NASDAQ and Russell 2000 suggested thatTA rules
may be more successful on indexes ofless seasoned stocks. Thus, one
extensionwould beto apply the case study'srule settostockmarketin
dexes of other countries with securities of less-seasoned companies.
This would be practical if the raw data series needed to construct the
indicators were available, such as advance/decline statistics, up and
downvolume, new highs and lows, and such. Aliterature searchuncov
ered a number ofstudies8 that lend credibility to the notion that newer
stock markets may be more amenable to prediction than more devel
oped and presumablymore efficientmarkets. Onepracticallimitationof



==================================================
                     PAGE 468                     
==================================================

452 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
a newerstock marketwould be shortermarkethistoryand fewer histor
ical observations.
Improved Indicator and Rule Specification
The profitability of a rule is determined by the information content of
the indicators on which the rule is based. The more effectively an indi
cator quantifies an informative feature of market behavior the higher
the rule's profitpotential. The 6,402 rules used in the case study consid
ered three TA themes: trends, extremes, and divergences. The indica
tors used were thought to be reasonable ways to quantify these themes.
No doubt there is room for improvement. As pointed out in a subse
quentsection, the mostimportantfuture role ofthe TApractitionerwill
be developing indicators thatquantify market behavior in a more infor
mative manner.
For example, as mentioned in Chapter 8, divergences might be bet
ter measured with indicators based on conintegration.9 Indicators in
tended to measure extremes might prove more useful if they employed
the Fisher transform, discussed by Ehlers. He points out that the fre
10
quency distribution ofan indicator based on channel normalization has
an undesirable property, specifically, extreme values are considerably
more likely than values in the middle of the indicator's range. This is
hardly the behavior one would want in an indicator designed to detect
rare extremes. However, by applying the Fisher transform to channel
normalized data, Ehlers claims that the resultant indicator is close to
normallydistributed. An indicatorwitha normal distribution has the de
sirable characteristic that its extreme values are rare. This may, in turn,
make them more informative. Because Ehlers offers no datasupporting
evidenceofthe Fishertransform'sefficacy, atthis time itisonlyaninter
esting conjectureworthy offurther investigation.
Consideration ofComplex Rules
Complex rules are derived by combining the information in two or more
simple rules in orderto augment predictive power. There are two key is
sues in deriving complex rules: Which rules should be combined? and
How should they be combined? The how refers to the mathematical or
logical form used to combine the simple rules' outputs to produce the
complexrule's output.
The simplest method of combining is linear. Alinear combination is
based on summingrule the outputs ofsimple rules. It treats the outputof
each simple rule as ifitmakes an independentcontributi.on to the output
ofthe complexrule. Thatis to say, the contribution ofeachsimple rule is



==================================================
                     PAGE 469                     
==================================================

Case Study Results and the Future ofTA 453
unaffected by the output ofany other rule. The simplest linear combina
tion gives thesameweighttoeachconstituent. Thus, asimplelinearcom
bination of two binary reversal rules would assume a value of-2, when
both are short; zero, when one isshortand one islong; and +2 when both
rules are long. Amore sophisticated, though notnecessarilysuperior, lin
earcombinationassigns varying weights to eachrule, depending upon its
predictive power. The relative weightofeach rule is represented by a co
efficient, designatedbythe lettera. Thesetofweights-whicharebestac
cording to some figure of merit, such as minimum predictive error or
maximumfinancialperfonnance-canbefound byoptimization. Thegen
eral form ofa weighted linear combination is illustrated in the following
equation:
Linear Combining Is Additive
Where:
Yis Output of complex linear rule
q is the output of ith rule
ai is the weight for the
ith
rule
ao is a constant - y intercept
In contrast to a linear combination, a nonlinear combination consid
ersinteractionsbetweenthe rules. Inotherwords, the combinedvalue, Y,
is notsimply a weighted sum ofthe individual rule values. In a nonlinear
combination, the contribution ofany individual rule to the combined out
put Y depends, in some way, on the outputvalues ofthe otherrules. In a
linear combination, each rule's contribution to Y is unaffected by (inde
pendentof)thevalueofanyotherrule. Theadvantageofa nonlinearcom
bination is that it can express more complex infonnation patterns.This
createsa disadvantage, however, in thata nonlinearcombinationmay not
be expressible as a neat easily understood equation. Because we are left
in the dark about how the rule outputs are being combined (i.e., interact
ing), nonlinear models are sometimes referred to as black-box fonnulas.
Nonlinear combinations produced by neural network software would be
anexampleofablack-boxfonnula.
Thedistinction betweenlinearand nonlinearcombiningis moreeas
ily portrayed when one considers continuous variables rather than bi
nary rule outputs. Continuous variables can assume any value within a



==================================================
                     PAGE 470                     
==================================================

454 CASESTUDY: SIGNALRULES FORTHES&P SOO INDEX
range rather than being restricted to discrete values such as those ofbi
nary rule (-1 and +1). When a linear model is illustrated graphically, as
in Figure 9.4, the response surface, which represents the complex rule's
output (Y), assumes a flat shape. There are no hills and valleys. Another
way to say this is that the slope ofthe response surface, with respect to
each input (Xl andX ), remains constant throughout the entire range of
2
theinput. TheslopeofthesurfacewithrespecttoinputXl isdesignatedby
the coefficienta and the slope with respectto inputX isgivenbycoef
p 2
ficienta .Thevaluesa anda are typicallydiscoveredfrom asetofdata
2 j 2
bya statisticaltechnique called regressionanalysis. The valuesat and a
2
in combination with a third coefficient a o' called the Y intercept, orient
the plane so as to best slice through the set ofdata points used to esti
mate the model's coefficients. The fact that the slope of the response
surface is constant (flat) tells us thatthere are no nonlinearinteractions
between the inputs. Said differently, the inputs XI and X are interacting
2
inan additive manner.
Ina nonlinearmodel, nonadditiveinteractionsbetweenthe inputsare
allowed. This means that the value Yis notsimply a weighted sum ofthe
individual input values. In a nonlinear model, the contribution ofany in
put (indicator) depends, insome arbitraryway, on the values ofthe other
inputs. This manifests as a response surface with hills and valleys. In
otherwords, theslopeofthemodel'sresponsesurfacewithrespectto any
individual input, Xi, will not be constantthroughout its range. Thisvaria
tion in the slope overthe range ofthe inputs can produce a response sur
face likethe oneshowninFigure9.5.



==================================================
                     PAGE 471                     
==================================================

Case Study Results and the Future ofTA 455
y
Lincar COlllbinations ",Uhin a Thclllc. Despite the simplicity of
combining rules via addition, complex rules with a linear form have
proven useful. The study conducted by Hsu and Kuanll tried three meth
ods for creating complexrules. Two ofthe methods were based on linear
forms: voting rules and fractional position rules. The third type, which
they referred to as a learning rule, did not combine rules, but rather was
based on selecting the simple rule from a set ofsimilarrules that had the
best recent performance. The complex rules based on voting and frac
tional positions were essentiallyadditive combinations ofall simple rules
within a given theme. Examples of themes would be all rules based on
channel breakouts or all rules based on a moving average of on-balance
volume. The output ofa complex rule based on a voting scheme was de
rived by polling all rules within a theme (channel breakout) and taking a
one-unitposition thatreflected the majorityvote. For example, ifthe ma
jority of rules were indicating a short position, a one-unit short position
was taken. The output ofa complex rule based on the fractional-position
concept was also based on polling all the simple rules within a theme,
however, the size of the position taken was proportional to the net per
centage of rules that were long or short ([number long minus number
short) divided bytotal numberofrules). Forexample, Hsu and Kuan con
sidered 2,040 on-balancevolume rules. If1,158were signaling a shortpo
sition and 882 were signaling a long position, a 0.135 unit short position
would betaken([882- 1,158)/2,040= -0 .135).
Thusonelogicalextensionofthe casestudywould beto create linear
complex rules based onvoting orfractional-position schemes. This could
be done for rules within a given theme, for example all divergence rules.



==================================================
                     PAGE 472                     
==================================================

456 CASESTUDY: SIGNAL RULES FORTHES&P sao INDEX
An indicator commonly used in TA, called a diffusion indicator, is based
on a similar idea. An example ofa well known diffusion indicator is the
percentage of New York Stock Exchange stocks that are over their 200
day moving averages. In this instance the percentage is essentiallyan un
weightedlinearcombination.
The diffusion indicator would simply be the net percentage ofdiver
gence rules that were long or short. Using a voting scheme, ifmore than
50percentofthe divergence indicatorswere ina +1state, a long position
would beheld, elsea shortposition. Usingafractional-position scheme, a
partial long or short position would be held and adjusted as the net
long/shortpercentage changed. Itisimportantto note thatthe creation of
diffusion indicators from the 6,402 rules used in the case studywould re
quirethatinverserulesbeeliminated. Theirinclusionwouldcauseadiffu
sion indicator to always assume a value of zero. Thus, a diffusion
indicator based on the trend-rule category would be constructed only
from TT rules and TI rules would be excluded. Likewise, a diffusion indi
cator based on rules in the extreme and transition category would be
based only on types 1, 2, 3, 4, 5, and 6. Types 7 through 12 would have to
be excludedinasmuchastheyaretheinverseoftypes 1through 6.
Machine-Induced Nonlinear Combinations. Complex rules that
combine simple rules in a nonlinear ornonadditive fashion could also be
derivedfromthecasestudy'srules. However, thiswouldrequire theuse of
autonomous machine learning (data-mining) systems such as neural net
works,12 decision trees and decision tree ensembles, multiple regression
splines,13 kernel regression,14 polynomial networks,15 genetic program
ming,16 support-vector machines,17 and so on. These systems employ in
ductive generalization to synthesize a complex nonlinear rule (model) by
learningfrom a largesetofhistorical examples. Eachexample isaa case,
characterized by a set ofindicatorvalues as ofa given date and the value
ofanoutcomevariablethatthe modelisintendedto predict.
Complexnonlinearrules could bederived intwo ways. Onewould be
to submit the rules in binary form, as the candidate inputs. Asecond ap
proachwould be tosubmitthe indicatorsusedbythe rules.
To the best ofmy knowledge, there is no product currently available
for generating complex nonlinear rules that incorporates a significance
test that is robust to the data-mining-bias problem. In the absence ofthis
protection, data miners searching for complex nonlinear rules must rely
on the protection provided bya sophisticated form ofout-of-sample test
ing that involvesthree datasets (training, testing, and evaluation). Thisis
describedlaterinthis chapter.
Allowing the search to optimize a rule's complexity, rather than re
stricting it to finding the optimal parametervalues for a rule whose com-



==================================================
                     PAGE 473                     
==================================================

Case Study Results and the Future ofTA 457
plexityisfixed, hasanupside anda downside. The upside isthatallowing
complexity optimization increases the chance of discovering a superior
rule. The downside is that the added search dimension can greatly in
crease the number ofrules considered before a best is selected. This in
creases the chance that the best-performing rule is overfitted. This refers
to a rule that not only expresses the sample data's valid patterns butthat
also describes its random effects ornoise. Overfitted rules do not gener
alize well. That is to say, their out-of-sample performance is typically
poor relative to their in-sample performance. This is an example ofdata
mining bias.
To guard against the risk ofoverfittingwhen complexityoptimization
is allowed, three datasegments are oftenused: training, testing, andval
idation. Recall from Chapter 6that only two data segments are required
whendataminingisrestrictedtofindingtheoptimalparametervaluesofa
fixed-complexity!8 rule: trainingand testing.
In its search for the nonlinear rule of optimal complexity, machine
learning algorithms cycle through the training and testing sets in two dis
tinctloops. The innerloop searchesfor the optimalparameters ata given
level ofcomplexity. The outerloop searches for the optimal level ofcom
plexity. Once the best complex rule has been found, it is evaluated in the
third data segment, called the validation set. This data has been held in
reservependingthe discoveryoftheparametervaluesand complexityde
gree of the best-performing rule. Because the validation set was not uti
lized in this process ofdiscovery, the performance ofthe bestrule in this
dataset is an unbiased estimate ofits future performance. This is shown
in Figure9.6.
This three-segment scheme can be conducted on a walk-forward ba
sis.Afterthebestrulehasbeenfoundinthefirst datafold (train!test/vali
dation), the tripart data window slides forward and the entire process is
repeatedin the nextfold. ThisisillustratedinFigure 9.7.
The motivationfor the tripartdatawindow warrants some additional
explanation. Marketbehaviorispresumedto be a combinationofsystem
atic behavior(recurringpatterns) and random noise. Itis alwayspossible
to improve the fit of a rule to a given segment of data by increasing its
complexity. Inotherwords, givenenoughcomplexity, itisalwayspossible
to fashion a rule that buys at every market low point and sells at every
market high point. This is a bad idea.19 Perfect timing on past data can
only be the result of a rule that is contaminated with noise. In other
words, perfect signals or anything approaching them almost certainly
means the rule is, to a disturbingdegree, a descriptionofpastrandom be
havior(Le., overfitted).
Overfitting manifests when the rule is applied to the test data seg
ment. There, its performancewill be worse than inthe training data. This



==================================================
                     PAGE 474                     
==================================================

458 CASESTUDY: SIGNAL RULES FORTHES&P 500 INDEX
Foldi
(-----~-----\
Complexity
Search U~
Loop ~ Best
~ ~Ule
Parameter
Search
Loop
LJ'-----_I
r--
fl-------!raining Set . Test Set -- V- al- ida- tio- n. SL et ....
~
Optimal Rule's
Performance
<::sJ7 Evaluated Here
f-----------Time-----------....
F'IGURE 9.6 Searching for optimal parameters and complexityofbest rule.
Fold4
(-----~-----\
r
-----1
~
Time Training Set Test Set Validation
Fold3
(-----~-----"
------1
I---.~
Training Set Test Set Validation
Foldz
(-----~-----\
~---ll II------.~
Training Set Test Set Validation
Fold]
(-----~-----\
I--------.....
~ ~
Training Set Test Set Validation
FIGURE 9.7 Walk-forward complexity search.



==================================================
                     PAGE 475                     
==================================================

Case Study Results and the Future ofTA 459
is because the legitimate patterns found in a training setrecur in the test
set, butthe noise in the training setdoes not. Itcan be inferred thatprof
itabilityinthe trainingsetthatdoes notrepeatin the testingsetwas most
likelyaconsequence ofoverfitting.
During the process ofcomplexity optimization, the first sign that the
rule has become overfit is when its performance reaches a peak in the
testing set and then starts to degrade. Complexity optimization proceeds
as follows. It begins with a low complexity rule, which is run repeatedly
overthe training datasetusing differentparametervalues. Each run pro
duces a performance figure (e.g., rate of return). The parameter(s) that
yields the highestperformancein the training data, let'scallitbest-rule 1,
is then run onthe testdatasetand itstestsetperformance is noted. That
completes the first complexity loop. The second complexity loop begins
with arule thatisa notch higherincomplexitythan best-rule 1. Thatisto
say, ithasa greaternumberofparameters. Again, there isa searchforthe
best parametervalues ofthis more complex rule in the training set. The
best rule at this complexity level, let's call it best-rule 2, will almost cer
tainlyperform betterin the trainingsetthan best-rule 1because ofits ad
ditionalcomplexity. Best-rule2'sperformanceisthenmeasuredinthe test
set. This process continues as a succession of rules of increasing com
plexity are mined from the training set and evaluated in the testing set.
Typically, performance onthe testimproves as the rule's complexityisin
creased. The improvement is an indication that ever-more-intricate but
valid patternsin the trainingsethave been discovered. However, ata cer
tain pointin the rule's complexitygrowth, the performance in the testing
setwill start to decline. This indicates that the most recent increment in
complexity has started to describe the random effects (nonrecurring
noise) inthe trainingset. Inotherwords the rule hasnowbecome overfit
ted. Hadthe rule'scomplexitygrowthbeenstoppedpriortothispoint, the
rule would have failed to capture all the data's valid patterns. Such a rule
is said to be underfitted. The phenomenon ofunderfitting and overfitting
is illustrated in Figure 9.8. Note that increased complexity always pro
duces improved performance in the training set, which is shown by the
continualgrowthintrainingsetperformance.Alsonotethattheboundary
between underfitting and overfitting is defined by the point at which the
testsetperformancepeaksand beginsto decline.
Theconceptofcomplexitysearchisabstractandmaybenewtosome
readers. In the interestofclarification, considerfollowing the specific ex
ample ofa human guided rather than a machine guided complexity opti
mization. We begin with a basic two moving-average crossover reversal
rule.20Theruleisdefinedbytwoparameters,theshort-termmoving-average
look-back span and the long-term moving-average look-back span. Ini
tially, all possible combinations of the parameters for the two averages



==================================================
                     PAGE 476                     
==================================================

460
CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
Rule Under Fit :.--- Rule Over Fit
.--- Too Simple -------.: Too Complex
High ,
Training Set ,
Performance
Test Set
~
Rule """_----9------ ~e7ance
Performance
Optimal --
Complexity
(Properdegreeoffit)
Low
Low High
Rule Complexity
FIGURE 9.8 Optimizing rule complexity.
would be run onthe trainingset. Ifeachmoving average were allowed 10
possible parametervalues, a brute-force search wouldinvolve 100tests.21
The parameter combination with the highest performance in the training
set(Le., the bestrule) would thenberun onthe testdataset. Assume that
the best rule earned a 30 percent annualized return in the training data
anda 15percentreturninthe testdata. Sofar, thisisnothingbutordinary
parameteroptimization, andbecause the testdatahas beenused onlyone
time, the 15percentreturnisanunbiasedestimateofthe rule'sfuture per
formance.
Now, suppose that we are not satisfied with a 15 percent annualized
return, sowe startto datamine a broaderuniverse thatincludescomplex
rulesthat build onthe simpletwo moving-average crossoverrule.22 Wego
backto the drawing board and propose a more complex rule by addinga
rule based on an Wilder's Relative Strength Index (RSI), an indicatorthat
measurestherate ofpricechange. Inthisexample, theadditionofthe RSI
rule alters the original dual moving-average crossover rule from one that
always reverses from long to short or short to long to a rule that can re
mainoutofthe marketaltogether. Onewayto do this would betouse the
RSI indicator as a filter: Ifthe RSI indicator is greater than a threshold,
rsi_max, at the time of a sell signal by the basic dual-moving-average
crossoverrule, a neutral positionwould be taken instead ofa short posi
tion. Conversely, and ifthe RSI were less than the threshold, rsLmin, at
the time ofa dual-moving-average buysignal, a neutralposition instead of



==================================================
                     PAGE 477                     
==================================================

Case Study Results and the Future ofTA 461
a long position would be taken. This more complex rule has four parame
tersthatcanbeoptimized:shortmoving-averagespan,longmoving-average
span, rsLmax, and rsLmin. A large number of parameter combinations
arethenrunagainstthetrainingdata.
As expected, theincreased complexityallows thefour-parameterrule
toearnahigherreturninthetrainingdata. Supposeitmakes50percentin
thetrainingdatacomparedto30percentearnedbyitstwo-parameterpre
decessor. Theincreasedperformanceinthetrainingdatawasapredictable
consequence ofthe increased complexity (additionalparameters). Weare,
however, pleasantly surprised to learn that the four-parameter rule also
does better in the testing set, earning 20 percent versus the 15 percent
earned bythe basicdualmoving-average rule. Whena morecomplexrule
performs better in the test set, it is reasonable to assume that the addi
tional complexitysuccessfully exploited additional systematic aspects of
marketbehavior.
Therefore, it's back to the drawing board again with anotherattempt
to improve the rule performance with yet additional complexity. This in
volves new conditions with additional paranleters. The new conditions
might be other filters, or they could be alternative entry signals. At each
increment in complexity, all the possible combinations of the parameter
valuesare evaluatedonthe trainingsetand the bestperformingcombina
tion is then run against the test set. The process ofadding complexity is
continued until performance in the test set begins to decline. This is the
signal that the boundary between underfitting and overfitting has been
crossed; the additional complexityhas begun to fit the noise in the train
ing data. The optimal rule is the rule of highest complexity that did the
bestperformancein the testset.
Whatthen is thepurpose ofthethird datasegment, thevalidationset?
Thusfar, ithasnotbeentouched. Theastute readerwillhave realizedthat
the performance attained by the optimal complexity rule in the test data
setispositivelybiased. Therepeatedvisitstothetestsettofind theruleof
optinlal complexityinduced a positive bias-the data-mining bias. Hence,
to obtain an unbiased estimate ofthe optimal complexity rule's expected
performance in the future, we must go to the validation set. Because the
validation setwas not utilized in the search for optimalparametervalues
oroptimalcomplexity, itremainsuntainted.
THE FUTURE OF TECHNICAL ANALYSIS
Technical analysis stands at a fork in the road. Its future depends on the
path practitioners take. The traditional path continues the nonscientific
subjectivist tradition of naive theories built on untestable propositions,



==================================================
                     PAGE 478                     
==================================================

462 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
anecdotal evidence, and intuitive analysis. The otherpathis thescientific
approach, whichIrefertoasevidence-basedtechnicalanalysis(EBTA).It
is confined to testable methods and objective evidence, rigorously and
skeptically examined with the appropriate tools of statistical inference.
Thus, EBTA charts a course between foolish gullibility and relentless
skepticism.
Inowexerciseliterarylicenseforliteraryeffectand make thefollow
ingunfalsifiable, forecast: Technical analysis will bemarginalized to the
extent it does not modernize. There are ample precedents in the history
of science to substantiate this statement. Astrology, which gave birth to
the science ofastronomy, now languishes on the periphery while its off
springthrivesasthe mainstreamdiscipline.
Will TA remain on its traditional unscientific path, thusfating itto be
cast aside with otherancientpractices, orwill it adopt EBTAand remain
vital and relevant? The historical precedents are not favorable. The his
tory of science suggests that few practitioners who are committed to a
given paradigm ever abandon it. This is consistent with evidence pre
sentedinChapter2, whichestablishedthatbeliefsareextremelyresilient.
Studies have shown that a belief can survive a complete discrediting of
theevidencethatoriginallygave riseto thebelief. IfTAweretoremainon
itstraditionalpath, itwould indeed be unfortunate. Ibelieve thatthiswill
inevitably lead to TA relinquishing territory it originally staked to more
rigorous disciplines, suchasempiricalandbehavioralfinance.
This is not to suggest that traditional TA will lose all its fans. There
willalwaysbe consumersfortheservicesofsoothsayersandseers. Divin
23
ingthefuture is, afterall, the world'ssecond oldestprofession. Someau
dience is assured because of a deep psychic need to reduce anxiety
provoked bythe uncertainty ofthe future. According to ScottArmstrong,
anexpertinforecasting theory, the briskdemandfortheseers' servicesis
most certainly not attributable to forecast accuracy. In his paper, "The
Seer-SuckerTheory: TheValueofExpertsinForecasting,"24Armstrongar
gues that people pay heavily for the experts' views, despite their lack of
accuracy. Whatcustomers are reallybuyingis an illusory reduction inun
certainty and an off-loading ofresponsibility onto the expert. Armstrong
refers to these consumers as suckers because of the barrage of studies
cited in his paper showing that experts' forecasts are minimally better
than those ofnonexperts. This lack of accuracy has been found to hold
trueacrossa range ofdisciplines, includingfinancial predictions.
One seminal study cited by Armstrong is by Alfred Cowles,25 who
studied the track record ofthe market-guru ofthe early 1930s, Hamilton,
the editor of the Wall Street Journal. Despite Hamilton's reputation
among his readers, Cowles's study showed that, over the period 1902 to
1929, 50percentofthe guru'sforecasts ofdirectionalchangewere wrong.



==================================================
                     PAGE 479                     
==================================================

Case Study Results and the Future ofTA 463
Cowles also reviewed the predictive performance of20 insurance compa
nies, 16 financial services, and 24 financial publications, all ofwhich pro
duced unimpressive records. The same conclusion can be found in the
statistics provided by Hulbert's financial digest, which currently follows
the perforn1ance of over 500 investment portfolios recommended by
newsletters. In one Hulbertstudy, 57 newsletters were trackedfor the 10
yearperiodfrom August 1987through August 1998. Duringthattime, less
than 10 percent of the newsletters beat the Wilshire 5000 Index's com
pound rate ofreturn.
Am1strong also contends that expertise, beyond a minimal level, adds
littlein thewayofpredictiveaccuracy.Thus, consumerswould bebetteroff
buyingthe leastexpensivepredictions, whichare likelyto beasaccurateas
themostexpensive,orinvestingthemodesteffortrequiredtoachievealevel
ofaccuracythatwouldbecomparabletothemostexpensiveexperts.
Recently, there have been signs thatsophisticated consumers ofWall
Street advice are unwilling to pay for traditional TA. During 2005, two of
WallStreet'slargestbrokeragefirmsshutdowntraditionalTAresearchde
partments.26 Whetherthis is a random blip orthe start ofa trend remains
to beseen.
Evidence-Based Technjcal Analysis:
The Scientific Path
Adopting a scientific approach will have immediate benefits for technical
analysis.TheeliminationofsubjectiveTA, eitherbyitsbeingreformulated
into objective testable methods or by jettisoning it altogether, would
transform TA into a legitimate scientific activity. Only ideas that are
testable would be entertained, and onlythose thatprove themselves with
objectiveevidencewould beadmitted to its bodyofknowledge.
This is not to suggest that, if EBTA were to be adopted, all answers
will be clear-cut and all debate will end-far from it. Debate is part and
parcelofthescientificprocess. Eveninthehardsciences, importantques
tions seek resolution, butthe questions are about meaningful hypotheses
thatspinofftestableconsequences.
EBTA has limits. The majorone is the inabilityto conduct controlled
experiments, the gold standard ofscientific investigation. In a controlled
experiment, theeffectofonevariablecanbeisolatedandstudiedbyhold
ing all othervariables constant. TAis inevitably an observational science
thatmustlookto historyfor new knowledge.
Technical analysis is not alone in this regard. Archeology, paleontol
ogy, and geologyalso rely onhistorical data. Nevertheless, theyare scien
tific disciplines because they deal in testable questions and look to
objectiveevidencetoseparateusefulfrom uselessideas.



==================================================
                     PAGE 480                     
==================================================

464 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
There are statistical procedures that can partly ameliorate the lack
27
of experimental controls. For example, when Fosback quantified the
optimism/pessimism of mutual-fund managers, he removed the con
founding effectofshort-terminterestratesoncashreserves with regres
sion analysis. By nullifying the powerful influence ofshort-term interest
rates onmutual fund cash reserves, he wasable to derive an uncontami
nated measure offund managerpsychologycalledthe FosbackIndex. In
a like manner, Jacobs and Levy28 used multiple regression analysis to
minimize the confounding effects of multiple variables. This allowed
them to derive purified indicators for the prediction of relative stock
performance. For example, they were able to measure the pure return
reversaF9 effect, thus revealing it to be one of the most powerful and
consistent predictors of relative stock performance. In other words,
they found a TA effect that works, namely, stocks that have been weak
overthe pastmonth tend to have positive relative performance over the
following month.
Anotherdisadvantage ofnotbeingable to conductcontrolled exper
iments is the inability to produce new samples ofdata. Technical analy
sis researchers have one body ofmarket history that gets used overand
over again. This contributes to the data-mining-bias problem. The more
rules one tests in a given body ofdata, the more likely a rule will fit the
data by accident. The inability to produce new sets of observations,
which can be done in the experimentalsciences, underscores the impor
tance of using statistical inference methods that take the data-mining
biasinto account.
The Expert's Role in a
Human-Computer Partnership
I assert that the future ofTA would be best served by a partnership be
tween TA experts and computers. This is not a new prescription. I was
30
first exposedtoitinthewritingsofFelsen over30yearsago, andleading
edge TApractitionershave beentaking advantage ofadvanced datamod
31
eling and machine learning for almost as long. Within the past decade,
numerous articles in peer-reviewed journals have shown the value ofap
plying advanced data-mining methods to technical and fundamental indi
32
cators. Valid objectivepatternshave beenfound.
Thispartnership exploitsa synergybetweenhuman experts and com
puters. Thesynergyexists becausethe informationprocessingabilities of
these two entities are complimentary-computers are strong where hu
mans are weak and visa versa. I am referring specifically to computers
armed with data mining software and humans possessing expertise in a
particulardomain: TA, cytology, oilfield geology, etc.



==================================================
                     PAGE 481                     
==================================================

Case Study Results and the Future ofTA 465
Though it is an oversimplification, people are inventive but comput
ersare not. People canposequestionsandhypothesize explanationsthat
organize disparate facts into a pattern. However, this uniquely human
ability has a dark side; it makes us gullible. Our gullibility is a conse
quence ofa mind that evolved to be more prone to a state ofbeliefthan
an attitude ofskepticism. Therefore, although we are good at proposing
ideas, like new indicators or new rules to test, we are ineffectual at the
equallyimportanttaskofdisposing ofpoorideas. Moreover, ourintellect
never evolved to interpret highly complex or random phenomena. This
shows up in our limited abilities to engage in configural thinking, the in
tellectual task required when combining a multitude ofvariables into a
prediction orjudgment.
Though computers cannot invent, data-mining software allows them
to dispose ofirrelevantindicators and to synthesize complexmodels that
can combine numerous variables into predictions. In other words, com
puters can engage in configural analysis. A computer's ability to synthe
size complex high-dimensional models is only limited by the amount of
dataavailable.Thisconstraintonpredictivemodeling, knownasthecurse
of dimensionality, was named by mathematician Richard Bellman. In
essenceitsaysthatas the numberofindicators comprisinga multidimen
sionalspaceisincreased,whereeachindicatoris representedbyadimen
sion, the observations populating the space become less dense at an
exponential rate. Acertain level ofdatadensity is required to allow data
mining software to distinguish valid patterns from background random
ness. Maintainingagivenlevelofdatadensityas dimensionsare increased
thus requires an exponentially increasing number of observations. If 100
observationsmeetdatadensity requirementsatthe two-dimensional level,
1,000wouldberequiredatthreedimensionsand 10,000atfourdimensions.
Despite this very real limitation on data mining, modem data-mining soft
wareisinfinitelybetteratconfiguralreasoningand complexpatterndetec
tion thanthesmartesthumans.
The bottomlineis this: Computers cando whatpeople can'tand peo
plecando whatcomputerscan't. Itis theperfectmarriage.
The Futility ofthe Subjective Forecaster
Despitethepotentialofahuman-computersynergymanytraditionalprac
titioners insist on using subjective methods to derive a forecast or signal
from amultitude ofindicators. Theydosointheface ofan overwhelming
body ofevidence, that has been accumulating overthe last 50 years that
demonstratesthefutility ofsuchanapproach.Thesestudiesshowthat,for
recurrent prediction problems, across a variety offields, subjective pre
dictionsarerarelysuperiortopredictionsbasedonevensimplestatistical



==================================================
                     PAGE 482                     
==================================================

466 CASESTUDY: SIGNAL RULES FORTHES&P sao INDEX
rules (models). Arecurrent prediction problem is one in which the same
type ofpredictionmustbemaderepeatedlyonthebasisofasimilarsetof
information. Examplesofrecurringpredictionproblemsincludetechnical
analysis, predicting violence of prisoners being released, predicting if a
borrowerislikelyto defaultona loan, andsoon. Theinformation setthat
is available to the decision maker is a set ofreadings on a multitude of
variables. In TA, this would be a set of technical indicator readings. In
creditevaluation, itwouldbea setoffinancial ratios. The task isto deter
mineifthereexistsastablefunctional relationshipbetweentheindicators
and the outcome that is to be predicted, to ascertain what the nature of
that relationship is, and then to combine the variables according to that
functional relationship. When the relationship is complex and involves
morethanjustafewfactors, the decision makerisfaced withaconfigural
thinking problem that exceeds humanintellectual limits. The only choice
forthesubjective expertistofall backonintuition.
The seminal study comparing the accuracy ofsubjective predictions
(expert intuition) to predictions based on statistical rules (models) was
33
done byPaulMeehl in 1954. Itwas a review ofpriorstudies, known asa
meta-analysis, which examined 20 studies that had compared the subjec
tive diagnoses ofpsychologists and psychiatrists with those produced by
linear statistical models. The studies covered the prediction ofacademic
success, the likelihood of criminal recidivism, and predicting the out
comes of electrical shock therapy. In each case, the experts rendered a
judgment by evaluating a multitude ofvariables in a subjective manner.
"Inallstudies, thestatisticalmodelprovidedmoreaccuratepredictionsor
the two methods tied."34 A subsequent study by Sawyer35 was a meta
analysis of45 studies. "Again, there was nota single study in which clini
calglobaljudgmentwassuperiortothestatisticalprediction(termed 'me
chanicalcombination'bySawyer)."36Sawyer'sinvestigationisnoteworthy
becauseheconsideredstudiesinwhichthehumanexpertwasallowedac
cess to informationthat was not considered bythe statistical model, and
yet the model was still superior. Forexample, the academic performance
of37,500 sailors in naval training school was more accurately predicted
by a model based on test scores than by judges who had access to test
scores and who had conducted personal interviews. The judges appar
entlygave too muchweightto the nuanced informationthey thoughtthey
had obtained in the personal interviews and gave too little to objective
measurablefacts.
An article byCamerer37comparedthe accuracyofexperts' subjective
predictions with two objective methods: the predictions of multivariate
linear models derived with regression analysis and a second method
based on a mathematical model ofhow the experts rendered theirjudg
ments. The least accurate predictions were the subjective judgments



==================================================
                     PAGE 483                     
==================================================

Case Study Results and the Future ofTA 467
made by experts. The most accurate predictions were made bythe linear
regression model. Figure 9.9 was prepared from datapresented by Russo
and Schoemaker38 summarizing Camerer's results. The figure of merit
used to compare the forecasting methods was the correlation coefficient
between predicted outcome and actual outcome. The correlation coeffi
cient can range from 0, signifying the predictions were wortWess, to 1.0
signifying perfect predictive accuracy. The prediction problems spanned
nine different fields: (1) academic performance ofgraduate students, (2)
life-expectancy ofcancerpatients, (3) changes in stockprices, (4) mental
illness using personality tests, (5) grades and attitudes in a psychology
course, (6) businessfailures usingfinancial ratios, (7) students' ratings of
teaching effectiveness, (8) performance oflife insurance salespersonnel,
and (9) IQ scores using Rorschach Tests. Note that the average correla
tion ofthe statisticalmodel was0.64versus the expertaverage of0.33. In
termsofinformationcontent,whichismeasuredbythecorrelationcoeffi
cient squared or r-squared, the model's predictions were on average 3.76
timesasinformativeastheexperts'.
Numerousadditionalstudiescomparingexpertjudgmenttostatistical
models (rules) have confirmed these findings, forcing the conclusionthat
people do poorlywhen attempting to combine a multitude ofvariables to
make predictions orjudgments. In 1968, Goldberg39 showed that a linear
predictionmodel utilizingpersonalitytestscoresas inputscoulddiscrimi
nate neurotic from psychotic patients betterthan experienced clinical di
agnosticians. The model was 70 percent accurate compared with expert
accuracy thatranged from 52percentto67percent. A1983studyshowed
0.9
0.8
Model
0.7
------------ ----------- -- Mean
Correlation 0.6 - 0.64
Predicted 0.5 - l- -
versus
Actual 0.4 :-
..::.-:..:..:...:- r-=-=-=
Ff -=- --
-
-l 1--
=-'-
-
-:=
-f r-=- -=-
-:-
.-
:..:-
E Mx ep ae nrt
0.3 0.33
o-
0.2 F - f-F' f- f- I- -
0.\ - I- l- I-
Expert 0
0
Model 0
-0.1
2 3 4 5 6 7 8 9
I~IGURE 9.9 Predictive performance ofsubjective expertversus objective linear
model in nine fields.



==================================================
                     PAGE 484                     
==================================================

468 CASESTUDY: SIGNAL RULES FORTHES&P sao INDEX
howpoorlyhumanexperts do when the numberofvariables is increased.
Thetaskwastopredictthepropensityforviolenceamongnewlyadmitted
male psychiatricpatientsbased on 19inputs. The average accuracy ofthe
experts, as measured by the correlation coefficientbetween their predic
tionofviolence and the actual manifestationofviolence, was apoor0.12.
The singlebestexperthad a score of0.36. The predictions ofa linearsta
tistical model, using the same set of 19 inputs, achieved a correlation of
0.82. Inthisinstancethemodel'spredictionswerenearly50timesmorein
formative than theexperts'.
Meehl continued to expand his research of comparing experts and
statistical models and in 1986 concluded that "There is no controversy in
socialsciencewhichshowssuchalarge bodyofqualitativelydiversestud
ies coming out so uniformly in the same direction as this one. When you
4
are pushing 90 investigations [currently greater than 150°] predicting
everything from the outcomes offootball games to the diagnosis ofliver
diseaseandwhenyoucanhardlycomeupwitha halfdozenstudiesshow
ing even a weak tendency in favor of the clinician, it is time to draw a
practicalconclusion."41
The evidence continuesto accumulate, yetfew experts payheed. The
most comprehensive study comparing the accuracy of experts' predic
tions with those produced by linear statistical prediction models was
done by Grove and Meehl in 1996,42 which covers 136 studies. In 96 per
cent of these studies, the rules either outperfornled or equaled the ex
perts. In a similar study in 2000, Swets, Monahan, and Dawes compared
three methods ofprediction in medicine and psychology: (1) expert sub
jectivejudgment, (2) statistical rules (models), and (3) a combination of
the first two where the output ofthe statistical rule was modified by the
expert'sjudgment. Theirfinding wasconsistentwithpriorstudies, leading
to the conclusion that experts who continue to operate on the basis of
subjective judgment, are not only in error in a predictive sense but per
43
hapsinanethical oneaswell.
Fromtheforegoing, HastieandDawes44drawthefollowingconclusions.
1. Experts and novices make subjectivejudgments byrelying on a rel
atively small number of inputs (3 to 5). Assessments of livestock
and weather forecasting are exceptions in that a larger number of
inputs is typically used. The ability to use a larger set of informa
tion can be explained by the fact that immediate and precise feed
back is available, which enables the experts in these two domains
to learn. In other areas, like medical diagnosis, school admissions,
and financial forecasting, feedback is often delayed or never avail
able. Hastie and Dawes's conclusion suggests why subjective tech
nicians do not learn from their mistakes-precise feedback is not



==================================================
                     PAGE 485                     
==================================================

Case Study Results and the Future ofTA 469
available because of the lack of objective pattern definitions and
evaluation criteria.
2. For a wide range ofjudgment tasks, experts' judgments can be ex
plained byan additive (linear) modelofthe inputs.
3. Fewjudges use nonlinearlconfigural thinking to combine inputs even
though they are under the impression that they are doing so. How
ever, when they do use configural reasoning, they tend to do it
poorly.45
4. Experts do not understand how they arrive at their own judgments.
Thisproblem is mostpronounced among highlyexperienced experts.
As a result, theirjudgmentsare inconsistentwhen given the same set
ofinputsondifferentoccasions.
5. Inmanydomains,judgesdonotagreewitheachotherwhen giventhe
same set ofinformation. The low level of interjudge agreement says
thatsomeoneiswrong.
H. When irrelevant information is added to the set ofinputs, judges be
come more confident although accuracy does not improve. Clearly,
theyare tillable to distinguishrelevantfrom irrelevantvariables.
7. Thereareveryfewjudgeswho aredemonstrablysuperior.
Hastie and Dawes's conclusions have received additional support
from Tetiock,4Gwho evaluatedpredictionsfrom expertsindifferentfields
and compares them to relatively simple statistical models that extrapo
laterecenttrends. Tetlock'swork, whichappears to bethe mostrigorous
long-term study ever conducted of expertjudgment in the political and
economic domains, does suggest that certain subjective cognitive styles
ofdecision making are superior. He calls this style the fox. However, he
shows that all human-based predictions, whatever their style, pale in
comparison to formal model-based predictions. Using a two-dimensional
framework for evaluating forecast accuracy, discrimination, and calibra
tion, the accuracy offormal models, based on a method called general
ized autoregressive distributed lags, far exceeds the performance of
human experts.47
Why Are Experts' Subjective Predictions So Poor?
Although there is general agreementamong those who study expertjudg
ment, that it is rarely superior to objective forecasts, there is less agree
ment about why. A number of explanations have been proposed: the
intrusion ofemotional factors, the lack ofconsistency in howfactors are
weighted, excessiveweightgiventovividconcretefeaturesthathavelittle



==================================================
                     PAGE 486                     
==================================================

470 CASESTUDY: SIGNALRULES FORTHES&P 500 INDEX
predictivepower, inadequate weightgiven to abstractstatisticsthatpos
sess predictive power, thefailure to use configural rulesproperly, falling
prey to illusory correlations, failing to notice valid correlations, and be
ingaffectedbytheactionsandstatementsofothers. Manyofthese were
covered in Chapter2, with respectto the birthand maintenance oferro
neous beliefs, and in Chapter 7, with respect to information cascades
and herd behavior, and I will not repeat them here. Here, Iwill consider
the intrusion of emotional factors on the subjective forecaster, which
has notbeen discussed in priorchapters.
Anumberofinvestigatorshaveconsideredtheeffectofemotionson
subjectivejudgments made under conditions ofuncertainty. One model,
proposed by Loewenstein et al.,48 suggests that a subjective forecast is
impacted by the decision maker's anticipated emotional reaction to the
prediction's outcome. Even though the expert's anxieties about the
chance of being wrong have no valid role in a forecast, they appear to
playa role. Nofsinger49 presents Lowenstein's model in a diagram that I
have reproduced in Figure 9.10. Note that when a person is making a
prediction they are impacted by anticipating their emotions in response
to the possible outcomes. This affects both the cognitive evaluation of
informationand the currentemotionalstate ofthe decisionmaker.5o
Slovic, Finucane, Peters, and MacGregor51 explain the intrusion of
emotionsonthesubjectivedecisionmakerintermsofaffect. Affectrefers
to the good or bad feelings associated with a stimulus. These feelings of
ten operate unconsciously and produce rapid reflex-like behaviors that
Anticipated
Outcomes
Including ~
Anticipated ,--------,
Emotions Cognition
I ~
'---_Ev_a_lu_a_t_io_n-----'
1--
Subjective Outcomes
I
Probability B,h,v;o< (Including
Assessments Emotions)
1/
I
/L--F-ee-lin-gs-J
Other Factors;
Vividness /' ~
Immediacy
Background
Mood
171G HE 9.10 Lowenstein's model ofjudgment impacted byaffect.



==================================================
                     PAGE 487                     
==================================================

Case Study Results and the Future ofTA 471
are not under the decision maker's conscious control. Slovic points out
that feelings and images can weigh heavily in judgments that would be
better handled by a purely analytical and rational consideration of evi
dence. For example, a market analyst who has negative feelings associ
ated with a declining market is likely to remain more bullish than the
evidence would warrant. However, not alljudgments based on affect are
bad. Some situations are accurately represented by the feelings they pro
voke and do demand the fast reflex-like decisions produced bythis mode
ofthinking. Affective decision making was probably the dominant mode
ofjudgment during the evolution of the human species. In other words,
the speed ofaffective decision making hadsurvivalvalue. Fear ofheights
oroflargepredatorsdemandedfasterchoicesthanthosethata conscious
weighing of pros and cons would have been able to produce. "However,
like other heuristics that provide generally adaptive responses but occa
sionally lead us astray, reliance on affect can also deceive US."52 This oc
curswhen the relevantfeatures ofa situation are not well represented by
the feelings they stimulate. Financial market prediction is such a situa
tion, yet the subjective forecaster remains vulnerable to the automatic
and subconsciousjudgmentsofa heuristic based onaffect.
The model offered byJoseph Forgasfurther illuminates thefutility of
subjective forecasting.53 It explains that "the greater the complexity and
uncertainty in a situation, the more emotions will influence a decision."54
This characterizes the task faced by the subjective TA analyst who oper
atesagainstinsurmountableobstacleswhentryingtoevaluatea multitude
ofindicatorsin a rational manner. Icontend thatsubjective forecasting is
no longerthe analyst'sproperrole.
The Expert's Proper and Crucial Role
in Human-Machine Partnership
Theproperrole oftheTApractitionerinthe envisionedhuman-computer
partnership is twofold: (1) proposing an information-rich set of candi
date inputs suitable for presentation to the data-mining software, (2)
specifying the problem to be solved by data-mining software (i.e., the
target variable. In other words, the TA expert's proper role is to supply
what the computer lacks, namely, expertise in the domain ofTA and in
ventive talent.
It is somewhat ironic that in this high-tech era, the analyst's role has
now taken centerstage. This is a resultofthe changed economics ofdata
mining. Within the lastdecade, the price ofcomputingpowerand power
ful data-mining software has declined sharply. Whereas these resources
had beenthe exclusive domain ofwell-financed researchorganizationsas
recentlyasa decadeortwo ago, todaytheyarecommoditiesaccessibleto



==================================================
                     PAGE 488                     
==================================================

472 CASESTUDY: SIGNAL RULES FORTHES&P 500 INDEX
almostall marketparticipants. Thesetrends have democratized datamin
ing. Most investors now have access to the same tools that once gave a
huge competitive advantage to the largestinstitutionalinvestors. Thatad
vantaged has been greatly diminished. Consequently, the competitive ad
vantage-the source ofaddedvalue-has nowshifted backto the human
analyst. The competitive advantage now lies with those TA experts pos
sessing the best insights with regard to proposing indicators and specify
ing target variables. The TA practitioner who embraces the EBTA
philosophyiswellpositionedtoplaythis role.
Indicator design and target specification is a huge topic suitable for
entirebooks. Thereare manybooksthatcoverindicatorsthatarespecific
toTA. Thereisnoneedtoduplicatethathere.Thereareotherbooksthat
55
treat the more general problem ofdesigning inputs (predictive variables)
56
that are suitable for specific types of data-mining algorithms. Different
algorithms have different requirements. For example, although it is cru
cial that inputs to a neural net be properly scaled, inputs to decision-tree
algorithmsneednotbe. Masters,57Pyle,58andWeissand Indurkhya59cover
these issuesnicely.
Indicatordesign refers to the specification oftransformationsapplied
to raw market data before it is presented to the data-mining tool. Data
minersoftenreferto this aspreprocessing. Thetaskofproposingalistof
information-rich and properly designed candidate indicators for a data
miningproblemrequiresthe domainexpertiseand inventivetalentsofthe
humananalyst.
Anyone who has gotten their hands dirty in the data-mining trenches
knows the following truth: a necessaryconditionfor success indatamin
ing is a good set ofcandidate indicators. Good does not mean that all in
puts have to be informative. However, some of the proposed variables
must contain useful information about the target variable. According to
Dorian Pyle, a noted authority on data mining, "after finding the right
problemtosolve, datapreparationisoftenthe keytosolvingtheproblem.
Itcan easily be the difference between success and failure, between use
able insights and incomprehensible murk, between worthwhile predic
tions and useless guesses."60 Dr. Timothy Masters, another data-mining
authority, says, "Preprocessingisthe is the keytosuccess...evenaprim
itive prediction model can perform well if the variables have been pre
processedinsuchaswayasto clearlyrevealtheimportantinformation.
"61
The message is amplified by Weiss and Indurkhya, in their book Predic
tive Data Mining. "The greatestgains in performance are often made by
describing more predictive features (indicators). It is the human who
specifiesthesetoffeatures ... andwho investigateshowtotransformthe
originalfeatures intobetterfeatures." "Thecomputerissmartaboutdelet
ing weak features, but relatively dumb in the more demanding task of



==================================================
                     PAGE 489                     
==================================================

Case Study Results and the Future ofTA 473
composing new features or transforming raw data into more predictive
forms ... the composition offeatures is a greater determining factor in
thequalityofresultsthanthespecificpredictionmethodsusedtoproduce
those results. In most instances, feature composition is dependent on
knowledge ofthe application."62
The TA practitioner who understands data-mining methods and the
inlportant issues in indicator design will be well positioned to play this
crucial role in the human-machine partnership of twenty-first-century
technical analysis. With the computer as an intelligence amplifier and a
scientific orientation as the mindset, the EBTA practitioner is uniquely
well positioned to push the frontiers ofmarket knowledge forward at an
acceleratedpace. Giventhathumanintelligenceisessentiallyunchanging,
but computer intelligence is increasing at an exponential rate,63 no other
approachtoTAmakessense.



==================================================
                     PAGE 490                     
==================================================





==================================================
                     PAGE 491                     
==================================================

Proof That
Detrending Is
Equivalent to
Benchmarking Based
on Position Bias
Inorderto compensatefor marketbiasinteractingwithlong-shortbias,
one should compute the expected return ofa random system with the
same long-short bias, and subtractthis bias-relatedreturn from the re
turn ofthecandidatesystem.
The expectedvalue ofa single raw return drawnfrom the population
ofhistorical rawreturns is, bydefinition, themeanofthese returns.
(AI)
Let Pi signify the position of a trading system at opportunity i. This
will be long, short, orneutral. The expected return ofa trading system in
whichpositionsarerandom is:
L L
ERandam = ERaw - ERaw (A2)
P;=Long P;=Shorf
Thetotal return ofthe candidatesystem is thesumofthe raw returns
at those times when the candidate system is long, minus the sum ofthe
rawreturnswhenthesystemisshort:
L L
Total = R; - R; (A3)
P,=Long P;:::.ShoYt
475



==================================================
                     PAGE 492                     
==================================================

476 DETRENDING IS EQUIVALENTTO BENCHMARKING BASEDON POSITION BIAS
Thetotalreturn ofthe candidatesystem (Equation (A.3)) iscorrected
for long/shortprejudicebysubtractingthe expectation ofa similarlyprej
udiced random system (Equation (A.2)). Subtraction distributes overthe
summation, giving:
L L
Corrected= (R
j -
ERaw)- (R
j -
ERaw) (A.4)
Pj=Long Pi=Short
Notice that the corrected return ofEquation (A.4) is identical to the
uncorrected return ofEquation (A.3) exceptthat the mean ofthe raw re
turns issubtractedfrom eachindividualrawreturn. Inotherwords, tore
move the effect oflong/short imbalance in a biased market, all you need
to do iscenterthe rawreturns.



==================================================
                     PAGE 493                     
==================================================

Notes
INTRODUCTION
1. Data typically considered by TA includes prices of financial instruments;
trading volume; open interest, in the case ofoptions and futures; as well as
othermeasuresthatreflecttheattitudesandbehaviorofmarketparticipants.
2. J. Hall,PracticallyProfound:PuttingPhilosophy to Work inEverydayLife
(Lanham, MD: Rowman& LittlefieldPublishers,2005).
3. Ibid.,4.
4. Ibid.,4.
5. Ibid.,5.
6. Ibid.,5.
7. Ibid.,5.
8. Ibid.,6.
9. Ibid.,5.
10. Ibid.,81.
11. RD. Edwards and J. Magee, Technical Analysis ofStock Trends, 4th ed.
(Springfield,MA:JohnMagee, 1958).
12. Fora complete description ofElliottwave theorysee RR Prechterand A.J.
Frost,Elliott WavePrinciple(NewYork: NewClassicsLibrary, 1998).
13. Any version of these methods that has been made objective to the point
whereitisbacktestablewould negatethiscriticism.
14. The professional association oftechnical analysts, the MarketTechnicians
Association (MTA), requires compliance with the National Association of
SecuritiesDealersand the NewYorkStockExchange. Theseself-regulating
bodies require"that research reports have a reasonable basis and no un
warranted claims." Going even further, the MTA requires of its members
that they "shall not publish ormake statements concerning the technical
position ofa security, a market or any ofits components oraspects un
less such statements are reasonable and consistent in light ofthe avail
able evidence and the accumulated knowledge in the field of technical
analysis."
15. Some peer-reviewed academic journals include Journal of Finance,
Financial Management Journal, Journal ofFinancial Economics, JOUT
nal of Financial and Quantitative Analysis, and Review ofFinancial
Studies.
477



==================================================
                     PAGE 494                     
==================================================

478
NOTES
16. Outside ofacademia, there has been a move to greateremphasis on objec
tive methods ofTA, but often the results are notevaluated in a statistically
rigorousmanner.
17. F.D. Arditti, "CanAnalysts Distinguish Between Real and Randomly Gener
ated Stock Prices?," Financial Analysts Journal 34, no. 6 (November/
December1978),70.
18. J.J. Siegel,Stocksfor theLongRun, 2nded. (NewYork: McGraw-Hill, 1998),
243.
19. G.R Jensen, RR Johnson, and J.M. Mercer, "Tactical Asset Allocation and
Commodity Futures: Ways to Improve Performance," Journal ofPortfolio
Management 28,no. 4(Summer2002).
20. C.RLightner,"ARationaleforManagedFutures,"TechnicalAnalysisofStocks
& Commodities (2003). Notethatthispublicationisnotapeer-reviewedjour
nal butthe article appeared to be wellsupported and itsfindings were con
sistentwiththepeer-reviewedarticlecitedinthepriornote.
21. P.-H. Hsuand C.-M. Kuan, "Reexaminingthe ProfitabilityofTechnicalAnaly
sis with Data Snooping Checks," Journal ofFinancial Economics 3, no. 4
(2005),606-628.
22. R Gency, "The Predictability of Security Returns with Simple Technical
TradingRules,"Journal ofEmpiricalFinance5(1998),347-349.
23. N. Jegadeesh, "EvidenceofPredictableBehaviorofSecurityReturns,"Jour
nalofFinance45(1990),881-898.
24. N. Jegadeeshand S. Titman, "Returnsto BuyingWinnersand SellingLosers:
Implications for Stock Market Efficiency," Journal ofFinance 48 (1993),
65-91.
25. T.J. Georgeand C.-Y. Hwang, "The 52-WeekHighand MomentumInvesting,"
Journal ofFinance59,no. 5(October2004),2145-2184.
26. B.R Marshall and R Hodges, "Is the 52-Week High Momentum Strategy
Profitable Outside the U.S.?" awaiting publication in Applied Financial
Economics.
27. C.L. Osler, "Identifying Noise Traders: The Head and Shoulders Pattern in
U.S. Equities," StaffReports, Federal Reserve Bank ofNew York 42 (July
1998),39pages.
28. 1. Blume and D. Easley, "MarketStatisticsand TechnicalAnalysis: TheRole
ofVolume,"Journal ofFinance49, no. 1(March1994),153-182.
29. V. Singal, Beyond the Random Walk: A Guide ofStock Market Anomalies
and Low-Risk Investing (New York: Oxford University Press, 2004). These
resultsarediscussedinChapter4, "ShortTermPriceDrift."Thechapteralso
contains an excellent list of references of other research relating to this
topic.
30. A.M. Safer, "AComparisonofTwoDataMiningTechniquestoPredictAbnor
mal StockMarketReturns," Intelligent DataAnalysis 7, no. I (2003),3-14;
G. Armano, A. MuITU, and F. Roli, "StockMarketPredictionbya Mixture of
Genetic-Neural Experts," International Journal ofPattern Recognition &
Artificial Intelligence 16, no. 5 (August 2002), 501-528; G. Armano, M.
Marchesi, andA. Murru, "AHybridGenetic-NeuralArchitectureforStockIn-



==================================================
                     PAGE 495                     
==================================================

Notes 479
dexes Forecasting," Information Sciences 170, no. 1(February 2005), 3-33;
T. Chenoweth, Z.O. Sauchi, and S. Lee, "EmbeddingTechnical Analysis into
Neural Network BasedTradingSystems,"AppliedArtijicialIntelligence 10,
no. 6 (December 1996), 523-542; S. Thawornwong, D. Enke, and C. Dagli,
"NeuralNetworksasaDecisionMakerforStockTrading:ATechnicalAnaly
sisApproach,"InternationalJournal ofSmartEngineeringSystemDesign
5, no. 4 (OctoberlDecember2003), 313-325; A.M. Safer, "TheApplication of
Neural-Networks to PredictAbnormal Stock Returns Using InsiderTrading
Data,"AppliedStochasticModels inBusiness& Industry 18,no. 4(October
2002),380-390;J. Yao, C.L.Tan,andH.-L. Pho,"NeuralNetworksforTechni
cal Analysis: AStudy on KLCI,"InternationalJournal ofTheoretical & Ap
pliedFinance2,no. 2(April1999),221-242;J. KorczakandP.Rogers,"Stock
Timing Using Genetic Algorithms,"Applied Stochastic Models in Business
& Industry 18, no. 2 (April 2002), 121-135; Z. Xu-Shen and M. Dong, "Can
Fuzzy Logic Make Technical Analysis 20/20?," Financial Analysts Journal
60,no. 4(July/August2004),54-75;J.M. Gorriz, C.G. Puntonet, M. Salmeron,
andJ.J. DelaRosa, "ANew ModelforTime-SeriesForecastingUsing Radial
Basis Functions and Exogenous Data," Neural Computing & Applications
13,no. 2(2004), 100-111.
31. ThisfirmwasacquiredbyGoldmanSachsinSeptember2000.
CHAPTER t Objective Rules and
Their Evaluation
1. Alongpositioninasecuritymeanstheinvestorownsthesecurityand hopes
to benefitbysellingitin thefuture atahigherprice.
2. Ashortposition ina security means the investorhassoldthe securitywith
out owning it but is obligated to buy it back at a later point in time. The
holderofashortpositionthereforebenefitsfromasubsequentpricedecline,
thereby permitting the repurchase at a lower level than the selling price,
earningaprofit.
3. Neutral refers to the case where the investor holds no position in the mar
ket.
4. Itis assumed that all marketinformation required bythe method is known
andpubliclyavailableatthetimethemethodproducestherecommendation.
5. Thepossibilityofthe timeseriesbeingequalto the movingaverageiselimi
nated by computing the value ofthe moving average to a greater degree of
precisionthanthepricelevel.
6. J. Bollinger,BollingeronBollingerBands(NewYork: McGraw-Hill,2002).
7. T. Hayes, TheResearchDrivenInvestor:How to UseInformation, Dataand
AnalysisforInvestmentSuccess (NewYork: McGraw-Hill, 2001),63.
8. Adiffusionindicatoris basedonananalysisofnumerousmarkettimeseries
within a defined universe (e.g., all NYSE stocks). The same rule, such as a
moving average cross, is applied to all the series comprising the universe.
Eachseriesisratedasinanuptrendordowntrend, dependingonitsposition



==================================================
                     PAGE 496                     
==================================================

480 NOTES
relative to its movingaverage.Thevalueofthe diffusion indicatoristheper
centage oftimeseriesthatare inan upward trend. Theindicatorisconfined
to therange0to 100.
9. This analysis does not make clear how many rule variations were explored
to attainthislevelofdiscrimination. AswillbepointedoutinChapter6,itis
impossibletoevaluatethesignificanceofthesefindingswithoutinformation
ontheamountofsearchingthatledtothediscoveryofarule.
10. Manyofthe rulestested in thisbook use dataseriesotherthantheS&P500
togeneratesignalsontheS&P500.
11. ThislawofstatisticsisdiscussedindetailinChapter4.
12. (0.9) X 0.035%- (0.1) X 0.035%=0.028%perdayor7.31%annualized.
13. (0.60) X 0.035%- (0040) X 0.035%=0.007%perdayor1.78%annualized.
14. Thelook-ahead biasisdiscussedbyRobertA. Haugen, The InejficientStock
MaTket: What Pays Off and Why (Upper Saddle River, NJ: Prentice-Hall,
1999),66.
15. The percentage of fund portfolios invested in interest-bearing cash instru
mentssuchasT-bills,commercialpaper, andsoforth.
CHAPTER 2 The Illusory Validity ofSubjective
Technical Analysis
1. J. Baron, Thinking and Deciding,3rd ed., (Canlbridge, UK: Canlbridge Uni
versityPress,2000), 12.
2. Thisfigure, redrawnbytheauthor, issimilarto and inspired byonefound in
G. Gigerenzer, CalculatedRisks:How to Know WhenNumbeTs Deceive You
(New York: Simon&Schuster, 2002). That book cites as the original source
of the illustration R.N. Shepard, Mind Sights: Onginal Visual Illusions
(NewYork: W.H. Freeman&Company, 1990).
3. T. Gilovich,How WeKnow What Isn't So: TheFallibility ofHumanReason
inEve-rydayLife(NewYork: FreePress, 1991).
4. See "Defying Psychiatric Wisdom, These Skeptics Say Prove It," New YOTk
Times (March9,2004), FI-F3.
5. Alpharefers to the capitalassetpricingmodel term, which represents the
Y-intercept obtained by regressing the returns of an investment strategy
onthereturns ofamarketindex. Itrepresentstheportionofreturnthatis
not attributable to the volatility ofthe strategy (its beta). Meritorious in
vestment strategies have alphas that are positive to a statistically signifi
cantdegree.
6. J. Murphy, TechnicalAnalysis oftheFinancial MaTkets: A CompTehensive
Guide to TradingMethods andApplications(NewYork: NewYorkInstitute
ofFinance, 1999),20,wheremarkettrendsaresaidtobeclearlyvisible.
7. H.Y. Roberts, "Stock Market'Patterns' and Financial Analysis: Methodologi
calSuggestions,"JouTnal ofFinance14,no. 1(March 1959), 1-10.
8. F.D. Arditti, "Can Analysts Distinguish Between Real and Randomly Gen
erated Stock Prices?," Financial Analysts JouTnal 34, no. 6 (November/
December 1978), 70. An infornlal test ofthe same nature is discussed by



==================================================
                     PAGE 497                     
==================================================

Notes 481
J.J. Siegel, Stocks for- the Long Run, 3rd ed. (New York: McGraw-Hill,
2002),286.
9. Gilovich,How WeKnow.
10. Shermer, M., Why People Believe Weir-d Things: Pseudoscience, Super-sti-
tion, andOther-Confusions ofOur-Time (NewYork: W.H. Freeman, 1997).
11. Gilovich,How WeKnow.
12. Ibid., 10.
13. H.A. Simon, "Invariants ofHuman Behavior,"AnnualReview ofPsychology
41 (January 1990), 1-20.
14. Shermer, WhyPeopleBelieve,26.
15. Ibid.,26.
16. C. Sagan, The Demon-Haunted Wor-ld: Science as a Candle in the Dar-k
(NewYork: RandomHouse, 1995),6.
17. C.Sagan.
18. U. Neisser,CognitivePsychology(EnglewoodCliffs, NJ: Prentice-Hall, 1967).
19. D. Kahneman, P Slovic, and A. Tversky, Judgment under- Uncer-tainty:
Heuristics andBiases(Cambridge, UK: CambridgeUniversityPress, 1982).
20. HA Simon, Models ofMan: Social and Rational (New York: John Wiley &
Sons, 1957).
21. GA Miller,"TheMagical NumberSeven, PlusorMinusTwo: SomeLimitson
Our Capacityfor Processing Information," Psychological Review 63 (1956),
81-97.
22. J.R Hayes, Human Data Processing Linlits in Decision Making, Report o.
ESD-TDR-62-48, Massachusetts: Air Force System Command, Electronics
SystemDivision, 1962.
23. Theterm algebmicin this contextrefers to the fact thatnumberswith both
positiveandnegativevaluesarebeingcombined.
24. This is a simple linear combination. There are more sophisticated methods
oflinearcombiningwherethevariablesareassignedweights(weightedalge
braicsum).
25. R HastieandRM.Sawes,RationalChoiceinanUncer-tain Wor-ld: ThePsy
chologyofJudgment andDecisionMaking (ThousandOaks, CA: SagePub
lications,2001),52.
26. J.E. RussoandPJ.H.Schoemaker,DecisionTraps: TheTenBarner-stoBril
liantDecision-MakingandHow to Over-come Them (NewYork: Doubleday,
1989), 137. The authors cite 10 studies comparing the accuracy ofintuitive
expertpredictionwithpredictionsproduced byobjectivelinearmodels.The
studiesincludepredictionofacadenlicperformance, life-expectancyofcan
cer patients, changes in stock prices, psychological diagnosis, bankruptcy,
studentratingsofteachereffectiveness, salesperformance, andIQbasedon
Rorschachtest.Theaveragecorrelationbetweenpredictionandoutcomefor
expertjudgmentwas0.33 ona scale of0to 1.0. The average correlationfor
the objective model was 0.64. In a meta-analysis ofover 100 peer-reviewed
studies comparing expert judgment with statistical rules, statistical rules
were moreaccurate in96percentofthe cases. SeeJ.A. Swets, RM. Dawes,
andJ. Monahan,"PsychologicalScienceCanImproveDiagnosticDecisions,"
PsychologicalScienceinthePublicInter-est1(2000).



==================================================
                     PAGE 498                     
==================================================

482 NOTES
27. Ifeach variable is assigned a value of0for low and 1for high, the possible
valuesofasumare: 0(allhavevaluesofzero), 1(onevariable hasavalueof
1and twoarezero), 2(twovariables havevaluesof1and onehasavalueof
zero),and3(allhavevaluesof1).
28. If financial markets are complex nonlinear systems, a predictive model
would have a nonlinear, functional form. Thus indicators would need to be
combinedinaconfigural(nonlinear) mannerratherthanasimplesequential
linearfashion.
29. B. Fischhoff, P. Slovic, and S. Lichtenstein, "Knowingwith Certainty: The Ap
propriateness ofExtreme Confidence,"Jomnal ojExperimenlal Psycfwlogy:
Human Perception and Pmformance 3 (1977), 552-564; LA Brenner, D.J.
Koehler, V. Liberman, and A Tversky, "Overconfidencein Probabilityand Fre
quency Judgments: A Critical Examination," Organizational Behavior and
HumanDecisionProcesses65(1996), 212-219; R. Vallone, D.W. Griffin,S. Lin,
andL.Ross,"OverconfidentPredictionofFutureActionsandOutcomesbySelf
andOthers,"JournalojPersonalityandSocialPsychology58(1990),.582-592;
A Cooper,C.Woo,andW.Dunkeelberg,"Entrepreneurs'PerceivedChancesfor
Success,"JournalojBusinessVentm1,ng3,no.97(1988),97-108
30. D.G. Myers, Intuition, Its Powm' and Perils (New Haven: Yale University
Press, 2002).
31. J. Metcalfe, "Cognitive Optimism: Self-DeceptionofMemory Based Process
ing Heuristics," Personality and Social Psychology Review 2 (1998),
100-110, as referenced by D.G. Myers, in Intuition, Ils Powm's and Perils
(NewHaven: YaleUniversityPress,2002).
32. S. Lichtenstein, B. Fischhoff, and L. Phillips, "Calibration of Probabilities:
The State ofthe Art to 1980," in Judgment under Uncm·tainty: Heuristics
andBiases, D. Kahneman, P. Slovic, and A. Tversky(Eds.) (Cambridge, UK:
CanlbridgeUniversityPress, 1982),306-334.
33. B. Fischoff and P. Slovic, "A Little Learning ... Confidence in Multi-Cue
Judgment Tasks," in R. Nickerson (Ed.), Attention & Pe1fo1fnance VIII
(Hillsdale, NJ: Earlbaum, 1980), the originalsource, as mentioned inJudg
mentunderUncertainty: Heuristics andBiases, D. Kahneman, P. Slovic,
and A Tversky (Eds.) (Cambridge, K: Cambridge University Press,
1982).
34. L. Goldberg, "Simple Models orSimple Processes? Some Research onClini
calJudgments,"AmericanPsychologist23(1968),338-349.
35. J.J.J. Christensen-Szalanski and J.B. Bushyhead, "Physicians' Use ofProba
bilisticInformationinaRealClinicalSetting,"JournalojExperimenlalPsy
chology:HumanPerceptionandPe'1formance7(1981),928-935.
36. AA DeSmet, D.G. Fryback, andJ.R. Thornbury, "ASecond Lookat the Util
ity of Radiographic Skull Examination for Trauma," American Jou1f",al oj
Radiology132(1979),95-99.
37. RussoandSchoemaker,Decision Traps, 72.
38. Ibid.
39. D. Dreman and M. Berry, "Analysts Forecasting Errors and Their Implica
tions for Security Analysis," Financial Analysts Jou1'nal 51 (May/June
1995),30-41.



==================================================
                     PAGE 499                     
==================================================

Notes 483
40. B. Barber and T. Odena, "Boys Will Be Boys: Gender, Overconfidence, and
CommonStockInvestment," WorkingPaper, UniversityofCalifornia, Davis,
1998a.
41. HershShefrin,BeyondGreedandFear: UnderstandingBehavioralFinance
and the Psychology ofInvesting (Boston: Harvard Business School Press,
2000),51.
42. D. Kahneman and M. W Riepe, "AspectsofInvestorPsychology,"Journal of
PortfolioManagement (Summer1998).
43. T. Gilovichcitesthefollowingstudies: N.D. Weinstein,"UnrealisticOptimism
aboutFutureLifeEvents,"Journal ofPersonality andSocialPsychology39
(1980), 806-820; N.D. Weinstein, "Unrealistic Optimism about Susceptibility
to Health Problems," Journal ofBehavioral Medicine 5 (1982), 441--460;
N.D. Weinstein and E. Lachendro, "Eco-centrism and Unrealistic Optimism
about the Future," Personality and Social Psychology Bulletin 8 (1982),
195-200.
44. Hulbert Digest, www3.marketwatch.com/Store/products/hfd.aspx?siteid=
mktw.
45. The self-attribution bias is discussed in the following studies: D.T. Miller
and M. Ross, "Self-Serving Biases in the Attribution ofCausality: Fact or
Fiction?" Psychological Bulletin 82 (1975), 213-225; R Nisbettand L.
Ross, Human Inference: Strategies and Shortcomings of Social Judg
ment(Englewood Cliffs, NJ: Prentice-Hall, 1980);P.E. Tetlockand A. Levi,
"Attribution Bias: On the Inconclusiveness of the Cognition-Motivation
Debate," Journal of Experimental Social Psychology 18 (1982), 68-88;
RR Lauand D. Russel, "Attributionsinthe SportsPages,"Journal ofPer
sonality and Social Psychology 39 (1980), 29-38; C. Peterson, "Attribu
tions in the Sports Pages: An Archival Investigation of the Covariation
Hypothesis," Social Psychology Quarterly 43 (1980), 136-141; RM. Arkin
and G. M. Maruyama, "Attribution, Affect and College Exam Perfor
mance," Journal ofEducation Psychology 71 (1979), 85-93; H.M. Dawes
and WG. Stephan, "Attributions for Exam Performance," Journal ofAp
pliedSocial Psychology 10 (1980); T.M. Gilmourand D.W Reid, "Locus of
Control and Causal Attribution for Positive and Negative Outcomes on
University Examinations," Journal ofResearch in Personality 13 (1979),
154-160; RM.Arkin, H. Cooper, andT. Kolditz, "AStatisticalReviewofthe
LiteratureConcerningSelf-ServingAttributionBiasinInterpersonal Situa
tions,"Journal ofPersonality48(1980), 435--448; D.T. Millerand M. Ross,
"Self-Serving Biases in the AttributionofCausality: FactorFiction?"Psy
chologicalBulletin82 (1975), 213-225; R NisbettandL. Ross,Human In
ference: Strategies and Shortcomings ofSocial Judgment (Englewood
Cliffs, NJ: Prentice-Hall, 1980).
46. T. Gilovich, "Biased Evaluation and Persistence in Gambling," Journal of
Personality andSocialPsychology 44 (1983), 1110-1126; T. Gilovich and C.
Douglas, "Biased Evaluations of Randomly Determined Gambling Out
comes,"Journal ofExperimentalSocialPsychology22(1986), 228-241.
47. R. Nisbett and L. Ross, Human Inference: Strategies and Shortcomings of
SocialJUdgment (EnglewoodCliffs, NJ: Prentice-Hall, 1980).



==================================================
                     PAGE 500                     
==================================================

484 NOTES
48. John R Nofsinger, Investment Madness (Upper Saddle River, NJ: Prentice
Hall,2001).
49. D. Dreman, Contrarian Investment Strategies: The Next Genemtion (New
York: Simon& Schuster, 1998)80,regardingastudybyPaulSlovicdiscussed
inaspeechbeforetheIGRF, May1973,entitled"BehavioralProblemsAdher
ingtoaDecisionPolicy."
50. J.E. Russo and PJ.H. Schoemakerrefer to tlle following studies: E. Langer,
"The Illusion ofControl," Journal ojPersonality and Social Psychology 32
(1975), 311-328; L.C. Perlmuter and RA. Monty, "The Importance of Per
ceived Control: Fact or Fantasy?," Ammican Scientist 65 (November
December1977),759-765.
51. John R Nofsingerreferences P Presson and V. Benassi, "Illusion ofControl:
A Meta-Analytic Review," Journal ofSocial Behavi01' and Personality 11,
no.3(1996), 493-510.
52. B. Fischoff, "Hindsight Is Not Equal to Foresight: The Effect of Outcome
Knowledge onJudgmentunderUncertainty,"Journal ofExperimental Psy
chology:HumanPerceptionandPerformance 1(1975),288-299; B. Fischoff
and R Beyth, "I Knew It Would Happen-Remembered Probabilities of
Once-Future Things," Organizational Behavior and Human Performance
13 (1975); B. Fischoff, "ForThose Condenmed to Study the Past: Heuristics
and BiasesinHindsight,"inJudgment under'Uncm·tainty:Heur'istics & Bi
ases, D. Kahneman, P Slovic, and A. Tversky (Eds.) (Cambridge, UK: Cam
bridgeUniversityPress, 1982),201-208.
53. GaryL. WellsandElizabethF. Loftus,Eyewitness Testimony: Psychological
Perspectives (Cambridge, MA: Harvard University Press, 1984), and Eugene
Winograd, "WhatYou Should Know about Eyewitness Testimony," Contern
pomryPsychology31, no. 5(1986),332-334.
54. Baruch Fischoff, "Debiasingin a Research Paperfor tlle OfficeofNaval Re
search,"inJudgment under" Uncer·tainty: Heuristics andBiases, D. Kalme
man, P Slovic, andA. Tversky(Eds.) (Cambridge, K: CambridgeUniversity
Press, 1982),Chapter31.
55. RF. PoW.andB.Gawlik,"HindsightBiasandtheMisinformationEffect:Sep
arating Blended Recollections from Other Recollection Types," Mern01Y 3,
no.l (March 1995), 21-55; D. Stahlberg and A. Maass, "Hindsight Bias: Im
pairedMemoryorBiasedReconstruction?,"EuropeanReviewojSocialPsy
chology(1998).
56. R HastieandRM. Dawes,RationalChoiceinan Uncm·tain World: ThePsy
chology ofJudgment and DecisionMaking, Chapter7, "Judging by Scenar
ios and Explanations" (Thousand Oaks, CA: Sage Publications, 2001); RC.
Shank and R P Abelson "Knowledge and Memory: The Real Story," in Ad
vances in Social Cognition, vol. 8, R Weyer(Ed.) (Hillsdale, NJ: Lawrence
Earlbaum, 1995), 1-86.
57. RE. Nisbett,E. Borgiada, R Rich Crandall, andH. Reed, "PopularInduction:
Information is Not Necessarily Informative," Chapter 7in Judgment under'
UnceTtainty: Heuristics andBiases,D. Kalmeman, P Slovic,andA.Tversky
(Eds.)(Cambridge, UK: CambridgeUniversityPress, 1982).
58. B. Russell,Philosophy(NewYork: Norton, 1927).



==================================================
                     PAGE 501                     
==================================================

Notes 485
59. Gilovich,How WeKnow, 91.
60. HastieandDawes,RationalChoice, Chapter7.
61. Ibid., 134.
62. Ibid., 135.
63. Ibid., 136, referring to N. Pennington and R. Hastie, "A Cognitive Theory of
Juror Decision Makjng: The Story Model," Cardozo Law Review 13 (1991),
519-557.
64. Gilovich,How WeKnow, 105.
65. For a definitive explanation of Elliott wave theory, see R. Prechter, Elliott
WavePrinciple: Key to StockMarket Profits (Gainesville, GA: NewClassics
Library, 1978).
66. M. Livo, The Golden Ratio: The St017J ofPhi, the Worlds Most Astonishing
Number(NewYork: BroadwayBooks,2002).
67. In a recent book, Socionomics (New York: New Classics Library, 2003),
Prechterdescribesan objectivecomputerizedversionofEWP thathasbeen
realized as an expertsystem. Prechterasserts thatthesystemhasproduced
profitablesignalsand should be able to distinguish real pricehistoriesfrom
pseudo-price histories generated by a random process, though as ofJune
2006,theseresults havenotbeenmadepublic.
68. Thiscriticismwouldbenullifiedifanobjectivemethodofwavecountingcan
demonstrate the ability to distinguish random data series from authentic
price data and predict betterthan an appropriate random benchmark. I re
cently learned that an objective EWP algorithm has been developed by
Prechter's finn and iscurrentlybei.ng tested. This program, called EWAVES,
which is discussed in Precther's 1999 book The Wave Principle ofHuman
SocialBehaviora.nd theNew Science ofSoC"ionomics. ThoughEWAVEShas
not yet been tested as to its ability to discriminate real from random price
data,iftheprogramwereabletodoso,itwouldprovideinlpOrtantconfinna
tion that Elliott's original insights about market behavior have merit. This
test, however, would notestablishthe predictivepowerofthe objectivesig
nals.AccordingtoPrechter,aseparatetestofthisassertionisalsobeingcar
ried out.
69. Afalsifiablepredictionisonethatissufficientlyspecificsuchthaterrorscan
beclearlyidentified. Forexample,thesuccessorfailure ofasignalissuedby
an objectiverule can bedetermined. Iteithermadeorlostmoney. Forecasts
ofa continuous target variable, such as in a regression model, are typically
evaluated bya lossfunction, suchassquared orabsolute error, where error
isequaltopredictedminusactual.
70. Hulbert's Financial Digest is an independent newsletter that evaluates
the performance ofotherfinancial advisory newsletters. According to the
July2005long-termperformanceratings, the annualized returnsearned by
the best known newsletter based on Elliott wave analysis, The Elliott
WaveFinancialForecast, using bothitslongand shortsignalsisgivenas:
+1.1 percentfor 5years +1.1 percent,-28.2percentfor 10years,-25.9per
centfor 15years, and -18.2 percentfor the duration ofthe life ofthe ser
vice since 12/31/84. For the same time periods, Hulbert's gives the
following annualized returns for the DowJones Wilshire 5000 index with



==================================================
                     PAGE 502                     
==================================================

486 NOTES
dividends reinvested: -1.3 percent for 5 years, +10 percent for 10 years,
and +10.7 percent for 15 years. It should be pointed out that a buy and
hold in a marketindex may not be a suitable benchmark for a timingser
vice thatassumes both longand shortpositions. Perrecommendations in
Chapter1, abenchmarkthatrepresents the null hypothesisfor testingthe
significance of a long/short signals returns is a random signal with the
sameproportionoflong/shortpositions.
71. Regardingthe author'sself-interestinbooksales, making the message more
compellingservesthisinterest.TryasImight, Icannotcompletelyeliminate
biasfrom decidingwhattosayornotsay. Ultimately,TA's bestprotection is
objective evidence evaluated in an objective fashion. In Part 1\\'0 of this
book, evidence regarding the efficacy of4,500TA rules will be presented in
asunvarnishedandobjectivewayIas knowhow.
72. David G. Myers, Intuition, Its Powers andPerits (New Haven: Yale Univer
sityPress,2004), 116.
73. Research articles on the confirmation bias can befound in PC. Wason, "On
the Failureto EliminateHypothesesina ConceptualTask," QuarterlyJour
nal ofExperimental Psychology 12 (1960), 129-140; K. Klayman and PC.
Wason, "Reasoningabouta Rule," Quarterly Journal ofExperimentalPsy
chology 20 (1968), 273-281; H.J. Einhorn and RM. Hogarth, "Confidence in
Judgment: Persistence in the Illusion ofValidity," Psychological Review 85
(1978), 395-416; J. Klayman and Y.-w. Ha, "Confirmation, Disconfirn1ation,
and Information in Hypothesis Testing," Psychological Review 94, no. 2
(1987),211-228.
74. R Park, Voodoo Science: The Roadfrom Foolishness to Fraud (New York:
OxfordUniversityPress,2000).
75. G.T. Wilson and D. Abrams, "Effects ofAlcohol on SocialAnxiety and Psy
chologicalArousal: CognitiveversusPharmacologicalProcesses,"Cognitive
Researchand Therapy 1(1975), 195-210.
76. Gilovich,How WeKnow, 50.
77. M.Jonesand R Sugden,"PositiveConfirmationBiasintheAcquisitionofIn
formation," Theory andDecision50(2001),59-99.
78. L. Festinger, A Theory of Cognitive Dissonance (Palo Alto, CA: Stanford
UniversityPress, 1957).
79. S.Pious, ThePsychologyofJudgmentandDecisionMaking (NewYork: Mc
Graw-Hill, 1993),23.
80. Practitioners who actually trade on theirpredictions do get clear feedback
on results, butaspointed outinthis chapterthisfeedback can bediluted by
othercognitivedistortions(e.g.,self-attributionbias).
81. Abulltrap, orbreakoutfailure, occurswhenshortlyafterthebreakoutpene
tration occurs, prices repenetrate the breakout level in the opposite direc
tion.Thishasbeenreferredtoasaturtlesoupsignalbecauseacontingentof
commodity traders called the Turtles, who trade on the basis ofbreakouts,
find themselvesinhotwaterwhenbreakoutfailures occur.
82. Gilovich,How WeKnow, 58.
83. Gilovichcitesthis from M. Gazzaniga, The Social Brain (NewYork: Harper
& Row, 1985); RE. Nisbett and T.D. Wilson, "Telling More Than We Can



==================================================
                     PAGE 503                     
==================================================

Notes 487
Know: Verbal Reports on Mental Process,"Psychological Review 84 (1977),
231-259.
84. T. Plummer,ForecastingFinancialMarkets: ThePsychologicalDynamicsof
Successful Investing, 3rd ed. (London: Kogan Page Limited, 1998). Plummer
describes waves patterns similar to EWP but sometimes there are three
wavesandsometimesfive.
85. Gilovich,How WeKnow, 53.
86. Ibid., 54.
87. C.G. Lord, 1. Ross, and M.R. Lepper, "Biased Assimilation and Attitude
Polarization: The Effects of Prior Theories on Subsequently Considered
Evidence," Journa,l of Personality and Social Psychology 22 (1979),
228-241.
88. Gilovich,How We Know, 54.
89. Ibid., 54.
90. T. Gilovich, "Biased Evaluation and Persistence in Gambling," Journa.l of
Personality and SocialPsychology 44 (1983), 1110-1126;T. Gilovichand C.
Douglas,"BiasedEvaluationofRandomlyDeterminedGamblingOutcomes,"
Journal ofExperimentalSocialPsychology22 (1986),228-241.
91. This refers to the situationwhere a subjective method orsome aspect ofit
hasbeenobjectified. Forexample,astudyofthehead-and-shoulderspattern
iscitedinChapter3thatprovidesanexampleofasubjectivepatternthathas
beenmade testable, atleastinoneform. The evidencefrom thatstudy con
tradictstheefficacyofthehead-and-shoulderspattern.
92. 1. Ross, M.R. Lepper, and M. Hubbard, "Perseverance in Self Perception
and Social Perception: Biased Attributional Processes in the Debriefing
Paradigm," Journal of Personality and Social Psychology 32 (1975),
880-892.
93. D.L.Jennings, M.R. Lepper, and L. Ross, "Persistence ofImpressionsofPer
sonalPersuasiveness: Perseverance ofErroneous SelfAssessments Outside
the Debriefing Paradigm," unpublished manuscript, Stanford University,
1980; M.R. Lepper, L. Ross, and R. Lau, "Persistence ofInaccurate and Dis
credited Personal Impressions: AField Demonstration ofAttributional Per
severance,"unpublishedmanuscript,StanfordUniversity, 1979.
94. Failure would be if artificially created random charts cannot be discrimi
natedfrom authenticpricechartsandifEWPsignalsdonotgenerateprofits
superior to a random signals. Even ifEWP signals prove to be useless on
theirown,theyhaveproventohavevalueinamultivariatemodelduetosyn
ergieswithotherindicators.
95. Prechter,Socionomics, Chapter4.
96. Gilovich,How WeKnow, 22.
97. 1.Ross, M. Lepper,F. Strack,andJ.1. Steinmetz,"SocialExplanationandSo
cial Expectation: The Effects ofReal and Hypothetical Explanations upon
Subjective Likelihood," Journal ofPersonality and Social Psychology 35
(1977),817-829.
98. IshouldpointoutthatinwritingthisbookIhave mademyselfvulnerable to
this very problem. Explaining my position may make me resistant to evi
dencethatinvalidatessaidpositions.



==================================================
                     PAGE 504                     
==================================================

488 NOTES
99. This study was quoted by Michael Shermer in Why People Believe Weird
Things. Shermeralso discussesthephenomenonofideologicalimmunityas
definedbysocialscientistJayStuartSnelson.
100. J.S. Snelson, "The Ideological Immune System," Skeptic Magazine 1, no. 4
(1993),44-55.
101. C.A. Anderson, M.R Lepper, and L. Ross, "ThePerseverance ofSocialThe
ories: The Role ofExplanation in the Persistence ofDiscredited Informa
tion," Journal of Personality and Social Psychology 39, no. 6 (1980),
1037-1049.
102. Note, for purposes of this discussion the patterns, signals, and their ex
pectedoutcomesarethoseemployedbysubjectiveanalysts. Becauseweare
dealingwith theproblemoffaultyperceptions, whatmattersis the analyst's
subjectiveperceptionsofillusorycorrelations.
103. Accordingtothesubjectiveinterpretationoftheanalyst.
104. As reported in T. Gilovich, L. Allen, and H.M. Jenkins, "The Judgment of
ContingencyandtheNatureofResponseAlternatives,"CanadianJournal
ofPsychology34(1980), 1-11;R Beyth-Marom,"PerceptionofCorrelation
Reexamined," Memory & Cognition 10 (1982), 511-519; J. Crocker, "Bi
ased Questions inJudgmentofCovariation Studies,"Personality and So
cial Psychology Bulletin (1982), 214-220; H.M. Jenkins and WC. Ward,
"Judgments of Contingency between Responses and Outcomes," Psycho
logicalMonographs: General andApplied 79, no. 594 (1965), entire publi
cation; WD. Ward and H.M. Jenkins, "The Display ofInformation and the
Judgment of Contingency," Canadian Journal ofPsychology 19 (1965),
231-241.
105. Afalse positive signal occurs when the signal occurs butthe expected out
come does not occur. Afalse negativesignal (failure tosignal) occurswhen
the pattern/signal does not occur butthe outcome thesignal/pattern is pur
portedtopredictoccursanyway.
106. Infactsomestudiesshowthatpeoplealsoconsidertheinstancesfallinginthe
lower-rightcell (correctidentificationofnoneventsornonopportunities), but
byandlargemostattentionispaidtothosecasesfallingintheupper-leftcelL
107. Gilovich,.How WeKnow,30.Here,Gilovichmakesreferencetoanearlierar
ticle: H.J. Einhorn and RM. Hogarth, "Confidence inJudgment: Persistence
oftheIllusionofValidity,"PsychologicalReview85(1977),395-416.
108. The following studies discuss aspects of illusory correlation: R Beyth
Marom, "Perception ofCorrelation Reexamined," Memory & Cognition 10
(1982),511-519;J. Crocker, "JudgmentofCovariationbySocialPerceivers,"
PsychologicalBulletin 90 (1981), 272-292; J. Crocker, "Biased Questions in
Judgment ofCovariation Studies," Personality and Social Psychology Bul
letin8(1982), 214-220;H.M.Jenkinsand WC. Ward, "JudgementsofContin
gency between Responses and Outcomes," Psychological Monographs:
General and Applied 79, no. 594 (1965), entire issue; J. Smedslund, "The
Concept of Correlation in Adults," Scandinavian Journal ofPsychology 4
(1963), 165-173; WD. Ward and H. H. Jenkins, "The Display ofInformation
and the Judgment of Contingency," Canadian Journal of Psychology 19
(1965),231-241.



==================================================
                     PAGE 505                     
==================================================

Notes 489
109. D.L.Jennings,T.M. Amabile, and L. Ross, "lnfornlalCovariationAssessment:
Data-Based versus Theory-Based Judgments," Chapter 15 in Judgment un
der Uncertainty: Heuristics and Biases, D. Kalmeman, P Slovic, and A.
Tversky(Eds.)(Cambridge,UK: CambridgeUniversityPress, 1982),220-222.
1l0. The variablesin theseexperimentswere continuous, meaningthatthey can
assumeanyvalue within a range, like temperature. Thusfar, Ihave beenre
ferring tobinaryvariables thatassumeonlytwopossiblevalues: patternpre
sent orpatternnotpresent.
111. Jennings, Amabile,and Ross, "InformalCovariationAssessment."
112. Smedslund,"ConceptofCorrelationinAdults."
113. J. Baron, Thinking and Deciding, 3rd ed. (Cambridge, UK: Cambridge Uni
versity Press, 2000), 177. Baron citesthe following studies assupportive: H.
ShakleeandD. Tucker, "Aruleanalysisofjudgmentsofcovariationbetween
events," Memory and Cognition 8 (1980), 459-467; M.W. Schustak and RJ.
Sternberg, "EvaluationofEvidence inCausalInference,"Journal ofExperi
mental Psychology; Geneml 110 (1981); H. Shaklee and M. Mims, "Sources
ofErrorinJudging EventCovariations," Journal ofExperimentalPsychol
ogy: Learning, Memory and Cognition 8 (1982), 208-224; H.R Arkes and
A.R Harkness, "Estimates ofContingency between1\vo Dichotomous Vari
ables,"Journal ofExperimentalPsychology: General 112(1983), 117-135.
114. Fora discussion ofthe chi-square test, see RS. Witte and J.S. Witte, Statis-
tics, 7thed. (Hoboken, NJ:John Wiley& Sons,2004),.469-492.
115. Signijicantinastatisticalsenseisatermthatwillbedefined inChapter5.
116. Gilovich,How WeKnow,32.
117. Gilovichcitesaseriesofstudiesthatdiscussproblemofillusorycorrelations
in connection with asymmetric binary variables: RH. Fazio, S.J. Sherman,
and PM. Herr, "The Feature-Positive Effect in the Self-Perception Process:
Does NotDoingMatterasMuchas Doing?,"Journal ofPersonality andSo
cial Psychology 42 (1982), 404-411; H.M. Jenkins and RS. Sainsbury, "Dis
crimination Learning with the Distinctive Feature on Positive or Negative
Trials," in Attention: Contemporary Theory and Analysis, D. Mostofsky
(Ed.) (New York: Appleton-Century-Crofts, 1970); J. Newman, w.T. Wolff,
and E. Hearst, "TheFeaturePositive-EffectinAdultHumanSubjects,"Jour
nal ofExperimental Psychology: Human Learning; and Memory 6 (1980),
630-650; R Nisbetand L. Ross,HumanIrlje-rence: StrategiesandShortcom
ings ofSocial Judgment (Englewood Cliffs, NJ:: Prentice-Hall, 1980); PC.
Wason and PN. Johnson-Laird, Psychology of Reasoning: Structure and
Content(London:Batsford, 1965).
118. Infactvaguepattern/signaldefinitionsand lackofobjectiveevaluationcrite
riafacilitate thediscoveryofseeminglyconfirmatoryinstances.
119. Gilovich,How WeKnow, 21.
120. A. Tverskyand I. Gati, "StudiesinSimilarity," in Cognition and Categoriza
tion, E. Roschand B. Loyd (Eds.) (Hillsdale, NJ: LawrenceEarlbaum, 1978).
121. D. Bernstein, E.J. Clarke-Stewart,A. Roy, and C.D. Wickens,Psychology, 4th
ed. (Boston: HoughtonMifflin, 1997),208.
122. Reference to this study can be found at www.azwestern.edu/psy/
dgershaw/loVReinforceRandom.html.



==================================================
                     PAGE 506                     
==================================================

490 NOTES
123. Bernstein,Clarke-Steward, Roy, andWickens,Psychology, 208.
124. Gilovich,How WeKnow, 10.
125. www.datamininglab.comJ.
126. N. JegadeeshandS. Titman, "Returnsto BuyingWinnersand SellingLosers:
Implications for Stock Market Efficiency," Journal ofFinance 48 (1993),
65-91; N. Jegadeesh and S. Titman, "Profitability of Momentum Strategies:
An Evaluation ofAlternative Explanations," Journal ofFinance 56 (2001),
699-720.
127. F.D. Arditti, "CanAnalysts Distinguish between Real and Randomly Gener
ated Stock Prices?" Financial Analysts Journal 34, no. 6 (NovemberlDe
cember1978),70.
128. H.Y. Roberts, "StockMarket 'Patterns' and FinancialAnalysis: Methodologi
calSuggestions," TheJournal ofFinance 14,no. 1(March 1959), 1-10.
129. Ibid.
130. Roberts' examples did notincludevolumedata, whichsome claim is neces-
saryforpatternidentification.
131. Arditti, "CanAnalystsDistinguish."
132. J. Siegel,Stocksfor theLongRun,3rded. (NewYork: McGraw-Hill,2002).
133. Ibid.,286-288.
134. There arespecific methods instatisticssuchas the runs test thatdetectde-
parturesfromrandombehavior.
135. Gilovich,How WeKnow.
136. Ibid., 14-15.
137. HastieandDawes,RationalChoice, 4.
138. D.G. Myers, Intuition: Its Powers and Perils (New Haven: Yale University
Press,2004), 1.
139. Ibid., 4.
140. Forexample, ifwealways predictourgradeonanexamto behigherthan it
turns outto be. An unbiasedjudgmentcan also bewrong, butits errors are
not always on the same side ofthe true answer-for example, ifyou pre
dictedagradethatwassometimesaboveandsometimebelowthegradeyou
finallyreceived.
141. Uncertainsituationsarethose whoseoutcomesaredifficultorimpossibleto
predict.
142. B. Malkiel, A Random Walk Down Wall Street (New York: w.w. Norton,
1973).
143. There is a school ofthoughtwithinprobabilitytheory that defines probabil
ityasthepriorrelativefrequency, whererelativefrequency is thenumberof
pasteventsdividedbythetotalnumberofinstancesinwhichtheeventcould
havetakenplace.WetakeupthistopicingreaterdetailinChapters4and5.
144. This is true if events are independent. However, if events are dependent,
thenthe occurrenceofan eventmakesanothermorelikely. Forexample,an
earthquakeistypicallyfollowed byaseriesofsmallerquakesoraftershocks.
Plane crashes are independent events-the occurrence ofone has no bear
ingontheoccurrenceofanother.
145. A. TverskyandD. Kahneman, "JudgementunderUncertainty: Heuristicsand
Biases,"Science 185(1974), 1124-1131.



==================================================
                     PAGE 507                     
==================================================

Notes 491
146. Avalid classtraitisonethatis trulyindicativeofclassmembership. Often
the most obvious traits give valid signals of class membership, but not
always.
147. HastieandDawes,RationalChoice, 116.
148. Fora description ofBayes' law see S. Kachigan, Statistical Analysis (New
York: RadiusPress, 1986),476-482,andWikipedia,theonlinefree encyclope
dia,seereferencetoBayes'theorem,8pages.
149. Aconjunction refers to the logical operatorAND; for example, the entity is
ananimalANDahorse.
150. A. Tverskyand D. Kahneman,"Judgmentsofand byRepresentativeness," in
JUdgment Under Uncertainty: Heuristics and Biases, D. Kahneman, P.
Siovic, and A. Tversky(Eds.) (Cambridge, UK: CambridgeUniversityPress,
1982).
151. TheprobabilityofaseriesofcharacteristicsconnectedbytheANDoperator
is equal to the product ofthe probability ofeach item in the list. Since the
probabilityofanyitemislessthanone,aproductoftermsallofwhichhavea
valuelessthan 1.0becomessmallerasthelistgrowslarger. ForexampleifX,
Y,andZareindependentcharacteristics, andtheprobabilityofsomeonepos
sessingcharacteristicX=0.5,theprobabilityofY=0.5andtheprobabilityof
Z=0.5, then the probabilitysomeone possesses bothXand Y=0.5 X 0.5 or
0.25,andtheprobabilityofXandYandZ=0.5 X 0.5 X 0.5is0.125.
152. Thisexample comesfrom S. Pious, ThePsychology ofJudgment andDeci
sionMaking (NewYork: McGraw-Hill, 1993), 111.
153. Nassim NicholasTaleb,Fooled byRandomness: TheHiddenRoleofChance
intheMarkets andinLife, (NewYork:Texere,2001).
154. This is the two-standard-deviation confidence interval for the proportion of
headsin 100,000cointosses.
155. With regard to people'sfaulty expectations about how a tossed coinshould
behave, Gilovich refers to the followingstudies: R. Falk, "The Perceptionof
Randomness," Proceedings, 5th International Conferencefor the Psychol
ogy ofMathematics Education (Grenoble, France: 1981); W.A. Wagenaar,
"GenerationofRandom SequencesbyHuman Subjects: ACriticalSurveyof
Literatures," Psychological Bulletin 77 (1972), 65-2; D. Kahneman and A.
Tversky, "Subjective Probability: A Judgment of Representativeness," in
Judgment under Uncertainty: Heuristics and Biases, D. Kahneman, P.
Siovic, and A. Tversky(Eds.) (Cambridge, UK: Cambridge UniversityPress,
1982),Chapter3.
156. Theprobabilityofeithersequenceis.056, or0.015626.
157. Gilovich,How WeKnow, 15.
158. Ibid.,20.
CHAPTER 3 The Scientific Method and
Technical Analysis
1. M. Shermer, Why People Believe Weird Things: Pseudoscience, Supersti
tion, andOtherConfusionsofOurTime(NewYork: W.H. Freeman, 1997).



==================================================
                     PAGE 508                     
==================================================

492 NOTES
2. C. Van Doren, A History ofKnowledge: Past, Present, and Future (New
York: BallantineBooks, 1991).
3. S. Richards, Philosophy &Sociology ofScience: An Introduction (Oxford,
UK: BasilBlackwell, 1983),45.
4. Ibid.
5. Ibid., 189.
6. Ibid., 201.
7. AsquotedbyShermer, WhyPeopleBelieve.
8. VanDoren,History ofKnowledge, 189.
9. Ibid..
10. Richards,Philosophy &SociologyofScience, 14.
11. D.J.Bennett,LogicMadeEasy:How toKnow WhenLanguageDeceives You
(NewYork: w.w. Norton, 2004),30.
12. Ibid.,31.
13. Ibid.,99and 108;referringtoPC. Wason, "Self-Contradictions,"in Thinking:
Readings in Cognitive Science (Cambridge, UK: Cambridge University
Press, 1977), 114-128; S.S. Epp, "A CognitiveApproach to Teaching Logic,"
DIMACSSymposium, TeachingLogicandReasoninginanmogicalWorld,
Rutgers, The State University of New Jersey, July 25-26, 1996, available at
www.cs.comell.edulInfolPeople/gries/symposiumlsymp.htm.
14. The fact thata given set ofobservations can be consistentwith (explained
by)morethanonetheoryorhypothesisandperhapsaninfinitenumberofal
ternativetheoriesisa problemknown inphilosophicaland scientific circles
as the underdetermination oftheoriesproblem. Becausea multitude ofhy
potheses can be consistentwith the same setofobservations, the hypothe
sesaresaidtobeempiricallyidentical. Theimplicationisthatobservational
data, onitsown,cannottelluswhichofthemiscorrect.
15. F. Schauer,ProjUes, ProbabilitiesandStereotypes(Cambridge,MA: Belknap
PressofHarvard,2003), 59. Referencestostudiesondogbreedbehaviorare
foundinfootnote 7.
16. In a subsequent chapter it will be shown that in this case the uncertainty
would be 10 times greater with 10 observations than with 1,000 if price
changesconformtothenormaldistribution, butmaybefarmoreuncertainif
theydonot.
17. http://en.wikipedia.orglwikilPhilosophy_oCscience.
18. Shermer, WhyPeopleBelieve,24.
19. Richards,Philosophy & SociologyofScience, 45.
20. B.L. Silver,TheAscentofScience(NewYork: OxfordUniversityPress, 1998),
15.
21. Ibid., 14.
22. RS. Percival,AboutKarl Popper, adapted from his PhD thesis, available at
www.eeng.dcu.ie/-tkpw/intro_popper/intro---.popper.html.
23. Thisisthefrequency definitionofprobability.Thereare others.
24. Richards,Philosophy&Sociology ofScience, 52.
25. K.R Popper, TheLogicofScientificDiscovery(London: Hutchinson, 1977).
26. K.R Popper, ConJectures andRefutations(London: Routledge& Kegan 1972).
27. Thisisknownastheunderdeterrninationoftheoriesproblem.



==================================================
                     PAGE 509                     
==================================================

Notes 493
28. Ifhypothesis is true, thenX should beobserved. X is observed(affirms the
consequent). Invalidconclusion: The hypothesisis true(fallacyofaffirming
theconsequent).
29. Anecessary condition is one that is required to produce an effect, but it
alone is not enough to produce the effect. For example ifXis a necessary
conditionofY, itmeansthat, ifXislacking,Ywillnotoccurbutthefact that
Xis presentis notenough to insure Y. In shorthand: IfnotX, then not Y. A
suJficientconditionismorecomprehensiveinthatitspresenceisenough(is
sufficient)toproducetheeffect.Thusitmeans:IfW, then Y.
30. Richards,Philosophy &SociologyofScience, 59.
31. My contention may be challenged in the nearfuture byan objectiveversion
ofEWTcalledEWAVESdevelopedbyRobertPrechter'sfirm, ElliottWaveIn
ternational. Prechterclaimsitgeneratesdefinitivesignalsforwhichfinancial
performancecanbemeasured.
32. Descriptions ofEMH-weak vary. Some sayonlyhistorical price data cannot
be used to generated excess returns. Other versions say any stale market
data,price,volume, orany otherdataseriesthatTApractitionersstudycan
notbe used togeneratedexcessreturns.
33. N. Jegadeeshand S. Titman, "Returnsto Buying Winnersand SellingLosers:
Implications for Stock Market Efficiency," Journal of Finance 56 (1993),
699-720.
34. R.A. Haugen, The IneJficientStockMarket: What Pays Offand Why (Upper
SaddleRiver, NJ: Prentice-Hall, 1999),63.
35. F. Nicholson, "Price-Earnings Ratios in Relation to Investment Results," Fi
nancialAnalystsJournal(JanuarylFebruary1968),105-109;J.D. McWillianls,
"PricesandPrice-EarningsRatios,"FinancialAnalystsJournal22(May/June,
1966), 136-142.
36. Becausetherearenotinfinitetimeandresourcestotestallideas,receptivity
is tempered with prior knowledge, experience, and intuition. Some new
ideas meritmoreattention than others. However, being overlyrestrictive at
this phase could have a bigcost-missinga breakthrough. In this phase, if
oneistoerr,itisbettertoerronthesideofopenness.
37. W.A. Wallis and H.Y. Roberts, Statistics: A New Approach (New York: The
FreePressofGlencoe, 1964),5.
38. pw. Bridgman, "The Prospect for Intelligence," Yale Review 34 (1945),
444-461,quotedinJ.B.Conant,OnUnderstandingScience(NewHaven: Yale
UniversityPress, 1947), 115.
39. Wallisand Roberts,.Statistics,6.
40. J.E. RussoandPJ.H.Schoemaker,Decision Traps: The TenBarrierstoBril
liantDecision-MakingandHow to Overcome Them(NewYork: Doubleday,
1989),135.
41. L.R. Goldberg, "ManversusModelofMan: ARationale, PlusSomeEvidence
for a Method of Improving Clinical Inference," Psychological Bulletin 73
(1970),422-432.
42. PH.K. Chang and C.L. Osler, "Methodical Madness: Technical Analysis and
the Irrationality of Exchange-Rate Forecasts," Economic Journal,109, no.
458 (1999), 636-661; C.L. Osler, "Identifying Noise Traders: The Head-and-



==================================================
                     PAGE 510                     
==================================================

494 NOTES
ShouldersPattern in U.S. Equities," Publication ofFederal Reserve Bankof
NewYork, 1998.
43. R.D. Edwards, J. Magee, and WH.C. Bassetti (Eds.), Technical Analysis of
Stock Trends,8thed.,(BocaRaton, FL:CRCPress,2001);M.J. Pring,Technical
AnalysisExplained,4thed. (NewYork: McGraW-Hill,2002);J.J.Murphy,Tech
nicalAnalysisoftheFinancialMarkets:A ComprehensiveGuideto Trading
MethodsandApplications(NewYork: NewYorkInstituteofFinance,1999).
44. It may be possible to define chart patterns using fuzzy logic where class
membershipisgivenaclassmembershipfunction (0to 1.0)ratherthanayes
orno. However,evaluatingpatterneffectivenesswillstillrequireathreshold
ontheclassmembershipfunctiontodecidewhichpatternsbelonginagiven
classandwhichdonot.
45. S.S.Alexander,"PriceMovementsinSpeculativeMarkets:TrendsorRandom
Walks," Industrial Management Review 2 (1961), 7-26; S.S. Alexander,
"Price Movementsin SpeculativeMarkets: TrendsorRandom Walks No.2,"
IndustrialManagementReview5(1964), 2~6.
46. AA Merrill, Filtered Waves: Basic Theory (Chappaqua, NY: Analysis Press,
1977).
47. PH.K.Chang, and C.L. Osler, "Methodical Madness: Technical Analysis and
theIrrationalityofExchange-RateForecasts,"EconomicJournal 109,no. 458
(1999),63tH>61.
48. C.L. Osler, "Identifying Noise Traders: The Head-and-Shoulders Pattern in
U.S. Equities,"PublicationofFederalReserveBankofNewYork, 1998.
49. AW Lo, H. Marnaysky, and W Jiang, "Foundations of Technical Analysis:
ComputationalAlgorithms, StatisticalInference, and EmpiricalImplementa
tion,"Journal ofFinance55,4(August2000), 1705-1765.
50. Kernel regression estimates the value of a smoothed curve by averaging
nearby or local observations, where the weight of each observation is in
verselyrelatedto itsdistancefrom the locationfor whicha smoothedvalue
isdesired.Thus, nearbyobservationsaregiventhegreatestweight.Theterm
kernel refers to the shape of the weighting function used. One cornmon
weightfunction isbased ontheshape ofthe normal probabilitydistribution
(Gaussiankernel).
51. The authors ofthe paperdo not corne outdirectly and say thepatterns are
notusefulfor forecasting trends, butthe datatheypresentindicatesthis as
doesthecommentaryonthepaperprovidedbyNarasimahanJegadeesh(pp.
1765-1770). In Table 1he shows that the post pattern returns (trends) are
notsignificantlydifferentthanwhenthepatternsarenotpresent.
52. T.N. Bulkowski, Encyclopedia ofChart Patterns (New York: John Wiley &
Sons,2000),262-304.
CHAPTER 4 Statistical Analysis
1. David S. Moore, TheBasicPractice ofStatistics (New York: WH. Freeman,
1999).
2. B. Russell, A History ofWestern Philosophy (New York: Simon&Schuster,
1945).



==================================================
                     PAGE 511                     
==================================================

Notes 495
3. R.P. Abelson, Statistics as Principled Argument (Mahwah, NJ: Lawrence
Erlbaum, 1995).
4. A.w. Wallis and H.Y. Roberts, Statistics: A New Approach (New York: The
FreePressofGlencoe, 1956), 101-110.
5. The method for detrending or zero normalizing the market data series to
whichtherulesareappliedisdiscussedinChapter1.
6. Probability density is formally defined as the first derivative ofthe cumula
tive probabilityofa randomvariable. Forthe purposesofthis discussion, it
canbethoughtofaschanceorprobabilitythatsomethingwillhappen.
7. Fractionalareareferstotheproportionoftheareaofthedensityfunctionly
ing to the right ofthe observed performance relative to the total area cov
ered bythedensityfunction.
8. Instances ofdogs missing limbs are excluded from this discussion for pur
posesofsimplification.
9. Wallisand Roberts,Statistics, 101-110.
10. D.S. Moore, The Basic Practice of Statistics (New York: W.H. Freeman,
1999).
11. S. Kachigan,Sta,tisticalAnalysis(NewYork: RadiusPress, 1986),77.
12. In thisexample, itisasswned thattheunderlyingphenomenonisstationary;
that is, it remains unchanged over time. Financial markets are most likely
nonstationary systems. In general, statistical inference is not intended to
copewith nonstationaryphenomena. Nonstationaryis a risk inherentwhen
investingon thebasisofanysortofrule,statisticallyderivedornot.
13. Daily returns refers to returns earned by a rule on zero-centered or de
trended marketdataasdefinedinChapter1.
14. S.J. Grossman andJ.E. Stiglitz, "Onthe ImpossibilityofInformationallyEffi
cientMarkets,"AmericanEconomicReview70,no.3(1980),393-408.
15. Chapter 7, Theories of Nonrandom Price Motion, discusses the possibility
thatTArulesmaybeprofitablebecausetheysignalopportunitiestoearnrisk
premiums.Underthisnotion,TAruleprofitsarecompensationtothetechni
cal traderforprovidingvaluableservices to othermarketparticipants, such
as liquidity, information in the form ofprice discovery, orrisk transference.
Becausetheseareongoingneeds,profitsearnedbyrulesthatresultfromful
fillingsaidneedsarelikelytopersist.
16. This definitionofstationary isnotstrictlycorrectbutclose enough for pur
posesofthisdiscussion.
17. N.N. Taleb, Fooled by Randomness-The Hidden Role of Chance in the
Markets andinLife(NewYork:Texere,2001),97.
18. The rigorous definition ofa statistic is a function ofthe cases in the sam
ple that does not depend on any unknown parameter. Thus, ifcomputing
thestatistic requires apiece ofinformation thatis unknown, thenitis not
astatistic.
19. The trimmed mean is computed after removing the largest 5 percent wins
and losses. This can provide a more accurate estimate of the population
meanbyremovingoutliersthatcanskewresults.
20. In fact, the Law of Large Numbers refers to the convergence between a
sampleaverageand apopulationaverage. However,arelativefrequency is



==================================================
                     PAGE 512                     
==================================================

496 NOTES
the same as an average ifthe occurrence ofan eventis given the value of
1 and the nonoccurrence a value of O. Relative frequency refers to the
proportion of event occurrences within an observed sample, whereas
the theoretical idea of probability refers to the relative frequency of the
event in the unlimited number oftrials that constitutes the population of
observations.
21. Kachigan,StatisticalAnalysis,75.
22. TheLawofLarge Numbersrequiresthatthe numberofobservationsineach
interval be sufficiently large to reveal the true relative frequency ofthe val
uesrepresentedbythatinterval.
23. Probability density is the slope ofthe cumulative distribution function. For
anypoint on the horizontal axis, the heightofthe probability density func
tion atthatpointisthefirst derivative ofthe probabilitythatan observation
willbelessthanorequaltothepointon thehorizontalaxis.
24. Forexample,theprobabilityofobtainingthevalue4.23iszero.Thisisnotso
strange becausea continuousrandom variable neverassumesavalueofex
actly4.23.Therealvalueiseitherslightlyhigherorlowerthan4.23,ifitwere
givenwithadditionalaccuracy(e.g.,4.229).
25. Infacteachindividualobservationthatwindsupin thesampleisaprobabil
ityexperiment.
26. A more technical definition ofsample statistic: A function ofone or more
random variables that does not depend on any unknown parameters. This
impliesthatastatisticisitselfarandomvariable.
27. Kachigan,StatisticalAnalysis, 101.
28. R.S. Witte and J.S. Witte, Statistics, 7th ed. (New York: John Wiley& Sons,
2004),230.
29. Other statistics include: Z, t, F-ratio, chi-square, and so forth. There are
many.
30. TheSharperatioisdefined asthe annualizedaverage returninexcessofthe
risk-freerate(e.g.,90-daytreasurybills)dividedbythestandarddeviationof
thereturnsstatedonanannualized basis.
31. Theprofitfactor isdefined as a ratio ofthesumofgainsofprofitabletrans
actions to thesum ofall losseson unprofitable transactions. The denomina
torisstatedasanabsolutevalue (nosign)sothevalueofthe profitfactoris
always positive. Avalue of1would indicate the rule is break even. Asupe
rior way to calculate the profitfactor to transforn1 it to have a natural zero
pointistotakethelogoftheratio.
32. The UlcerIndexisan alternativeand possiblysuperiormeasure ofrisk that
considers the magnitude ofequityretracements, whichare notdirectlycon
sidered by the Sharpe ratio. The standard deviation, the risk measure em
ployed by the Sharpe ratio, does not take into account the sequence of
winningand losingperiods. ForadefinitionoftheUlcerIndex,seePG. Mar
tin and RB. McCann, The Investor's Guide to Fidelity Funds (New York:
John Wiley & Sons, 1989), 75-79. A sinillar concept, the return to retrace
mentratio, isdescribedbyJ.D. Schwager,SchwageronFutures-Technical
Analysis(NewYork:John Wiley& Sons, 1996).



==================================================
                     PAGE 513                     
==================================================

Notes 497
33. Thisfigure wasinspired byasimilarfigure inLawrenceLapin, Statisticsfor
Modern Business Decisions, 2nd ed. (New York: Harcourt Brace Jo
vanovich, 1978),Figure6-10, 186.
34. The qualifyingcondition for the mean ofa sampleto approach the mean of
theparentpopulationisthatthemeanintheparentpopulationbefinite.
35. This is true when the test statistic is the sample mean, but it is not true in
general.Acriticalassumptionisthattheobservationscomprisingthesample
are independent. Ifthe observations are serially correlated, as might be the
case in a time series, the variance reduction is slowerthan the square root
rulesuggests.
36. Thepopulationhasafinite meanandafinitestandarddeviation.
37. The general principle that biggersamples are betterapplies to stationary
processes. In the context offinancial market time series, which are most
likely not stationary, old data may be irrelevant, or even misleading. For
example, indicators based on short selling volume by NYSE specialists
seem to have suffered a decline in predictive power. Larger samples may
notbe better.
38. The Central LimitTheorem applies to a sample for which observations are
drawn from the same parent population, which are independent, and in
whichthemeanandstandarddeviationoftheparentpopulationarefinite.
39. Thisfigure wasinspired byasimilarfigure inLawrenceLapin,Statisticsfor
Modern Business Decisions, 2nd ed. (New York: Harcourt Brace Jo
vanovich, 1978),216-217.
CHAPTER 5 Hypothesis Tests and
Confidence Intervals
1. J.E. Burtand G.M. Barber, Elementary Statisticsfor Geographers, 2nd ed.
(NewYork:TheGuilfordPress, 1996),5.
2. We saw in Chapter 2 that because ofthe self-attribution bias, people can
maintainadelusionbyrationalizingaloss: headache,shoelaceswereuntied,
arguedwith wifebeforegame, theotherplayerputanevileyeonme,andso
on. Distasteful evidence can always be explained away if loose evidence
standardsareapplied.
3. As discussed in Chapter 3, according to Popper and Hume, no amount of
confirmatoryevidencecanprovide certitudefor a claim. However, evidence
canbe usedtofalsifyaclaim. Forthisreason, asciencetestclaimsbyfocus
ingonevidence that has the potentialto showthat the claim isfalse rather
thanonevidencethatmightconfirmtheclaim.
4. R.S. Witte and J.S. Witte, Statistics, 7th ed. (Hoboken, NJ: John Wiley &
Sons,2004),264.
5. Ibid.
6. Eric W. Noreen, Computer Intensive Methods for Testing Hypotheses: An
Introduction (NewYork:JohnWiley& Sons, 1989),2.



==================================================
                     PAGE 514                     
==================================================

498 NOTES
7. David S. Moore, The Basic Practice ofStatistics, 2nd ed. (New York: W. H.
Freeman,2000),314.
8. As described in Chapter 1, detrending transforms the marketdatasuch that
it has a mean daily change ofzero. This eliminates the distortion in a rules
performance due to the conjoint effects of position bias and the market's
trendduringtheback-testperiod.
9. Randomlypairedmeansthatthehistoricaldailypricechangesofthe market
arescranlbled,andtheneachisrandomlypairedwitharuleoutputvalue(+1
or-1).
10. Quantmetrics,2214ElAmigoRoad, DelMar, CA92014.
11. B. Efron, "Bootstrap Methods: Another Look at the Jackknife," Annals of
Statistics 7(1979), 1-26.
12. E.W. Noreen in Computer Intensive Methods for Testing Hypotheses cites
thefollowing referencestobootstrapping: B. Efronand G. Gong, "ALeisurly
LookattheBootstrap,theJacknifeandCross-Validation,"AmericanStaisti
can37(February1984),36--48;B. Efron, "BetterBootstrapConfidenceInter
vals," LCS Technical Report No. 14, Department ofStatistics and Stanford
LinearAccelerator; B. Efronand R. Tibshirani, "BootstrapMethodsforStan
dard Errors, Confidence Intervals, and Other Methods of Statistical Accu
racy,"StatisticalScience 1(1984),54--77.
13. Here,orderedmeansthevaluesoccurintheirpropertemporalorder.
14. Achannel breakoutrule, undertraditionalTAinterpretation, registersa buy
(sell) signal when the rule's inputseries is ata maximum (minimum) value
forthecurrentdayandallpriordaysintherule'slook-backwindow.
15. L.J. Kazmier, StatisticalAnalysisfor Business andEconomics (New York:
McGraw-Hill, 1978),217.
16. Standarderrorreferstothestandarddeviationofsamplemeansaround the
populationmean when manyindependentsamplesare taken from thesanle
population.
17. Kazmier,StatisticalAnalysis,216.
18. Ibid.
CHAPTER 6 Data-Mining Bias: The Fool's Gold
ofObjective TA
1. H. White, "A Reality Check for Data Snooping," Econometrica 68, no. 5
(September2000), 1102,Proposition2.1. Theproofshowsthatas thesam
ple size goes to infinity, the probability that the rule with the highest ex
pected return will also have the highest observed performance
approaches 1.0.
2. R.N. Kalm, "DataMining MadeEasy: Seven QuantitativeInsightsintoActive
Management: Part 5," www.barra.com/Newsletter/NLl65/SevIns5NLI65.asp.
3. BiblecodescholarsclaimthatapredictionthatIsraeliPrimeMinsterYitzhak
Rabin would be assassinated was made in advance ofthe event. They also
claimthatefforts were made to warn Rabin, butto no avail. Ihave been un
abletoauthenticatetheveracityofthisclaim.



==================================================
                     PAGE 515                     
==================================================

Notes 499
4. The code scholars do not define in advance what constitutes a signilicant
pattern.Itcan beanygroupofwordsthatare deemedtobeincloseproxim
ityin the textand to berelated, in the opinion ofthe coderesearcher, to the
purportedly predicted event. They are free to use any spacing interval they
wish todefinethepattern.
5. See "The CaseAgainst the Codes," ProfessorBarrySimon, www.wopr.com/
biblecodes/.
6. Ibid.
7. Kahn, "DataMiningMadeEasy."
8. Here, the termpTediction does not necessarily mean foretelling the future,
though forecasting is certainly the ultimate intent of TA. Rather, the tern1
pTediction refers to observations not yet made, regardless ofwhether they
lieinthefuture. Itiscrucialthattheobservationshavenotyetbeenmadebe
causepriorobservationscanalwaysbeexplainedafterthefactbyaninfinite
number of hypotheses or rules. The only way to determine which ofthese
numeroushypothesesmightbecorrectisforpredictionstobemadethatare
then compared with new observations. Thus the term new obseTVation can
refertothebacktestofanewlyproposedrule. (SeeChapter3forafullerex
positionofthispoint.)
9. Out-oj-sampledatareferstodatanotusedforbacktestingtherule.
10. R. Pardo, Design, Testing and Optimization oj Trading Systems (New
York:John Wiley& Sons, 1992).
11. A.W Lo, "TheAdaptiveMarketsHypothesis: MarketEfficiencyfrom an Evo
lutionary Perspective," Journal ojPOTifolio Management 30 (2004), 15-29;
A. Timmermann, "Structural Breaks: Incomplete Information, and Stock
Prices," Journal oj Business & Economic Statistics 19, no. 3 (2001),
299-314.
12. WV. Kidd and WB. Borsen, "Why Have Returns to Technical Analysis De
creased?" Journal ojEconomics and Business 56, no. 3 (May/June 2004),
159-176. The article examines the deterioration oftechnical system perfor
manceoverthecourseoftheout-of-sampledata. Thus,itcouldnotbedueto
data-miningbias.
13. D. Meyers, "Optimization: The World of Illusion," Active Trader (March
2004), 52; D. Meyers, "The Optimization Trap," Active Trade?" (November
2001), 68; D. Meyers, "The Siren Call ofOptimized Trading Systems, 1996,
availableontheauthor'swebsite,www.MeyersAnalytics.com.
14. T. Hastie, R. Tibshirani, andJ. Friedman, TheElements ojStatisticalLearn
ing: Data Mining, Iriference and Prediction, Springer Series in Statistics
(NewYork: Springer,2001).
15. S.H. Weiss and N. Indurkhya, PTedictive Data Mining: A Practical Guide
(San Francisco: MorganKaufmann, 1998).
16. I.H. Witten and E. Frank, Data Mining: Practical Machine Learning Tools
and Techniques, 2nded. (San Francisco:MorganKaufmann, 2005).
17. D.D. Jensen and PR. Cohen, "Multiple Comparisons in Induction Algo
rithms,"MachineLearning38,no.3(March2000),309-338.
18. The Ulcer Index was discussed by PG. Martin and B.B. McCann, The In
vestm"'s Guide to Fidelity Funds (New York: John Wiley & Sons, 1989). A



==================================================
                     PAGE 516                     
==================================================

500 NOTES
similar measure ofrisk based on the average magnitude of equity retrace
ment was previously proposed byJ.D. Schwager, A Complete Guide to the
FuturesMarkets (NewYork:John Wiley& Sons, 1984),471-472.
19. R. Pardo, Design, Testing and Optimization of Trading Systems (New
York:John Wiley& Sons, 1992).
20. J.O. Katz and D.L. McCormick, The Encyclopedia of Trading Strategies
(NewYork: McGraw-Hill,2000).
21. PJ. Kaufman, New Trading Systems and Methods, 4th ed. (Hoboken, NJ:
John Wiley& Sons, 2005).
22. Mean-reversion rules are based on the concept that when a data series
moves to extreme high orlow values relative to recentvalues, itshould re
vertbacktoitsmeanvalue.
23. Divergence rules are based on the concept that iftwo dataseries typically
move up and down together, itis importantinformation when they diverge
from eachother. Thus, divergencerulesgivesignalswhendataseriesdepart
from thisnormalpatternofactivity.
24. Hastie,Tibshirani,andFriedman,Etements ofStatisticalLearning.
25. White, "RealityCheck,"proofofProposition2.1
26. Thisdiagramisconceptualand isnotintendedtoaccuratelyconveythepre-
dictivepowerofscientificlawsorofTArulesexceptinageneralsense.
27. Jensenand Cohen, "MultipleComparisonsinInductionAlgorithms."
28. White, "RealityCheck"
29. JensenandCohen,"MultipleComparisonsinInductionAlgorithms."
30. Although this is notan exanlple ofdata mining, it is an example ofusing a
multiplecomparisonproceduretofind abestsolutiontoaproblem.
31. Actually, randomness has beenputto good use in the design ofsomeofthe
most powerful data-mining algorithms used for complex rule synthesis. An
exampleofthis isRandom Forests, developed by the lateLeoBrieman,pro
fessorofstatistics,attheUniversityofCalifornia,Berkeley.
32. E. Peters, Fractal Market Analysis: Applying Chaos Theory to Investment
& Economics (NewYork:JohnWiley& Son., 1994),21-38.
33. Aruleselectedbecauseofitssuperiorperfonnancein the mineddata.
34. Thisfigure wasinspired byasinlilarfigure inLawrenceLapin,Statisticsfor
Moder-n Business Decisions, 2nd ed. ( ew York: Harcourt Brace Jo
vanovich, 1978),Figure6-10, 186.
35. Pardo,Design, Testing and Optimization, lOS.
36. M. De La Maza, "Backtesting," Chapter8 in Computerized Trading: Maxi
mizing Day Trading and Over-night Profit, M. Jurik (Ed.) (Paramus, NJ:
Prentice-Hall, 1999).
37. Katzand McCormick,EncyclopediaofTradingStrategies.
38. Kaufman,New TradingSystems.
39. P-H.HsuandC.-M. Kuan,"RexaminingtheProfitabilityofTechnicalAnalysis
with Data Snooping Checks," Journal ofFinancial Econometrics 3, no. 4
(2005),606-628.
40. H.M. Markowitz and G.L. Xu, "DataMining Corrections: Sinlple and Plausi
ble,"JournalofPortfolioManagement (Fall 1994),60-69.
41. DeLaMaza,"Backtesting."



==================================================
                     PAGE 517                     
==================================================

Notes 50.
42. B. Efron, "Bootstrap Methods: Another Look at the Jackknife," Annals of
Statistics 7 (1979), 1-26; B. Efron and G. Gong, "A Leisurely Look at the
Bootstrap, the Jackknife, and Cross-Validation," American Statistician 37
(February1983),36-48.
43. Quantmetrics, 2214 EI Amigo Road, Del Mar, CA 92014; contact Professor
HalbertWhite, PhD.
44. Note that the subtraction ofthe average daily return from each daily re
turn can also be done prior to bootstrapping. In this illustration it was
doneafterward.
45. PR. Hansen, "The RealityCheckfor DataSnooping: ACommenton White,"
BrownUniversityDepartmentofEconomics; PR. Hansen, "ATestforSupe
rior Predictive Ability," Journal ofBusiness and Economic Statistics 23
(2005),365-380.
46. J.P Romano and M. Wolf, "Stepwise Multiple Testing as Formalized Data
Snooping," Econometrica 73, no. 4 (July 2005), 1237-1282. Note, this paper
and many others use the term datasnoopingfor whatIreferto as the data
miningbias.
CHAPTER 7 Theories of onrandom
Price Motion
1. C.R. Lightner, "A Rationale for Managed Futures," Technical Analysis of
Stocks & Commodities (March1999).
2. Kepleractuallyproposedthreedistinctlaws.
3. R.D. Edwards and J. Magee, Technical Analysis ofStock Trends, 4th ed.
(Springfield,MA:JohnMagee, 1958).
4. J.J. Murphy, Technical Analysis ofthe Futures Markets: A Comprehensive
Guide to TradingMethods andApplications(NewYork: NewYorkInstitute
ofFinance, 1999),2.
5. E.Fama,"EfficientCapitalMarkets:AReviewofTheoryandEmpiricalWork"
Journal ofFinance25(1970),383-417.
6. R.J. Shiller, Irrational Exuberance (Princeton, NJ: Princeton University
Press,2000), 135.
7. Ibid.
8. In this chapter in section "Nonrandom Price Motion in the Context ofEffi
cientMarkets," Istatethe case ofthose who sayEMH allowsfor price pre
dictability, namely, that random walks are not a necessary implication of
EMH.
9. Updating a belief in a probabilistically correct way implies that new in
formation impacts a prior beliefin accord with Bayes' theorem, whereby
the prior beliefis altered in accord with the predictive value ofthe new
information.
10. Correct prices induce the proper allocation of resources among different
possible uses. Forexample, ifthe price ofcorn is too high, more ofit than
necessary will be produced, flooding the economy with more com than is
neededattheexpenseofsomethingelsesuchaswheat. Netresult: toomany



==================================================
                     PAGE 518                     
==================================================

502 NOTES
boxes of Corn Flakes and too few boxes of Wheaties to meet consumer
demand.
11. P Samuelson,"ProofThatProperlyAnticipatedPricesFluctuateRandomly,"
IndustrialManagementReview6(1965),41-49.
12. B.G. Malkiel,A Random Walk Down Wall Street (NewYork: w.w. Norton&
Company,2003).
13. E. Fama, "Efficient Capital Markets: A Review of Theory and Empirical
Work,"Journal ofFinance25(1970),383-417,ascitedbyA.ShleiferinInef
ficient Markets: An Introduction to Behavioral Finance (Oxford, UK: Ox
fordUniversityPress,2000), 1.
14. A.Shleifer,InefficientMarkets:AnIntroductiontoBehavioralFinance(Ox
ford, UK: OxfordUniversityPress,2000), 1.
15. N.N. Taleb,Fooled byRandomness: TheHiddenRoleofChanceintheMar
kets andinLife(NewYork:Texere,2001).
16. Amoney manager with no skill whatsoever has a 0.5 probability ofbeating
the marketin any given time period (e.g., 1year). In this experiment, Taleb
assumedauniverseofmoneymanagerswitha0.50probabilityofbeatingthe
marketinagivenyear.
17. Actually there are three forms ofEMH-strong, semistrong, and weak. The
strongform contendsthat no information, includinginsideinformation, can
be used to beatthe marketonarisk-adjusted basis. Thisform isimpossible
to test because it cannotbe reliably ascertained who knew what and when
theyknew it. The testimonyofthose who trade on insideinformationis no
toriouslyuntrustworthy.
18. E. Fama, L. Fisher, M. Jensen, and R. Roll, "TheAdjustmentofStockPrices
toNewInformation,"InternationalEconomicReview 10(1969), 1-21.
19. Theseresultsconflictwithlaterstudiesinbehavioralfinancethatfound mar
ketsdid notproperlydiscountsurprisingearningsannouncements. Myinter
pretation is that Fama's studies did not consider the degree to which
earningsannouncementsdifferedfrom priorStreetexpectations. Laterstud
ies done byV. Bernardshowedthatstock prices do experience trends after
earnings announcements, if the degree ofsurprise in the announcement is
large. V. Bernard, "Stock Price Reactions to EarningsAnnouncements,"Ad
vances in Behavioral Finance, R. Thaler (Ed.) (New York: Russell Sage
Foundation, 1992; V. Bernard and J.K. Thomas, "Post-Earnings Announce
mentDrift: DelayedPriceResponse orRiskPremium?,"Journal ofAccount
ing Research, Supplement 27 (1989), 1-36; V. Bernard and J.K. Thomas,
"Evidence That Stock Prices Do Not Fully Reflect the Implications ofCur
rentEarningsfor Future Earnings,"Journal ofAccounting and Economics
13(1990),305-341.
20. Anoninformativeeventisonethatcarriesnoimplicationsforthefuturecash
flows oftheassetorthefuture returnsofthesecurity.
21. Thecapitalassetpricingmodel wasproposedby WilliamF. Sharpein "Capi
talAsset Prices: ATheoryofMarketEquilibriumunder ConditionsofRisk,"
Journal ofFinance 19(1964),425-442.
22. APTwasfirstproposedbyS. Ross, "TheArbitrageTheoryofCapitalAsset
Pricing,"Journal ofEconomic Theory (December1976). APTsaysalinear



==================================================
                     PAGE 519                     
==================================================

Notes 503
model relates a stock's returns to a number offactors, and this relation
ship is the result ofarbitrageurs hunting for riskless, zero-investmentop
portunities. APT specifies factors, including the rate of inflation, the
spread between low and high quality bonds, the slope ofthe yield curve,
andso on.
23. In other words, the current price change is correlated with the prior
change (Le., a lag interval of1), then with the change priorto that (lag in
terval =2), and so forth. Acorrelation coefficient is computed for each of
theselags. Typically, ina random dataseries, the correlationsdrop offvery
quicklyasafunction ofthe laginterval. However, ifthe timeseriesofprice
changes is nonrandom in a linearfashion, some ofthe autocorrelation co
efficients will differsignificantly from the pattern ofautocorrelations ofa
randomsequence.
24. E. Fama, "The Behavior of Stock Market Prices," Journal ofBusiness 38
(1965),34-106.
25. E.E.Peters,ChaosandOrderintheCapitalMarkets:ANew View ofCycles,
Prices andMarket Volatility (New York: John Wiley& Sons, 1991); E.E. Pe
ters, Fmctal Market Analysis: Applying Chaos Theory to Investment and
Economics (NewYork: John Wiley& Sons, 1994).
26. AW. LoandAC. MacKin!ay,ANon-Random WalkDown WaUStreet(Prince
ton, NJ: PrincetonUniversityPress, 1999).
27. S.J. GrossmanandJ.E. Stiglitz, "Onthe ImpossibilityofInformationallyEffi
cientMarkets,"AmericanEconomicReview70, no3(1980),393-408.
28. Shleifer,IneffICientMarkets, 2.
29. V. Singal, Beyond the Random Walk: A Guide to Stock MaTket Anomalies
andLowRiskInvesting (NewYork: OxfordUniversityPress,2004),5.
30. F. Black,"Noise,"Journal ofFinance41 (1986), 529-543.
31. Shleifer,lnefft.eientMarkets, 10.
32. D. Kahneman and A. Tversky, "Prospect Theory: An Analysis of Decision
MakingunderRisk,"Econometrica47(1979), 263-291.
33. R. Shiller, "Stock Prices and Social Dynanlics," Brookings Papers on Eco
nomicActivity2, (1984),457-498.
34. CitedbyShleifer, InefficientMarkets; J.B. DeLong, A Shleifer, L. Sunlmers,
and R. Waldmann, "NoiseTraderRisk in Financial Markets,"Journal ofPo
liticalEconomy98(1990),703-738.
35. R. Merton and P. Samuelson, "Fallacy ofthe Log-Normal Approximation to
Optimal Portfolio Decision-Making over Many Periods," JOU1-nal ofFinan
cialEconomics 1(1974),67-94.
36. Theexpectation ofagameis(probabilityofawin X amountwon)- (proba
bility ofa loss X amount lost). Thus, perdollarbet, the expectation is +50
cents. In investment-strategy ternls, this would be called a profit factor of
2.0.Veryfew investmentstrategieshavesuchafavorable edge.
37. R. Shiller, "Do Stock Prices Move Too Much to BeJustified by Subsequent
ChangesinDividends?,"Ame'licanEconomicReview71 (1981),421-436.
38. Cited by Shleifer, Inefft.eient Markets, 20; D. Cutler, J. Poterba, and L. Sum
mers, "Speculative Dynanucs," Review of Economic Studies 58 (1991),
529-546.



==================================================
                     PAGE 520                     
==================================================

504 NOTES
39. Cited by Shleifer, Ineffi.cient Markets; R Roll, "Orange Juice and Weather,"
AmericanEconomicReview 74 (1984), 861-880; R Roll, "R-squared,"Jour
nalofFinance43, (1988), 541-566.
40. Singal,BeyondtheRandomWalk, ix.
41. Relative performance refers to the stock's performance relative to the per
formanceoftheaveragestock,orabenchmark.Relativeperformanceisalso
knownasexcessreturn.
42. Stockscanalsoberanked onthebasisofapredictionmodelthatemploysa
multitudeofindicators,typicallyreferredtoasfactors.
43. When multiperiod horizons are used, the number of independent observa
tionsarealsoreduced,soaone-monthhorizon, basedonmonthlydata,isof
tenused.
44. ThestrongformofEMHisbroaderyetinclaimingthatnoteveninsideinfor
mation is useful in generating superior returns, but this version is not
testable.
45. R Banz, "The Relation between Market Return and Market Value for Com
monStocks,"Journal ofFinancialEconomics9(1981),3-18.
46. E. Fama and K. French, "The Cross-Section of Expected Stock Returns,"
Journal ofFinance47(1992), 427-465.
47. Cited by Shiller, Irrational Exuberance; S. Basu, "The Investment Perfor
mance ofCommonStocksRelative to TheirPrice-EarningsRatios: ATestof
theEfficientMarkets,"Journal ofFinance32,no.3(1977),663-682.
48. CitedbyShiller, IrrationalExuberance; E. Farnaand K. French, "TheCross
SectionofExpected StockReturns,"Journal ofFinance47(1992), 427-466.
49. Singal,BeyondtheRandom Walk.
50. Althoughthe weakform ofEMH typicallyonlytalks aboutpastpricesand
past price changes, I believe advocates of this position would also claim
that any other data relied upon by TA would be useless for predicting fu
turereturns.
51. N. Jegadeesh,andS. Titman, "ReturnstoBuyingWinnersandSellingLosers:
ImplicationsforMarketEfficiency,"Journal ofFinance48, (1993),65-91.
52. E. Farna, "Efficient Capital Markets: II," Journal of Finance 46 (1991),
1575-1617.
53. W. De Bondtand R H. Thaler, "Doesthe Stock MarketOverreact?,"Journal
ofFinance40, no. 3(1985), 793-805.
54. Inthiscase,riskisasdefinedbyCAPM,volatilityrelativetothemarket.
55. T.J. Georgeand C.-Y. Hwang, "The52-WeekHighand MomentumInvesting,"
Journal ofFinance59,no. 5(October2004),2145.
56. C.M.C. Lee and B. Swaminathan, "Price Momentum and Trading Volume,"
Journal ofFinance 55, no. 5 (October 2000), 2017-2067; S. E. Stickel and
RE. Verecchia, "Evidence That Trading Volume Sustains Stock Price
Changes," Financial Analysts Journal 50, no. 6 (November-December
1994),57-67. Itshould be noted thatwhen the forward-looking horizon is3
to 12months, highvolumeincreasesthepricemomentum'seffect. However,
when the return is looked at 3 years out, high-momentum stocks with low
volume are more persistent on the upside, whereas negative-momentum
stockswithhighvolumepersistonthedownside.



==================================================
                     PAGE 521                     
==================================================

Notes 505
57. E. Fama and K. French, "1996 Common Risk Factors in the Returns on
Bonds and Stocks," Joumal of Financial Economics 33 (1993), 3-56; E.
FamaandK. French, "MultifactorExplanationsofAssetPricingAnomalies,"
JoumalofFinance 51 (1996),55-84.
58. W.Sharpe,"CapitalAssetPrices:ATheoryofMarketEquilibriumunderCon
ditions ofRisk,"Joumal ofFinance 19 (1964), 435-442; also seeJ. Lintner,
"The Valuation of Risk Assets and the Selection of Risky Investments in
StockPortfoliosand Capital Budgets,"Review ofEconomics and Statistics
47(1965), 13-37.
59. Shleifer, InefficientMar-kets, 19.
60. J. Lakonishok, A. Shleifer, andR. Vishny, "ContrarianInvestment,Extrapola-
tion, and Risk,"JoumalofFinance49(1994), 1541-1578.
61. Shleifer,InefficientMar-kets, 24.
62. Ibid.
63. Ibid.
64. W. Edwards, "Conservatism in Human Information Processing," in For-mal
Repr-esentation ofHuman Judgment, B. Kleinmutz (Ed.) (New York: John
Wiley& Sons, 1968)17-52.
65. D.G. Myers, Intuition, Its Power-s and Perils (New Haven: Yale University
Press,2004), 157.
66. A. Tverskyand D. Kahneman,Judgmentunder-Uncertainty HeuTistics and
Biases, inJudgment under- Uncertainty: HeuTistics andBiases, D. Kahne
man, P. Slovic, and A. Tversky (Eds.) (New York: Cambridge University
Press, 1982),14.
67. Georgeand Hwang, "52-WeekHigh."
68. Shiller,IrrationalExuber-ance,138.
69. Ibid., 139.
70. E. Shafir,I.Simonson,andA. Tversky,"Reason-BasedChoice,"Cognition49
(1993), 11-36.
71. Systematicprocesscan exhibitrealtrends, whicharestreaksthatare longer
indurationthan thosefound inrandomwalks,ortheycanexhibittruemean
reversionbehavior,whichmeansthatstreakswillbeshorterinduration(Le.,
morefrequentflip-flopsbetweenpositiveandnegativeoutcomes)thanistrue
for random walks. In both truly trended processes and true mean-reverting
process,priorobservationscontainusefulpredictiveinformation.Intrueran
dom-walkprocesses,priorobservationscontainnopredictiveinformation.
72. Shiller,IrrationalExubemnce, 148-169.
73. A. Asch, Social Psychology (Englewood Cliffs, NJ: Prentice-Hall, 1952),
450-501.
74. M. DeutschandH.B.Gerard, "AStudyofNonnativeandInformationalSocial
Influence uponIndividualJudgment,"Joumal ofAbnor-malandSocialPsy
chology51 (1955),629-636.
75. Shiller,IrrationalExuber-ance, 148-169.
76. R.J. Shiller cites D.D. Bikhchandani, D. Hirshleifer, and I. Welch, "ATheory
ofFashion, Social Customand CulturalChange,"Joumal ofPoliticalEcon
omy 81 (1992), 637-654; A.v. Nanerjee, "A Simple Model ofHerd Behavior,"
Quar-ter-lyJoumal ofEconomics 107,no. 3(1993), 797-817.



==================================================
                     PAGE 522                     
==================================================

506 NOTES
77. Shiller,IrrationalExuberance, 152.
78. Ibid., 164.
79. Ibid., 165.
80. Ibid., 166.
81. See Carl Anderson, "Notes on Feedback and Self-Organization," at
www.duke.edu/-carl/pattemlcharacteristic_student_notes.htm.
82. Shiller,IrrationalExuberance,67.
83. N. Barberis, A Shleifer, and R Vishny, "A Model of Investor Sentiment,"
Journal ofFinancialEconomics49(1998),307-343.
84. This assumption is consistent with studies showing that people have diffi
cultydistinguishingrandomseriesfromseriesthatdisplaysystematicbehav
ior such as mean-reversion or trending behavior. This failure accounts for
sports fans' misperception ofthe hot hand and chartists' inability to distin
guishrealfromrandomchartscitedinchapter2.
85. Shiller,IrrationalExuberance, 144.
86. RJ. Shiller cites Barberis, Shleifer, and Vishny, "Model of Investor Senti
ment,"andN. Barberis,M. Huang,andT.Santos, "ProspectTheoryandAsset
Prices,"QuarterlyJournalofEconomics 116,no. 1(February2001),1-53;K.
Daniel, D. Hirshleifer, and A Subrahmanyam, "InvestorPsychology and Se
curityMarketOverand Underreaction,"Journal ofFinance53,no. 6(1998),
1839-1886; H. Hong and J. Stein, "A Unified Theory of Underreaction, Mo
mentum Trading, and Overreaction in Asset Markets," Journal ofFinance
54,no. 6(December1999),2143-2184.
87. Daniel, Hirshleifer, and Subrahmanyam, "Investor Psychology"; and K.
Daniel,D.Hirshleifer,andA Subrahmanyam,"Overconfidence,Arbitrageand
EquilibriumAssetPricing,"Journal ofFinance56(2001), 921-965.
88. N. Barberis, "ASurveyofBehavioralFinance," inHandbook oftheEconom
ics of Finance, Volume 1B, G.M. Constantinides, M. Harris, and R Stulz
(Eds.) (NewYork: ElsevierScience, B.V.,2003).
89. N. Chopra and R Lakonishok, "Measuring Abnonnal Performance: Do
Stocks Overreact?,"Journal ofFinancial Economics 31 (1992), 235-268; R
La Porta, R Lakonishok, A Shleifer, and R Vishny, "Good News for Value
Stocks: Further Evidence on Market Efficiency," Journal of Finance 49
(1997), 1541-1578.
90. J.B. DeLong, A Shleifer,1. Summers, and R Waldmann, "PositiveFeedback
Investment Strategies and Destabilizing Rational Speculation," Journal of
Finance45, no. 2(1990), 379-395; N. Barberisand A Shleifer, "StyleInvest
ing,"Journal ofFinancialEconomics68(2003),161-199.
91. HongandStein, "UnifiedTheory."
92. S.F. LeRoy, "Risk Aversion and the Martingale Property of Stock Returns,"
International Economic Review 14 (1973), 436-446; RE. Lucas, "Asset
PricesinanExchangeEconomy,"Econometrica46(1978), 1429-1446.
93. AW. LoandAC. MacKinlay,ANon-Random WalkDown WallStreet(Prince
ton, NJ: PrincetonUniversityPress, 1999),5.
94. Ibid.
95. S. Grossman, "On the Efficiency of Competitive Stock Markets Where
TradersHaveDiverseInformation,"Journal ofFinance31 (1976), 573-585.;



==================================================
                     PAGE 523                     
==================================================

Notes 507
S. GrossmanandJ. Stiglitz,"OntheImpossibilityofInformationallyEfficient
Markets,"AmericanEconomicReview70(1980),393-408.
96. L. Jaeger, Managing Risk inAlternativeInvestment Strategies: Successful
Investing in Hedge Funds and Managed Futures (London: Financial
Times-PrenticeHall,2002),27.
97. Gains from a true inefficiency are not a free lunch, because it is costly to
identifythem, butthey need notentail the risk ofadditional volatility in re
turns. Thus true inefficiencies can generate investmentperformance with a
highrelativeSharperatio.
98. H.D. Platt, Counterintuitive Investing: Profiting from Bad News on Wall
Street(Mason, OH:ThomsonHigherEducation, 2005).
99. Equityand bond markets provide a quick way for owners ofequities to sell
their position to other investors. The liquidity of stocks and bonds thus
makes them attractive to the initial investors, thereby making it easier for
companiestoraisedebtandequityfinancing.
100. Grains(corn,soybeans,soybeanmeal,soybeanoil,andwheat);financials(5
year T-notes, 10-year T-notes, long-term treasury bonds); currencies (Aus
tralian,British,Canadian,German,Swiss,andJapanese);energy(heatingoil,
naturalgas,crudeoil,andunleadedgas);cattle;metals(gold,copper,andsil
ver);andsoft/tropical (coffee,cotton,andsugar).
101. G.R Jensen, RR Johnson, and J.M. Mercer, "Tactical Asset Allocation and
Commodity Futures," Journal ofPortfolio Management 28, no. 4 (Summer
2002).
102. Anasset-classbenchmarkmeasuresthereturnsearnedandrisksincurredby
investinginaspecificassetclass,withnomanagementskill.
103. Lars Kestner, Quantitative Trading Strategies: Harnessing the Power of
Quantitative Techniques to Create a Winning Trading Program (New
York: McGraw-Hill,2003), 129-180.
104. Theeightmarketsectorstested wereforeign exchange, interestrates, stock
index, metals,energy,grains,meats,andsofts.
105. The nine industry sectors were energy, basic materials, consumer discre
tionary, consumer staples, health care, financials and information technol
ogy, telecom. The three stock indexes were S&P 500, NASDAQ 100, and
Russell2000.
106. The five trend-followingsystems were channel breakout, dual moving-aver
age crossover, two version ofmomentum, and MACD versus its signal line.
FormoredescriptionseeKestner'sQuantitative TradingStrategies.
107. M. Cooper, "Filter Rules Based on Price and Volume in Individual Security
Overreaction," Review of Financial Studies 12, no. 4 (Special 1999),
901-935.
CHAPTER 8 Case Study ofRule Data Mining for
the S&P 500
1. J.P. Romano and M. Wolf, "Stepwise Multiple Testing as Formalized Data
Snooping,"Econometrica73, no. 4(July2005), 1237-1282.



==================================================
                     PAGE 524                     
==================================================

508 NOTES
2. J.J. Murphy, IntermaTket Technical Analysis: Trading Strategies fOT the
Global Stock, Bond, Commodity and Currency Markets (New York: John
Wiley& Sons, 1991).Iwasfirstexposedtotheideaofusingrelationshipsbe
tweenmarketsforindicatordevelopmentin 1979 byMichaelHammond and
Norman Craig of Advance Market Technologies (AMTEC), Ogden, Utah.
AMTEC was one ofthe first firms to apply artificial-intelligence methods to
market prediction. AMTEC's modeling approach was based on the work of
Roger Baron on adaptive learning networks, now known aspolynomial net
works, atAdaptronicsInc, in the 1970s. Iroutinely used intermarketindica
tors in work for clients of Raden Research Group, inclucting Cyber-Tech
Partners (1983), Tudor Investment Corporation (1987), and Manufacturer's
HanoverTrust(1988).
3. P-HHsuand C-MKuan, "ReexanliningtheProfitabilityofTeclmicalAnalysis
with Data Snooping Checks," JouTnal of Financial Economics 3, no. 4
(2005),606-628.
4. Many ofthe automated methods are described in T. Hastie, R. Tibshirani,
andJ. Friedman, TheElementsofStatisticalLearning:DataMining, Infer
enceandPrediction(NewYork: Springer,2001).
5. Asegmentofthefuture overwhichtherule'sprofitabilitypersiststhatislong
enough to compensatethe effortrequired to discoverthe rulebutnotofun
limitedduration.
6. H. White, "ARealityCheckforDataSnooping,"Econometrica68,no. 5(Sep
tember 2000). Note that White uses the term datasnooping to mean "data
miningbias,"asused inthisbook.
7. Mathematical operators include add, subtract, multiply, divide, powers,
roots, andsoforth.
8. Logical operators includeand; or,gTeatercthan; less-than; if, then; else, and
soforth.
9. Time-series operators include moving average, moving channel normaliza
tion (stochastics), breakout channels, moving slopes, and such (described
later).
10. p.K. Kaufman, 249.
11. Ibid.,200.
12. Ibid.,202. Kaufmannotesthatastudydone in 1970byDunn& HargittFinan
cialServicesshowedthatachannelbreakoutusingafour-weekspan,known
as Donchian'sfour-weekrule, was thebestofseveralpopularsystemswhen
tested on 16 years of data on seven commodities markets. Kaufman also
compares the performance ofchannel breakout rules to four other trend
following methods (exponential smoothing, linear regression slope, swing
breakout, and point-and-figure) to Eurodollars for the lO-year period 1985
through 1994. The channel breakout method had the highest risk-adjusted
performanceoverthe lO-yearperiod.
13. Asimplemovingaverageiscenteredbyplottingthemoving-averagevaluein
the centerofthe moving-datawindow. IfN is thenumberofperiods used to
compute the moving average, the average is plotted to correspond with a
valuethatislaggedby(n- 1)/2datapoints.



==================================================
                     PAGE 525                     
==================================================

Notes 509
14. J.F. Ehlers, RocketSciencefor Traders: Digital Signal Processing Applica
tions (NewYork:John Wiley& Sons,2001);J.F. Ehlers, CyberneticAnalysis
for Stocks and Futures: Cutting-Edge DSP Technology to Improve Your
Trading (NewYork:JohnWiley& Sons,2004).
15. Kaufman, 256.
16. Ehlers,RocketScienceforTraders, 27.
17. W.A. Heiby, Stock Market Profits thr-ough Dynamic Synthesis (Chicago: In
stituteofDynamicSynthesis, 1965).
18. Althoughalmostallsourcesattributethedevelopmentofthis timeseriesop
eratorto Dr. George Lane in 1972, no source mentions the use ofthe same
ideabyHeibyatan earliertime. Onthisbasis, Ihaveattributed the develop
mentto Heiby. Itis possible thatHeiby based his work on yetan earlierau
thor.
19. For information about various back-testing software platforms see: for
TradeStation www.TradeStationWorld.com; for MetaStock www.equis.com;
for AlQ Expert Design Studio www.aiqsystems.com; for Wealth-Lab
www.wealth-lab.com; for eSignal www.esignal.com; for Neuroshell Trader
www.neuroshell.com; for AmiBroker www.arnibroker.com; for Neoticker,
www.tickquest.com; forTrading$olutionswww.tradingsolutions.com;forFi
nancial DataCalculatorwww.mathinvestdecisions.com.
20. illtraFinancial Systems, PO Box 3938, Breckenridge, CO90424; phone 970
453-4956; websitewww.ultrafs.com.
21. MarketTimingReports, POBox225, Tucson, AZ85072;phone 520-795-9552;
websitewww.mktimingrpt.com.
22. Studiesshowingthattradingvolumecontainsuseful informationincludethe
following: L. Blume and M. Easley O'Hara, "MarketStatistics and Technical
Analysis: The Role ofVolume,"JOU1'nal ofFinance49, issue 1(March 1994),
153-181; M. Cooper, "Filter Rules Based on Price and Volume in Individual
Security Overreaction," Review ofFinancial Studies 12 (1999), 901-935;
C.M.C. Lee and B. Swaminathan, "Price Momentum and Trading Volume,"
JournalofFinance55,no.5(October2000),2017-2069;S.E. Stickeland R.E.
Verecchia, "Evidence That Trading Volume Sustains Stock Price Changes,"
FinancialAnalystsJournal50(November-December1994),57-67.
23. J.E. Granville, Granville's New Strategy ofDaily Stock Market Timingfor
Maximum Profit (Englewood Cliffs, NJ: Prentice-Hall, 1976). The attribu
tiontoWoodsandVignoliacanbefoundinthebodyofknowledgesectionon
thewebsiteoftheMarketTechniciansAssociation.
24. This formulation ofOBV is as presented in Technical Indicators Reference
Manual forAlQTradingExportPro, asoftwareproductofAlQSystems, PO
Box7530,InclineVillage, NV89452.
25. TheMTABodyofKnowledgeonTechnicalIndicatorsattributestheaccumu
lationdistributionindicatortoMarcChaiken,as doesR.W. Colby, TheEncy
clopedia ofTechnical Market Indicators, 2nd ed. (New York: McGraw-Hill,
2003). No specific publication is referenced. The accumulation distribution
indicatorproposed byChaiken issinillarto an indicatordeveloped byLarry
Williams, The Secrets ofSelecting Stocks for Immediate and Substantial



==================================================
                     PAGE 526                     
==================================================

510 NOTES
Gains (CarmelValley, CA: ConceptualManagement, 1971),laterrepublished
by Windsor Books in 1986. Williams's indicator compares the open price to
the close as the numeratorofthe range factor. Chaiken compares the close
pricetothernidpriceofthedailyrange.
26. As presented in Technical Indicators Reference Manual for AIQ Trading
ExportPro,asoftwareproductofAIQSystems,POBox7530,InclineVillage,
NV89452.
27. Ibid.
28. Attribution of NVI to Norman Fosback can be found in the MTA web site
Body of Knowledge and in S.B. Achelis, Technical Analysis from A to Z
(NewYork: McGraw-Hill,2001),214.
29. Paul Dysart is credited with the creation of both positive- and negative
volumeindexesbyC.Y.Harlow,AnAnalysisofthePredictiveValueofStock
Market ''Breadth'' Measurements (Larchmont, NY: Investors Intelligence,
1968).
30. Accordingto Fosback'sstudy,the NVIidentifiedthebullishphaseofthepri
marytrendwithaprobabilityof0.96overthe timeperiod 1941 to 1975.This
comparestoabaseratebull-trendprobabilityof0.70overthattimeperiod.
3!. N.G. Fosback, Stock Market Logic: A Sophisticated Approach to Profits on
WallStreet(Dearborn, MI: DearbornFinancialPublishing, 1993),120.
32. Ibid.
33. Harlow,AnalysisofthePredictiveValue.
34. R.w. Colby, TheEncyclopediaofTechnicalMarketIndicators, 2nded. (New
York: McGraw-Hill,2003).
35. Fosback,StockMarketLogic,76-80.
36. Thisformulation ofCNHLisaspresented in TechnicalIndicatorsReference
Manual forAIQTradingExportPro,asoftwareproductofAIQSystems.
37. C.R. Nelson, TheInvestor's Guide to Economic Indicators (NewYork: John
Wiley & Sons, 1987), 129. The authorpoints outthat a positive spread (Le.,
long-term rates higherthan short-termrates) is often a bullish indicatorfor
futurechangesinstockprices,especiallyforstockindexesthatarenotinfla
tionadjusted.
38. Moody's Investors Service is a widely utilized source for credit ratings, re
search, andriskanalysisfor corporatedebtinstruments. Informationcanbe
found atwww.moodys.com.
39. Aaaisthedesignateddefinitionforthehighestqualityofcorporatebond. Se
curitiescarryingthisratingarejudgedto have thesmallestdegree ofinvest
mentrisk. The principal is secure and interestpayments are protected bya
robustmargin. Althoughthereisalwaysachancethatvariousprotectiveele
mentscanchange,thesechangesareeasytovisualize.TheAaaratingmeans
that it is highly unlikely for any drastic changes to occurthat would erode
the fundamentally strongpositions onthese securities. Definitionprovided:
www.econoday.com/clientdemos/demoweekly/2004/Resource_Centerlabout
bondmkt/morefixed.htrnl.
40. Baabondsareconsideredmedium-gradeobligations.Theyareneitherhighly
protected norpoorlysecured. Interestpayments and security on the princi
palappearadequateforthepresent, buttheremaybecertainprotectiveele-



==================================================
                     PAGE 527                     
==================================================

Notes 511
mentslacking. Thesesecuritieshave the possibility ofbeing unreliable over
anygreatlengthoftime.Thesebondslackoutstandinginvestmentcharacter
istics, making them attractive to speculators at the same time. www.econo
day.com/clientdemos/demoweekly/2004/Resource_Center/aboutbondmktJm
orefixed.html.
41. Forafull discussionofalternative trend analysismethodsand rules derived
from themseeKaufman, Chapters5through8.
42. J. Hussman, "Time-VariationinMarketEfficiency:AMixture-of-Distributions
Approach,"availableatwww.hussman.net/pdf/mixdist.pdf.
43. M.J. Pring, TechnicalAnalysisExplained, 4th ed. (New York: McGraw-Hill,
2002).
44. Kaufman, 394-401.
45. Heiby,StockMarketProfits.
46. C. Alexander, Market Models: A Guide to Financial Data Analysis (New
York:John Wiley& Sons,2001), Chapter12.
47. R.F. Engle and C.w.J. Granger, "Co-integration and Error-Correction: Repre
sentation, EstimationandTesting,"Econometrica55(1987), 251-276.
48. Alexander,MarketModels, 324-328, 353-361. Onetype oftestused to estab
lish the fact that a time series is stationary, called the unit-root test, is de
scribed.
49. The indicatorisa timeseries ofthe errors (residuals) ofthe linear relation
shipbetweenthetwoseries.
50. Fosback,StockMarketLogic.
51. In 1983theauthor, inassociation with Raden Research Group, developeda
timeseries operatorcalled ITthat wassimilarin conceptto cointegration
analysis with the exception that IT values were normalized by the stan
dard errorofthe regression and no explicittestwas madefor residual sta
tionarity. ITs development was inspired by an article by Arthur Merrill,
"DFE: Deviation from Expected (Relative Strength Corrected for Beta),"
Market TechnicianJournal (August1982),21-28. The IT-based indicators
wereused ascandidateinputsforavarietyofpredictivemodelingprojects.
One example wasa sentimentindicatorbased on advisorysentimenton T
bonds. The predictive power of the raw consensus information was im
proved by a IT transformation that used past market changes as the
regressorvariable.
CHAPTER 9 Case StudyResults and
the Future ofTA
1. Thisisanextremerule,Type 12, basedoninputseries28(lO-daymovingav
erageofthe up/downvolume ratio), usingthreshold offsetof10(upper=60,
lower = 40) and a channel normalization look-back of 30 days. Type 12 is
short although the indicator is above the upper threshold, long at all other
times.
2. Thebias-compensatedp-valueswerethisbad(worsethanp= 0.5)inpartbe
causehalfofthe6,402rules wereinverse rules. Thisguaranteed thathalfof



==================================================
                     PAGE 528                     
==================================================

512 NOTES
the competing rules would have a negative return, and, hence, have no
chanceofbeinga true competitor. Ineffectthe testhad twiceas many rules
beingtested as were truly in competition. This makes your test overly con
servative.Thatistosay,thechanceofaType-l error(falserejectofthenull)
was lower than0.10. Italso means that thepowerofthe test(abilityto cor
rectlyrejectthe null) waslessthanitwould have beenhad the inverserules
beenleftoutofthe competition. However, as explainedinChapter1, the in
verse ruleswereincluded becauseitwas notknownifthe traditionalTAin
terpretationwascorrect.
3. J.P. Romano and M. Wolf, "Stepwise Multiple Testing as Formalized Data
Snooping,"Econometrica 73,no. 4(July2005), 1237-1282.
4. H. White, "ARealityCheckfor DataSnooping,"Econometrica68, no5(Sep
tember2000).
5. WR. Ashby, Introduction to Cybernetics (New York: John Wiley & Sons,
1963). In fact, Ashby was referring to the problem ofsystem control rather
than system prediction, though there are obvious similarities between the
twoproblems.
6. P-HHsuand C-M Kuan, "Reexaminingthe ProfitabilityofTechnicalAnalysis
with Data Snooping Checks," Journal ofFinancial Econometrics 3, no. 4
(2005),606-628. Note the authors are using the term datasnoopingas Iuse
the term data mining. They are checking and making adjustment for data
mining bias using a version of White's reality check, without the enhance
mentssuggestedbyWolfand Romano (seeChapter6).
7. Ibid.
8. EuiJung Chang, Eduardo Jose AraUjo Lima, and Benjamin MirandaTabak,
"Testing for Predictability in Emerging Equity Markets," Emerging Markets
Review 5, issue 3 (September 2004), 295-316; Malay K. Dey, "Turnover and
ReturninGlobalStockMarkets,"EmergingMarketsReview6, issue1(April
2005), 45-67; Wing-Keung Wong, MeherManzur, and Boon-KiatChew, "How
Rewarding Is Technical Analysis? Evidence from Singapore Stock Market,"
Applied Financial Economics 13, issue 7 (July2003), 543; Kalok Chan and
Allaudeen Hameed, "Profitability of Momentum Strategies in the Interna
tional Equity Markets," Journal ofFinancial & Quantitative Analysis 35,
issue2(June2000), 153.
9. C. Alexander, Market Models: A Guide to Financial Data Analysis (New
York:JohnWiley& Sons,2001),347-387.
10. J.F. Ehlers,CyberneticAnalysisforStocksandFutures: Cutting-EdgeTech
nology to Improve Your Trading (Hoboken, NJ: John Wiley & Sons, 2004),
1-10.
11. Hsuand Kuan, "ReexaminingtheProfitability."
12. 1. Hastie, R. Tibshirani, andJ. Friedman, TheElements ofStatisticalLearn
ing: Data Mining, Inference and Prediction (New York: Springer, 2001),
347-370.
13. Ibid.,283-290.
14. J.R. Wolberg, Expert Trading Systems: Modeling Financial Markets with
KernelRegression(NewYork:JohnWiley& Sons,2000).



==================================================
                     PAGE 529                     
==================================================

Notes 513
15. J.F. ElderIVand D.E. Brown, "Inductionand Polynomial Networks," inNet
workModelsjorControlandProcessing, M. D. Fraser(Ed.)(Bristol, UK: In
tellectLtd,2000).
16. W Banzhaf, P. Nordin, R.E. Keller, and F.W Francone, Genetic Program
ming: An Introduction; On the Automatic Evolution oj Computer Pro
grams andItsApplications(SanFrancisco:MorganKaufmann, 1998).
17. N. Cristianini and J. Taylor-Shawe, An Introduction to Support Vector Ma
chines (NewYork: CambridgeUniversityPress,2000).
18. Complexitycanmeandifferentthingsindatamining.Inthecaseofdatamin
ing for TA rules, it refers to the number ofseparate parameters and condi
tional statements required to define the rule. Higher complexity rules have
more parametersand conditions. Adual moving-average crossoverrule has
oneconditionandtwoparametervalues.Theconditionis:Ifshort-termmov
ing average >than long-term moving average, then hold long position, else
hold short position. The two parameters are look-back spans for the short
termandlong-termmovingaverages.
19. N. Gershenfeld, The Nature ojMathematical Modeling (New York: Canl
bridgeUniversityPress, 1999), 147.
20. Ifshorter-termmovingaverage>longer-termmovingaverage,thenholdlong
position,elseholdshortposition.
21. There are more intelligent and efficient search methods than testing every
possible combination. Theseincludegeneticalgorithmsearching, simulated
annealing,guidedsearchingbasedongradientdescent,andsoforth. Twoex
cellenttreatmentsofthesetopicscan befound inKatz andMcCormick, and
Pardo. Seeearlierfootnotereferences.
22. Inthisexample,thesimpletwo moving-averagecrossoverruleisasubsetof
amorecomplexsetofrulesofgreatercomplexity.
23. WA. Sherden, TheFortuneSellers: TheBigBusinessojBuyingandSelling
Predictions(NewYork:JohnWiley& Sons, 1998), 1.
24. J.S. Armstrong, "TheSeer-SuckerTheory: The ValueofExperts in Forecast
ing,"TechnologyReview(June/July1980), 16-24.
25. A. Cowles, "Can Stock Market Forecasters Forecast?" Econometrica 1
(1933),309-324.
26. In February 2005, Citigroup terminated its technical research department.
Prudential Securities disbanded its technical research department later in
theyear.
27. N.G. Fosback, Stock Market Logic: A Sophisticated Approach to Profits on
WallStreet(Dearborn, MI: DearbornFinancialPublishing, 1993),80.
28. B.I. Jacobs and K.N. Levy, Equity Management: QuantitativeAnalysisjor
StockSelection(NewYork: McGraw-Hill,2000),27-37.
29. The return reversal effectis similarto the residual reversal effect, which is
basedoncointegrationanalysis.
30. J. Felsen, Cybernetic Approach to Stock Market Analysis versus EjJicient
Market Theory (NewYork: ExpositionPress, 1975);J. Felsen,DecisionMak
ing under Uncertainty: An Artificial Intelligence Appr-oach (New York:
CDSPublishingCompany, 1976).



==================================================
                     PAGE 530                     
==================================================

514 NOTES
31. Twofirms with whichIhad contactinthelate 1970sthatwere usingstatisti
calpatternrecognitionandadaptivelearningnetworkswereBraxtonCorpo
ration in Boston, Massachusetts, and AMTEC Inc. in Ogden, Utah. Istarted
RadenResearchGroup, Inc. in 1982.
32. AM. Safer, "AComparisonofTwoDataMiningTechniquestoPredictAbnor
malStockMarketReturns,IntelligentDataAnalysis7,no. 1(2003),3-14;G.
Armano,A Murru, and F. Roli, "StockMarketPredictionbya MixtureofGe
netic-NeuralExperts,"International Journal ofPatternRecognition & Ar
tificial Intelligence 16, no. 5 (August 2002), 501-528; G. Armano, M.
Marchesi,andA Murru, "AHybrid Genetic-NeuralArchitectureforStockIn
dexes Forecasting," Information Sciences 170, no. 1(February2005), 3-33;
T. Chenoweth, Z.O. Sauchi, and S. Lee, "EmbeddingTechnicalAnalysis into
Neural Network Based TradingSystems,"AppliedArtifl.CialIntelligence 10,
no. 6 (December 1996), 523-542; S. Thawornwong, D. Enke, and C. Dagli,
"NeuralNetworksasaDecisionMakerforStockTrading:ATechnicalAnaly
sisApproach,"InternationalJournal ofSmartEngineeringSystemDesign
5, no. 4 (OctoberlDecember 2003), 313-325; AM. Safer, "TheApplication of
Neural-Networks to PredictAbnonnal Stock Returns Using InsiderTrading
Data,"AppliedStochasticModels inBusiness& Industry 18,no. 4(October
2002), 380-390; J. Yao, C.L. Tan, and H-L Pho, "Neural Networksfor Techni
calAnalysis: AStudyon KLCI,"International Journal ofTheoretical & Ap
pliedFinance2,no. 2(April 1999),221-242;J. KorczakandP Rogers,"Stock
Timing Using Genetic Algorithms,"Applied Stochastic Models in Business
& Industry 18, no. 2 (April 2002), 121-135; Z. Xu-Shen and M. Dong, "Can
Fuzzy Logic Make Technical Analysis 20/20?" Financial Analysts Journal
60, no. 4(July/August2004), 54--75;J.M. Gorriz, C.G. Puntonet, M. Salmeron,
andJ.J. DelaRosa, "ANewModelforTime-SeriesForecastingUsingRadial
Basis Functions and Exogenous Data," Neural Computing & Applications
13, no. 2(2004), 100-lli.
33. PE. Meehl, Clinical versusStaticalPrediction:A TheoreticalAnalysisand
a Review of the Evidence (Minneapolis: University of Minnesota Press,
1954).
34. R HastieandRM. Dawes,RationalChoiceinanUncertain World: ThePsy
chology ofJudgment andDecisionMaking (Thousand Oaks, CA: SagePub
lications,2001), 55.
35. J. Sawyer,"MeasurementandPrediction,ClinicalandStatistical,"Psycholog
icalBulletin66(1966), 178-200.
36. Hastieand Dawes,Rational Choice, 55.
37. C. Camerer, "General Conditionsfor the Success ofBootstrapping Models,"
OrganizationalBehaviorandHumanPerformance27(1981),411--422.
38. J.E. RussoandPJ.H. Schoemaker,DecisionTraps: The TenBarrierstoBril
liantDecision-MakingandHow toAvoidThem (NewYork: Doubleday/Cur
rency, 1989).
39. L.R Goldberg,"SimpleModelsorSimpleProcesses?SomeResearchonClin
icalJudgments,"AmericanPsychologist23(1968),483--496.
40. RM. Dawes, "TheEthicsofUsingorNotUsingStatisticalPredictionRules,"
anunpublishedpaperwrittenatCarnegieMellonUniversity.



==================================================
                     PAGE 531                     
==================================================

Notes 515
41. PE. Meehl, "Causes and Effects ofMy Disturbing Little Book," Journal of
PersonalityAssessment50(1986),370-375.
42. WM. Groveand PE. Meehl, "ComparitiveEfficiencyofWormal(Subjective,
Impressionistic) and Formal (Mechanical, Algorithmic) Prediction Proce
dures: The Clinical-Statistical Controversy," Psychology, Public Policy, and
Law2(1996),293-323.
43. Dawes,"EthicsofStatisticalPredictionRules."
44. Hastieand Dawes,RationalChoice, 54.
45. C.F. Camerer and E.J. Johnson, "The Process-Performance Paradox in Ex
pert Judgment: How Can Experts Know So Much and Predict So Badly?"
Chapter10inResearchonJudgment andDecisionMaking: Currents, Con
nections and Controversies, WM. Goldstein and R.M. Hogarth (Eds.), Cam
bridgeSeriesonJudgmentandDecisionMaking(Cambridge,UK:Cambridge
UniversityPress, 1997).
46. PE. Tetlock, Expert Political Judgment: How Good Is It? How Can We
Know?(Princeton, NJ: PrincetonUniversityPress,2005).
47. Ibid., 77.
48. G.F. Loewenstein, E.D. Weber, C.R. Hsee, and N. Welch, "Risk as Feelings,"
PsychologicalBulletin 127,no. 2(2001),267-287.
49. J.R. Nofsinger, "Social Mood and Financial Economics," Journal ofBehav
ioralFinance6, no.3(2005), 144-160.
50. Ibid., 151.
51. P Slovic, M. Finucane, E. Peters, and D. MacGregor, "TheAffect Heuristic,"
Chapter 23 in Heuristics and Biases: The Psychology ofIntuitive Judg
ment,T. Gilovich,D. Griffin,and D. Kahneman (Eds.) (Cambridge, UK: Cam
bridgeUniversityPress,2002),397-420.
52. Ibid.,416.
53. J.P. Forgas,"MoodandJudgment:TheAffectWusionModel(AlM),"Psycho
logicalBulletin117,no. 1(1995),39-66.
54. ofsinger, "SocialMood,"152.
55. Some books on the general topic of TA Indictors: S.B. Achelis, Technical
Analysisfrom A to Z, 2nd ed. (New York: McGraw-Hill, 2001); E.M. Azoff,
NeuralNetwork Time Series Forecasting ofFinancialMarkets (New York:
John Wiley&Sons,1994);R.W Colby, TheEncyclopediaofTechnicalMarket
Indicators, 2nded. (NewYork: McGraw-Hill,2003);PJ. Kaufman,New Trad
ingSystems andMethods, 4th ed. (Hoboken, NJ: John Wiley & Sons, 2005);
J.F. Ehlers, CyberneticAnalysisforStocks andFutures: Cutting-Edge DSP
Technology to Improve Your Trading (Hoboken, NJ: John Wiley & Sons,
2004).
56. D. Pyle, Data Preparationfor Data Mining (San Francisco: Morgan Kauf
mann, 1999); T. Masters, Neural, Novel & Hybrid Algorithmsfor Time Se
ries Pr-ediction (New York: John Wiley& Sons, 1995); T. Masters, Practical
Neural Net Recipes in C++ (New York: Academic Press, 1993); I.H. Witten
and E. Frank, Data Mining: Practical Machine Learning Toots and Tech
niques,2nded. (SanFrancisco:MorganKaufmann,2005);E.M.Azoff,Neural
Network Time Series Forecasting ofFinancial Markets (New York: John
Wiley&Sons, 1994).



==================================================
                     PAGE 532                     
==================================================

516 NOTES
57. Masters,Neural, Novel & HybridAlgorithms.
58. Pyle,DataPreparation.
59. S.M. Weiss and N. Indurkhya, Predictive Data Mining-a Practical Guide
(SanFrancisco:MorganKaufmann, 1998).
60. Pyle,DataPreparation, xviii.
61. Masters,Neural, Novel &HybriadAlgorithms,2.
62. WeissandIndurkhya,PredictiveDataMining, 21, 57.
63. R. Kurzweil, The Singularity Is Near: When Humans Transcend Biology
(NewYork: PenguinGroup, 2005).



==================================================
                     PAGE 533                     
==================================================

Index
Affirmingtheconsequent, 112-121, appliedtosinglerule,
170,219 241-243
Alexanderreversalfilter, 18 dataminingversussingle-rule,
Alternativehypothesis: 268-271
incasestudy, 394 positionbiasandmarkettrend
vs. nullhypothesis, 221-225 components, 23-27
Anchoring, behavioralfinance Bacon, Francis, 125-126
theoryand,358-361 Barberis,Shleifer, andVishny(BSV)
Arbitragepricingtheory (APT), 341 hypothesis, 372-374,376
limitsof, 356-357 Baseballstatistics, data-mining
rationalpricinglevelsand,343, biasand, 258
344,347-348 Baseratefallacy, 91
Arditti, F.D., 84 Bayes'theorem, 90,346
Argument, defined, 112 Beadsina boxsamplingexample,
Aristotle, 104-105, 111, 122 172-186
PuTnstrong, Scott, 462-463 samplingdistributionand, 203
Artificialtradingrules (ATRs): statisticaltheoryelementsand,
data-miningbiasexperiments 186-190
and,291-293 Behavioralfinance theory, 355-378
datamining'ssoundnessand, foundations of, 356-357
309-311 psychologicalfactors, 357-362
differingexpectedreturns scientifichypotheses of, 371-378
experiment, 307-309 self-organizingPonzischemes
equalmeritexperiment, 293-307 and, 370-371
Assumed executionprices, 29-30 socialfactors, 362-369
Asymmetricbinaryvariables, Behaviorism,81-82
78-80 Beliefs:
Availabilityheuristic, 87-88 beliefinertia, inbehavioral
finance theory, 375
Backtesting, 15-16.See also Data contrastedto knowledge, 1-5
mining; Data-miningbias erroneous, 33-38
computer-intensivemethods Bellcurve, 211-213
517



==================================================
                     PAGE 534                     
==================================================

518 INDEX
Bellman, Richard, 465 BoxTheory, 36-37
Benchmarks,22-29 Bulkowski, T.N., 161
detrendingand, 27-28
effectofpositionbiasand Camerer, C., 466-467
markettrend on, 23-27 Capitalassetpricingmodel
usinglogsinsteadof (CAJPM),34D-341
percentages,28-29 Casestudy, seeRule datamining
Best-performingrule: casestudy
data-miningbiasand, 263-264, Categoricalsyllogisms, 112-115
278-287 CentralLimitTheorem,
defined,256 211-213
Biases,seeSubjectivetechnical Centraltendencymeasurements,
analysis 191
Bible Codes, data-mining biasand, Chaiken, Marc, 409
258-260 Chang, Kevin, 151-161
Binaryrules, 15-31 Channelbreakoutoperator(CBO),
inputs, outputs, signals, 16, 397-398,419-420
17 Channel-normalizationoperator
look-ahead bias, 29-30 (CN),401-403
subjectivetechnicalanalysis Chartanalysis, misplacedfaithin,
and, 15-16, 72-78 82-86
thresholdsand: representativeness heuristicand
fixed, 16-19 illusorytrendsandpatterns,
multiple, 19-21 93-101
tradingcosts, 31 Clusteringillusion, 99-100, 362
traditionaland inverserules, Cognitive content, ofknowledge
21-22 and beliefs, 2-5
use ofbenchmarksinevaluation, Cognitivepsychology, see
22-29 Subjectivetechnical
Black, Fischer, 345 analysis
Bloom, Norman, 258 Cohen, PR., 282
Bootstrapsampling, 215, 235-238 Cointegration, 434-436
applied tobacktestofsingle Commodityand currencyhedge
rule, 241-242 risktransferpremium, 379,
confidenceintervalsand, 380-384
248-250 Complexrules, notin casestudy,
contrastedto MonteCarlo 392,452-461
method,235 Computer-basedsampling
data-miningbiassolutionsand, methods,215,234-243
320-330 humaninteractionwith, 464-465,
Bostian, David, 409 471-473
Boundedrationality, principle of, Conditionalprobability(p-value),
42 231-233



==================================================
                     PAGE 535                     
==================================================

Index 519
Conditionalsyllogisms, 115-116 hiddenormissingdataand, 80
invalidfOTITIs, 118-121 possibleoutcomesofbinary
validfOTITIs, 117-118 variablesandfaulty
Confidenceinterval, 243, 245-247 intuition, 73-78
defined,216 Cowles,Alfred, 462-463
generatingwith bootstrap Cumulativesumprice-volume
method, 248-250 functions, 407-413
hypothesis testcontrastedto, Curse ofdimensionality, 465
250-252 Cutler, D., 349
samplingdistributionand,
247-248 Daniel, Rirshleifer, and
forTI-4-91 rule, 252-253 Subrahmanyam(DRS)
Configural-thinkingproblems, hypothesis, 375-376
42-45 Darvas, Nicholas, 36-37
Confirmationbias, 62-71 Datadistributionofthepopulation,
behavioralfinance theoryand, 206-207
358 Datadistributionofthesample,
beliefsurvivaland,69 206-207
contradictoryevidenceand, Datamining, seealso Data-mining
67-69 bias; Rule dataminingcase
perceptionandmotivationand, study
62-63 confirmationbiasand, 64
questionsandsearchand, defined,171,255,256,264
63-64 asmultiple comparison
subjectivemethodsand, 69-71 procedure,264-265
vagueevidenceand, 6&-67 soundnessofpremise of,
valueevaluationcriteriaand, 309-311
64-66 asspecificationsearch, 265-267
Conjunctionfallacy, 91-93 Data-miningbias, seealso Data
Conservatismbias: mining; Rule datamining
behavioralfinance theoryand, casestudy
357-358 anecdotalexamplesof, 256-261
BVShypothesisand, 372-374 causesof, 263-264, 278-287
DRShypothesisand,375-376 defined,255-256
Consistency, rule of, 111-112 experimentalinvestigationsof,
Controlillusion, 50 291-320
Cooper, Michael, 353,384 ATRsand, 309-311
Correlations, illusory, 72-82 variablemeritrules and,
asymmetric binaryvariablesand, 311-320
78-80 factors detenniningmagnitude
behavioralpsychologyand, of,287-291
81-82 objectivetechnicalanalysisand,
binaryvariablesand, 72 267-272



==================================================
                     PAGE 536                     
==================================================

520 INDEX
Data-miningbias(Continued) Ehlers,J.F., 399,400, 452
solutionsfordealingwith, Einstein,Albert, 108
320-330 Elder,John, 83
statisticalinferenceand, 272-278 Elfron, B., 235
Data-snoopingbias, 390-391, 449 ElliottWave Principle (EWP),
Dawes, R.M., 46~69 60-61,69-70,137
Declarativestatements, beliefsand Endowmentbias, 375-376
lrnowledgeand, 2-5 Engle, R.F., 434, 436
Deductivelogic, 112-121 Enumeration, deductionby,
categoricalsyllogisms, 112-115 122-124
conditionalsyllogisms, 115-121 Equitymarketriskpremium,379
Denialofthe consequent, 112-121, Error-correctionmodel, 434--435
170,219,221 Errors, unbiasedandsystematic,
Descartes, Rene, 126 272-274
Detrending, 19, 27-28 Eulercircles, 114-115
incasestudy, 391-392 Evidence,vagueandcontradictory,
proofofvalue of, 475--476 64-69
Diaconis, Percy, 260 Evidence-basedtechnicalanalysis
Directionalmodes, 21 (EBTA),seealso Rule data
Discernible-differencetest, miningcasestudy
cognitivecontentand, 3--4 academicfindings and, 8-9
Divergencerules, testedincase defined, 1
study, 430--440 differsfrom technicalanalysis,
Drosnin, Michael, 259, 260 6-8
Dysart, Paul, 411, 412--413 future and, 463--465, 471--473
Expectedperformance, defined,
EfficientMarketsHypothesis 255
(EMH),331,334-355 ExtendedMiddle, Lawof, 111
assumptionsof, 343-345 Extremevaluesandtransitions (E
flaws in, 345-348 rules), testedincasestudy,
consequences ofmarket 420--430
efficiency, 335-336
efficientmarketsdefined, Falsifiableforecast, 47, 58
334-335 Falsificationism, 130-136
empiricalchallengesto, 349-355 informationcontentand,
evidence of, 337-342 136-139
false notionsof, 337 scientificresponsesto, 139-143
falsification and, 141-143 Falsificationoftheconsequent,
informationcontentof 219-221
hypothesesand, 137-138 Fama, Eugene, 339,354-355
nonrandompricemotionand, Feedback, behavioralfinance
378-385 theoryand, 366-371
paradoxesof, 342-343 Felsen,J., 464



==================================================
                     PAGE 537                     
==================================================

Index 521
Festinger, L., 63 Hindsightbias, 50-58
Finucase, M., 470 HongandStein (HS) hypothesis,
Flip-flop(s), 20, 361 376-377
Forgas,Joseph,471 Hsu, P.-H., 451, 455
Fosback, orman, 411, 412-413, Hulbert Digest, 48
416,436,464 Hume, David, 126-128
FosbackIndex, 436 Hussman,J., 430
French,Kenneth,354-355 Hypotheses, seealso Hypothesis
Frequencydistribution, 179-181, tests
190-191 alternative,221-225,394
measuringcentraltendency, developmentofconceptof,
191 128-130
variability(dispersion) falsifiabilityand, 130-143
measurements, 192-193 null, 139, 166-172,221-225,
Futuresmarkets, 380-384 393
Hypothesis tests:
Galileo Galilei, 106-107 computer-intensive methodsof
Gambler'sfallacy, 99-100, 362 samplingdistribution
Gilovich, Thomas, 38, 68, 80, 85 generation,234-243
Goldberg, L.R., 467 confidence intervalscontrasted
Gould, StephenJay, 58 to,250-252
Granger, C.w'J., 434, 436 defined,217-218
Granville,Joseph, 408 informalinference contrasted,
Grossman, S.J., 343,378 218-223
Grove, w'M., 468 mechanicsof, 227-234
rationaleof, 223-227
Hall,J.,3-4 Hypothetico-deductivemethod:
Hansen, Peter, 329 stagesof, 144-147
Harlow, C.v., 413 technicalanalysisexan1ple,
Hastie, R., 468-469 145-146
Hayes,T., 21
Head-and-shoulderspattern, Illusorycorrelations, 72-82
objectificationexample, asymmetric binaryvariablesand,
151-161 78-80
Heiby, w,A., 403, 433 behavioralpsychologyand,
Herdbehavior, 362-369 81-82
Heuristic bias, 41, 86-87 binaryvariablesand, 72
availabilityheuristic, 87-88 hiddenormissingdataand, 80
heuristicdefined, 86 possibleoutcomesofbinary
representativeness heuristic, variablesandfaulty
88-93 intuition, 73-78
illusorytrendsandpatterns Illusory knowledge, 41-42, 49-50
and,93-101 In1itativebehavior, 362-369



==================================================
                     PAGE 538                     
==================================================

522 INDEX
Immediatepracticalfuture: Knowledge:
incasestudy, 393 contrastedto beliefs, 1-5
defined, 186-187 erroneousbeliefsand, 33-38
Indicators, incasestudy, illusory,41--42, 49-50
40~17 scientific, 108-110
interestratespreads, 417 Kuan, C.-M., 451, 455
marketbreadthindicators, Kuhn, Thomas, 150
413--416
price andvolumefunctions, Lane, George, 403
406--413 LawofLarge Numbers, 96-99, 179,
prices-of-debtinstrumentsfrom 194-195,209
interestrates, 416--417 LawofNoncontradiction, 111-112
Indicatorscriptinglanguage (ISL), Lawofthe ExcludedMiddle,
403--405 111
Inductivelogic, 121-124.See also Leinweber, DavidJ., 260-261
Statisticalinference Levy, K.N., 464
Indurkhya, N., 472 Liquiditypremium, 379-380,
Information: 384-385
biased interpretationofprivate, Lo, Andrew, 341,378
375-376 Log-basedmarketreturns, 28-29
biasedinterpretation ofpublic, Logic:
372-374,376 consistencyand, 111-112
cascadesof, 364-365 deductive, 112-121
discoverypremium, 380 inductive, 121-124
stale, 340, 349, 351-354 propositionsand arguments, 112
In-sampledata, defined, 256 Long-shortposition bias, trends
Interestratespreads, incasestudy, and,23-27
417 Look-ahead bias, 29-30
Intervalestimates,seeConfidence Lowenstein, G.F., 470
interval Lowry, Lyman, 414
Inverse rules, 21-22
MacGregor, D., 470
Jacobs, B.L, 464 MacKinlay, A. Craig,341,378
Jegadeesh,N., 352-353 Magee,John, 333
Jensen, D.D., 282 Malkiel, Burton,87
Judgmentheuristics, seeHeuristic Marketbreadthindicators, in case
bias study, 413--416
Marketnettrend, positionbias
Kahn, Ronald, 258 and, 23-27
Kahneman, Daniel, 41, 86, 88, Markowitz,H.M., 323
91-92,345 MarkowitzlXu datamining
Kelly Criterion, 348 correctionfactor, 321,
Kestner, Lars, 383 323-324



==================================================
                     PAGE 539                     
==================================================

Index 523
Masters,Timothy, 238-239, 329, foundations of,356-357
447,472 psychologicalfactors, 357-362
Meehl, Paul,466, 468 scientifichypothesesof,
Mencken, H.L., 59 371-378
Metcalfe,Janet, 45 self-organizingPonzischemes
Mill,JohnStuart, 132 and, 370-371
Momentum, predictabilitystudies socialfactors, 362-369
and,352-353 incontextofefficientmarkets,
MonteCarlopermutationmethod 378-385
(MCP), 215, 238-240.See EfficientMarketsHypothesis
alsoArtificialtradingrules; and,331,334-355
Rule dataminingcasestudy assumptionflaws, 345-348
applied to backtestofsingle assumptionsof, 343-345
rule, 242-243 consequencesofmarket
contrastedtobootstrapmethod, efficiency, 335-336
235 efficientmarketsdefined,
data-mining biassolutionsand, 334-335
320--330 empiricalchallengesto,
notused to generateconfidence 349-355
intervals,248,250 evidence of, 337-342
Mosteller, Frederick, 260 false notionsof,337
Moving-average operator(MA), 18, information contentof
398-401 hypothesesand, 137-138
Movingaverage price-volume nonrandompricemotionand,
functions, 407-413 378-385
Mt. LucasManagementIndex paradoxesof, 342-343
(MLM),382-384 importance ofscientifictheory,
Multiplecomparisonprocedure 331-334
(MCP): Nonstationarystatisticalproblems,
dataminingas, 264-265 174,188
randomnessand, 281-285 Nonuniversalgeneralizations,
Multiplethresholdrules, 19-21 122
Murphy,John, 333,391 Noreen, Eric, 235
Normaldistribution, 211-213
Narratives, seeSecond-hand Nullhypothesis, 139
informationbias vs. alternativehypothesis,
Nofsinger,John R., 50, 151, 221-225
470 incasestudy, 393
Noise traders, 343,345,347 instatisticalanalysis, 166-172
Noncontradiction, Lawof, 111-112
Nonrandomprice motiontheories: Objectivebinarysignalingrules,
behavioralfinance theoryand, seeBinaryrules
355-378 Objectivereality, 107-108



==================================================
                     PAGE 540                     
==================================================

524 INDEX
Objectivetechnicalanalysis, 5-8 historyof, 125-136
confirmationbiasand, 64 hypothesesand, 136-143
dataminingand, 267-272 scientificmethodand, 124-125
erroneousknowledge and, Plato, 125
261-264 Pointestimates, 218, 243-245
faulty inferencesand, 36 Ponzischemes, self-organizing,
subsetsof, 161-163 370-371
Objectivity, ofscientific Popper, Karl, 130-136, 143
knowledge, 108-109 Populationmean, 191
Observedperformance, defined, Population/populationparameter:
256 incasestudy, 393
Occam'sRazor, 107-108,225-227 instatistical inferenceproblem,
Optimismbias, 48, 361 186-188
Oscillators, 19 Practicalsignificance, in case
Osler, Carol, 151-161 study, 394
Out-of-sampledata, defined, 256 Prechter, Robert, 60, 61
Out-of-sampleperformance Predictions, 3
deterioration, 261-264 confirmatoryevidenceand,
Out-of-sampletesting, as data 218-219
mining biassolution, 320, in hypothesis testingcontext,
321-323 133-134
Overconfidencebias, 45-58 immediatepracticalfuture and,
behavioralfinance theoryand, 186-187
361 Predictiverules, scientific method
controlillusion, 50 and,110
hindsightbias, 50-58 Priceandvolumefunctions, ascase
knowledge illusion, 49-50 studyindicator,406-413
manifestationsof, 47 Pricepredictability, EMHand,
optimismbias,48 349-353
self-attributionbias, 48-49 Prices-of-debtinstrumentsfrom
interestrates, as casestudy
Parameterestimation: indicator,416-417
defined,217-218 Pricevolatility, EfficientMarkets
intervalestimates, 218, 243, Hypothesisand, 349
245-253 Principle ofboundedrationality, 42
pointestimates, 218, 243-245 Principle ofsimplicity, 107-108,
Parameteroptimization, 266 225-227
Patternrenaming, 64-65 Pring, M.J., 431
Perkins, David, 71 Probability, 193.See also Sampling
Peters, Edgar, 341, 470 inductive logicand, 121-122
Philosophyofscience, 124-143 LawofLarge Numbers, 194-195
distinguishingsciencefrom probabilitydensityfunction,
pseudoscience, 134-136 167-172



==================================================
                     PAGE 541                     
==================================================

Index 525
probabilitydistribution, 197-202 Reversal rules, defined, 17
relativefrequency distribution, Risk, definingand quantifying,
181-186, 197-202 340-341
theoreticalversusempirical, 196 Risk transferpremiums, 378-385
Programmabilitycriterion, Roberts, Henry, 83
objectivetechnicalanalysis Roll, R., 349
and, 16 Romano, J.P., 330
Propositions, defined, 112 Rule dataminingcasestudy:
ProspectTheory, 345 critiqueof, 448-451
Psychologicalfactors, of indicatorsusedin, 405-417
behavioralfinance theory, interestratespreads, 417
357-362 marketbreadthindicators,
P-value,231-234 413-416
Pyle, D., 472 priceandvolumefunctions,
406-413
Quantification, ofscientific prices-of-debtinstruments
knowledge, 109 from interestrates, 416-417
Quants,104 parametersof, 389-392
possible extensions of, 451-461
Randomization methods, see rawtimeseriesusedin, 405-406,
Bootstrapsampling; Monte 417-418
Carlopermutationmethod results of, 441-448
Randomness, data-miningbiasand, rules testedin:
263-264, 278-287.See also divergence, 430-440
Nonrandompricemotion extremesandtransitions,
theories 420-430
Randomvariables, defined, 175 trendrules, 419-420
Randomwalks, seeEfficient instatisticalterms, 392-394
MarketsHypothesis time-seriesoperatorsin, 396-405
Rationalinvestorassumption, of channelbreakoutoperator,
EfficientMarkets 397-398
Hypothesis, 343-346 channel-normalization
Rationality, limitsof, 356-357 operator, 401-403
Reasoningbyrepresentativeness, indicatorscriptinglanguage,
87-88 403-405
Reinforcementschedule, illusory moving-averageoperator,
correlationsand, 81-82 398-401
Relativefrequency distribution, transformingdataseriesinto
181-186, 197-202 marketpositions, 394--396
Relative StrengthIndex (RSI), Rules, seeBinaryrules; Rule data
460 miningcasestudy
Representativeness heuristic, Russell,Bertrand,59,166
88-93, 93-101 Russo,J.E., 467



==================================================
                     PAGE 542                     
==================================================

526 INDEX
S&P500, seeRule dataminingcase natureofscientificknowledge
study and, 108-110
Sagan, Carl, 40-41 objectificationofsubjective
Samplemean, 191,243-245 technicalanalysis, 148-151
Samplesizeneglect, 361-362, example, 151-161
372-374 opennessandskepticismin, 143,
Sampling, seealso Sampling 225
distribution philosophyof, 124-143
beadsina boxexampleof, searchbiasand, 64
172-186 Secondhandinformationbias,
frequency distributionand, 58-61
179-181 anchoringand, 360-361
relativefrequency distribution, informationdiffusionand,
181-186 365-366
samplestatistics, 175-177, Self-attributionbias, 48--49
188-189,202,393 DHShypothesisand, 375-376
samplingvariability, 177-179 Self-interest, secondhandaccounts
Samplingdistribution, 201-202 and,61
classicalderivationapproach, Shermer, Michael, 38
209-215 Shiller, Robert, 333-334,365,
computer-intensivemethodsof 366
generating, 215, 234-243, Shleifer,Andre, 347
464-465,471--473 Siegel,Jeremy, 84
confidenceintervalsand, Signals, 16-18
247-248 Simon, Barry, 259
dataminingand, 276-278 Simon, Herbert, 42
defined,203 Simplicity, principleof, 107-108,
mechanicsofhypothesistesting 225-227
and,227-234 Single-ruleback-testing,versus
samplingdistributionofthe datamining, 268-271
mean, 209-213 Skepticism, 143,225
tradingperformanceand, 206 Slope ofyieldcurve, 417
uncertaintyqualifiedby, 203-206 Slovic, Paul, 41, 470
Samuelson, Paul, 335-336 Snelson,JayStuart, 71
Sawyer,J., 466 Socioeconomics, 151
Schoemaker, P.J.H., 467 Spatialclustering, 100-101
Scientificmethod: Staleinformation, 340,349,
defined, 103,332 351-354
historyof, 103-108 Standard deviation, 192
hypothetic-deductivemethod, Standarderrorofthemean,
144-147 213-215
keyaspectsof, 147-148 Statementaboutreliabilityof
logicand, 111-124 inference, 190



==================================================
                     PAGE 543                     
==================================================

Index 527
Stationarystatisticalproblems, defined,217-218
174,188 informalinference contrasted,
Stationarytimeseries, 19 218-223
Statisticalanalysis: mechanicsof, 227-234
descriptivestatisticstools: rationale of, 223-227
centraltendency parameterestimation:
measurements, 191 defined,217-218
frequency distribution, intervalestimates, 218, 243,
190-191 245-253
variability (dispersion) pointestimates, 218,
measurements, 192-193 243-245
inferentialstatistics: Statisticalsignificance, 23
elementsofstatistical incasestudy, 394
inferenceproblem, 186-190 statisticalsignificanceof
samplingexample, 172-186 observation, 171
three distributionsof, 206-207 statisticalsignificance oftest
probability, 193 (p-value),232-234
LawofLarge Numbers, Stiglitz,J.E., 343,378
194-195 Stochastics, 401-403
probabilitydistribution, Stories, seeSecondhand
200-202 informationbias
probabilitydistributionof Subjectivetechnicalanalysis, 5-8,
randomvariables, 197-199 15-16, 161-163
theoreticalversusempirical, adoptionofscientificmethod
196 and, 148-151
samplingdistributionand, example, 151-161
201-206 chartanalysisand, 82-86
classicalderivationapproach, confirmationbiasand, 62-71
209-215 erroneous beliefsand, 33-35
computer-intensiveapproach, futility offorecasting and,
215 465-471
usedto counteruncertainty, heuristic biasand, 86-93
165-172 illusiontrendsand chart
Statistical hypothesis, defined, 220 patterns, 93-101
Statisticalinference: humanpatternfinding and
dataminingand, 272-278 informationprocessing,
defined, 189 39-45
hypothesis tests: illusorycorrelationsand, 72-82
computer-intensivemethods overconfidencebiasand, 45-58
ofsamplingdistribution secondhandinformationbias
generation, 234-243 and,58-61
confidenceintervals asuntestableandnotlegitinlate
contrastedto, 250-252 knowledge, 35-38



==================================================
                     PAGE 544                     
==================================================

528 INDEX
Syllogisms: Ulam, Stanislaw, 238
categorical, 112-115 Uncertainty:
conditional, 115-116 inabilitytorecallprior, see
invalidforms, 118-121 Hindsightbias
validforms, 117-118 rulingout, seeStatisticalanalysis
Uncorrelatedinvestorerrors, in
Taleb, Nassim, 337 EfficientMarkets
Technicalanalysis (TA), 9-11.See Hypothesis, 343, 344,
also Evidence-based 346-347
technicalanalysis; Objective Unfalsifiable propositions, 134-136
technicalanalysis; Universesize, defined, 256
Subjectivetechnical
analysis Validity:
beliefsand knowledge and, 1-5 illusionof, 42
defined, 1 inlogic, 113-115
future of, 461--473 Variability (dispersion)
scienceand, 463--464 measurements, 192-193
Tetlock, P.E., 469 Variablemeritrules experiment,
Thoreau, Henry David, 62 311-320
Threshold:
binaryrules and, 16-21 Walk-forward testing, 322
defined,17 Ward,Artemus,36
Time-seriesoperators, incase Weiss, S.M., 472
study, 396--405 Wells, H.G., 165
channelbreakoutoperator, Whewell, William, 128-130
397-398 White, Halbert, 236, 281,
channel-normalizationoperator, 325
401--403 White's RealityCheck (WRC),
indicatorscriptinglanguage, 325-327,329-330. See also
403--405 Rule dataminingcasestudy
moving-averageoperator, Wilder,J. Welles, 460
398--401 Wilder's Relative StrengthIndex
Titman, S., 352-353 (RSI),460
Tradingcosts, 31 Williams, Larry, 409
Trend rules, testedin case study, Wolf, M., 330
419--420
Trends, illusory, 83-86, 99 Xu, G.L., 323
Truth, inlogic, 113
TT-4-91 rule, 252-253 Zero-centeringadjustment, 237
Tversky,Amos, 41,88,91-92, 345 Zweig, Martin, 390